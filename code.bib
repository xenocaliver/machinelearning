% This file was created with JabRef 2.10.
% Encoding: UTF8


@Article{weko_91323_1,
  Title                    = {Non-Volatileメインメモリとファイルシステムの融合},
  Author                   = {追川,修一},
  Journal                  = {情報処理学会論文誌},
  Year                     = {2013},

  Month                    = {mar},
  Number                   = {3},
  Pages                    = {1153--1164},
  Volume                   = {54},

  File                     = {:PDF\\IPSJ-JNL5403017.pdf:PDF},
  Timestamp                = {2015.06.30}
}

@Article{weko_95760_1,
  Title                    = {Non-Volatileメインメモリを用いたチェックポイント・リスタートシステム},
  Author                   = {追川,修一 and 三木,聡},
  Journal                  = {情報処理学会論文誌コンピューティングシステム（ACS）},
  Year                     = {2013},

  Month                    = {oct},
  Number                   = {4},
  Pages                    = {49--57},
  Volume                   = {6},

  File                     = {:PDF\\IPSJ-TACS0604007.pdf:PDF},
  Timestamp                = {2015.06.30}
}

@Article{三浦晋示:1992-11-25,
  Title                    = {ある平面曲線上の代数幾何符号},
  Author                   = {三浦, 晋示},
  Journal                  = {電子情報通信学会論文誌. A, 基礎・境界},
  Year                     = {1992},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {1735-1745},
  Volume                   = {75},

  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\j75-a_11_1735.pdf:PDF},
  ISSN                     = {09135707},
  Publisher                = {電子情報通信学会基礎・境界ソサイエティ},
  Timestamp                = {2014.06.06},
  Url                      = {http://ci.nii.ac.jp/naid/10006928347/}
}

@InProceedings{5454095,
  Title                    = {Trapping set enumerators for specific LDPC codes},
  Author                   = {Abu-Surra, S. and Declercq, D. and Divsalar, D. and Ryan, W.E.},
  Booktitle                = {Information Theory and Applications Workshop (ITA), 2010},
  Year                     = {2010},
  Pages                    = {1-5},

  Abstract                 = {In this paper, a method is presented for enumerating the trapping sets of a specific LDPC code given its Tanner graph. The technique involves augmenting the original Tanner graph with additional variable nodes, and then applying a weight-enumeration algorithm to the augmented Tanner graph. The proposed method is used to find trapping set enumerators for several LDPC codes in communication standards. The complexity of the proposed algorithm is discussed.},
  Doi                      = {10.1109/ITA.2010.5454095},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05454095.pdf:PDF},
  Keywords                 = {graph theory;parity check codes;LDPC codes;augmented Tanner graph;communication standards;trapping set enumerator;weight-enumeration algorithm;Code standards;Communication standards;Iterative decoding;Laboratories;Parity check codes;Propulsion;Sections},
  Timestamp                = {2013.12.17}
}

@Article{ackley1985learning,
  Title                    = {A learning algorithm for boltzmann machines*},
  Author                   = {Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
  Journal                  = {Cognitive science},
  Year                     = {1985},
  Number                   = {1},
  Pages                    = {147--169},
  Volume                   = {9},

  File                     = {:PDF\\s15516709cog0901_7.pdf:PDF},
  Publisher                = {Wiley Online Library},
  Timestamp                = {2015.07.30}
}

@InProceedings{7035602,
  Title                    = {A high throughput configurable partially-parallel decoder architecture for Quasi-Cyclic Low-Density Parity-Check Codes},
  Author                   = {Al Hariri, A.A. and Monteiro, F. and Sieler, L. and Dandache, A.},
  Booktitle                = {Design of Circuits and Integrated Circuits (DCIS), 2014 Conference on},
  Year                     = {2014},
  Month                    = {Nov},
  Pages                    = {1-6},

  Abstract                 = {In this paper, we are proposing a new architecture for the fast decoding of Quasi-Cyclic Low-Density Parity Codes (QC-LDPC). QC-LDPC codes are becoming more and more popular in a wide range of applications, including data transmission (WiMAX, DVB-S2) in telecommunication systems, increasing the need for effective decoder architectures. In the present approach, support for a large subset of QC-LDPC codes is provided thanks to the configurability of the architecture at the synthesis level. High levels of parallelism can be reached and hence high throughput achieved, thanks to the modular decoder architecture that takes advantage of the highly regular structure of QC-LDPC parity check matrices. The architectural design has been validated through the implementation of different decoders related to DVB-T2 and DVB-S2 on FPGAs of the Altera Stratix II family. Very high data rates (up to 28.9 GB/s) have been achieved with still acceptable hardware consumption (about 32k logic elements) proving the effectiveness of the approach.},
  Doi                      = {10.1109/DCIS.2014.7035602},
  File                     = {:PDF\\07035602.pdf:PDF},
  Keywords                 = {WiMax;cyclic codes;decoding;digital video broadcasting;field programmable gate arrays;matrix algebra;parity check codes;Altera Stratix II family;DVB-S2;DVB-T2;FPGA;QC-LDPC codes;QC-LDPC parity check matrices;WiMAX;architectural design;architecture configurability;data transmission;decoder architecture;fast decoding;hardware consumption;high-throughput configurable partially-parallel decoder architecture;highly-regular structure;modular decoder architecture;parallelism level;quasicyclic low-density parity-check codes;synthesis level;telecommunication systems;Decoding;Digital video broadcasting;Indexes;Iterative decoding;Throughput;WiMAX;Error correcting codes;FPGA implementation;QC-LDPC;parallel and configurable decoder architectures},
  Timestamp                = {2015.07.17}
}

@Article{0305-4470-11-5-028,
  Title                    = {Stability of the Sherrington-Kirkpatrick solution of a spin glass model},
  Author                   = {J R L de Almeida and D J Thouless},
  Journal                  = {Journal of Physics A: Mathematical and General},
  Year                     = {1978},
  Number                   = {5},
  Pages                    = {983},
  Volume                   = {11},

  Abstract                 = {The stationary point used by Sherrington and Kirkpatrick (1975) in their evaluation of the free energy of a spin glass by the method of steepest descent is examined carefully. It is found that, although this point is a maximum of the integrand at high temperatures, it is not a maximum in the spin glass phase nor in the ferromagnetic phase at low temperatures. The instability persists in the presence of a magnetic field. Results are given for the limit of stability both for a partly ferromagnetic interaction in the absence of an external field and for a purely random interaction in the presence of a field.},
  File                     = {:PDF\\0305-4470-11-5-028.pdf:PDF},
  Timestamp                = {2015.02.23},
  Url                      = {http://stacks.iop.org/0305-4470/11/i=5/a=028}
}

@Article{Alon2002,
  Title                    = {The Moore Bound for Irregular Graphs},
  Author                   = {Alon, Noga and Hoory, Shlomo and Linial, Nathan},
  Journal                  = {Graphs and Combinatorics},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {53-57},
  Volume                   = {18},

  Doi                      = {10.1007/s003730200002},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\A10.1007.003730200002.pdf:PDF},
  ISSN                     = {0911-0119},
  Language                 = {English},
  Publisher                = {Springer-Verlag},
  Timestamp                = {2013.12.27},
  Url                      = {http://dx.doi.org/10.1007/s003730200002}
}

@Book{alon:probabilistic,
  Title                    = {The Probabilistic Method},
  Author                   = {Alon, Noga and Spencer, Joel H.},
  Publisher                = {Wiley},
  Year                     = {1992},

  Address                  = {New York},

  Added-at                 = {2008-02-26T11:58:58.000+0100},
  Biburl                   = {http://www.bibsonomy.org/bibtex/2ec63f2bc7a58f88c650de0ea6194bfbb/schaul},
  Citeulike-article-id     = {2375612},
  Description              = {idsia},
  Interhash                = {3138bd2b231ea0afc1f1bed2197a7e0e},
  Intrahash                = {ec63f2bc7a58f88c650de0ea6194bfbb},
  Keywords                 = {nn},
  Timestamp                = {2008-02-26T11:58:58.000+0100}
}

@Article{Amari1999783,
  Title                    = {Improving support vector machine classifiers by modifying kernel functions },
  Author                   = {S. Amari and S. Wu},
  Journal                  = {Neural Networks},
  Year                     = {1999},
  Number                   = {6},
  Pages                    = {783 - 789},
  Volume                   = {12},

  Abstract                 = {We propose a method of modifying a kernel function to improve the performance of a support vector machine classifier. This is based on the structure of the Riemannian geometry induced by the kernel function. The idea is to enlarge the spatial resolution around the separating boundary surface, by a conformal mapping, such that the separability between classes is increased. Examples are given specifically for modifying Gaussian Radial Basis Function kernels. Simulation results for both artificial and real data show remarkable improvement of generalization errors, supporting our idea.},
  Doi                      = {http://dx.doi.org/10.1016/S0893-6080(99)00032-5},
  File                     = {:PDF\\1-s2.0-S0893608099000325-main.pdf:PDF},
  ISSN                     = {0893-6080},
  Keywords                 = {Support vector machine},
  Timestamp                = {2015.07.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0893608099000325}
}

@Article{5075875,
  Title                    = {Channel Polarization: A Method for Constructing Capacity-Achieving Codes for Symmetric Binary-Input Memoryless Channels},
  Author                   = {Arikan, E.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {3051-3073},
  Volume                   = {55},

  Abstract                 = {A method is proposed, called channel polarization, to construct code sequences that achieve the symmetric capacity I(W) of any given binary-input discrete memoryless channel (B-DMC) W. The symmetric capacity is the highest rate achievable subject to using the input letters of the channel with equal probability. Channel polarization refers to the fact that it is possible to synthesize, out of N independent copies of a given B-DMC W, a second set of N binary-input channels {WN(i)1 les i les N} such that, as N becomes large, the fraction of indices i for which I(WN(i)) is near 1 approaches I(W) and the fraction for which I(WN(i)) is near 0 approaches 1-I(W). The polarized channels {WN(i)} are well-conditioned for channel coding: one need only send data at rate 1 through those with capacity near 1 and at rate 0 through the remaining. Codes constructed on the basis of this idea are called polar codes. The paper proves that, given any B-DMC W with I(W) > 0 and any target rate R< I(W) there exists a sequence of polar codes {Cfrn;nges1} such that Cfrn has block-length N=2n , rate ges R, and probability of block error under successive cancellation decoding bounded as Pe(N,R) les O(N-1/4) independently of the code rate. This performance is achievable by encoders and decoders with complexity O(N logN) for each.},
  Doi                      = {10.1109/TIT.2009.2021379},
  ISSN                     = {0018-9448},
  Keywords                 = {binary codes;channel capacity;channel coding;decoding;memoryless systems;probability;channel capacity;channel coding;channel polarization;code sequence;polar codes;probability;successive cancellation decoding algorithm;symmetric binary-input memoryless channel;Capacity planning;Channel capacity;Channel coding;Codes;Councils;Decoding;Information theory;Memoryless systems;Noise cancellation;Polarization;Capacity-achieving codes;Plotkin construction;Reed– Muller (RM) codes;channel capacity;channel polarization;polar codes;successive cancellation decoding},
  Timestamp                = {2014.08.01}
}

@Article{e84-A-11-2930,
  Title                    = {Construction of Secure Cab Curves Using Modular Curves},
  Author                   = {Seigo ARITA},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2001},

  Month                    = {11},
  Number                   = {11},
  Pages                    = {2930-2938},
  Volume                   = {E84-A},

  File                     = {:PDF\\e84-a_11_2930.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.03}
}

@InProceedings{4641210,
  Title                    = {A Statistical Model for Assessing the Fault Tolerance of Variable Switching Currents for a 1Gb Spin Transfer Torque Magnetoresistive Random Access Memory},
  Author                   = {Asao, Y. and Iwayama, M. and Tsuchida, K. and Nitayama, A. and Yoda, H. and Aikawa, H. and Ikegawa, S. and Kishi, T.},
  Booktitle                = {Defect and Fault Tolerance of VLSI Systems, 2008. DFTVS '08. IEEE International Symposium on},
  Year                     = {2008},
  Month                    = oct,
  Pages                    = {507 -515},

  Abstract                 = {A comprehensive statistical model of the switching probability was proposed for a 1 Gb spin transfer torque magneto resistive random access memory (STT-MRAM). Since the switching current varies with every write cycle owing to the thermal instability, the read disturbance and the write error are critical issues in the STT-MRAM. In this paper, the operating condition of read and write was designed so as not to cause the read disturbance or the write error. The effect of an error correcting code (ECC) on the read disturbance was also calculated. Finally, it was demonstrated that the 1 Gb STT-MRAM could be realized with the optimal bit line voltages and the ECC.},
  Doi                      = {10.1109/DFT.2008.18},
  File                     = {:PDF\\A_Statistical_Model_for_Assessing_the_Fault_Tolerance_of_Variable_Switching_Currents_for_a_1Gb_Spin_Transfer_Torque_Magnetoresistive_Random_Access_Memory.pdf:PDF},
  ISSN                     = {1550-5774},
  Keywords                 = {error correcting code;fault tolerance;optimal bit line voltages;spin transfer torque magnetoresistive random access memory;statistical model;variable switching currents;error correction codes;fault tolerant computing;magnetoresistive devices;random-access storage;statistical analysis;}
}

@Book{Efficient_Learning_Machines,
  Title                    = {Efficient Learning Machines: Theories, Concepts, and Applications for Engineers and System Designers},
  Author                   = {Mariette Awad and Rahul Khanna},
  Publisher                = {Apress},
  Year                     = {2015},

  File                     = {:PDF\\978-1-4302-5990-9.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.23}
}

@Article{Bailey20094253,
  Title                    = {Error-correcting codes from permutation groups },
  Author                   = {Robert F. Bailey},
  Journal                  = {Discrete Mathematics },
  Year                     = {2009},
  Number                   = {13},
  Pages                    = {4253 - 4265},
  Volume                   = {309},

  Abstract                 = {We replace the usual setting for error-correcting codes (i.e.ﾂvector spaces over finite fields) with that of permutation groups. We give an algorithm which uses a combinatorial structure which we call an uncovering-by-bases, related to covering designs, and construct some examples of these. We also analyse the complexity of the algorithm. We then formulate a conjecture about uncoverings-by-bases, for which we give some supporting evidence and prove for some special cases. In particular, we consider the case of the symmetric group in its action on 2-subsets, where we make use of the theory of graph decompositions. Finally, we discuss the implications this conjecture has for the complexity of the decoding algorithm. },
  Doi                      = {http://dx.doi.org/10.1016/j.disc.2008.12.027},
  File                     = {:PDF\\1-s2.0-S0012365X09000041-main.pdf:PDF},
  ISSN                     = {0012-365X},
  Keywords                 = {Error-correcting code},
  Timestamp                = {2015.04.01},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0012365X09000041}
}

@Article{4373388,
  Title                    = {Lower Bounds on the Error Rate of LDPC Code Ensembles},
  Author                   = {Barak, O. and Burshtein, D.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {nov. },
  Number                   = {11},
  Pages                    = {4225 -4236},
  Volume                   = {53},

  Abstract                 = {The ensemble of regular low-definition parity-check (LDPC) codes is considered. Using concentration results on the weight distribution, lower bounds on the error rate of a random code in the ensemble are derived. These bounds hold with some confidence level. Combining these results with known lower bounds on the error exponent, confidence intervals on the error exponent, under maximum-likelihood (ML) decoding, are obtained. Over a large range of channel parameter and transmission rate values, when the graph connectivity is sufficiently large, the upper bound of the interval approaches the lower bound, and the probability that the error exponent is within the interval can be arbitrarily close to one. In fact, in this case the true error exponent approaches the maximum between the random coding and the expurgated random coding exponents, with probability that approaches one.},
  Doi                      = {10.1109/TIT.2007.907448},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04373388.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {LDPC code ensembles;error exponent;error rate;graph connectivity;low-definition parity-check codes;lower bounds;maximum-likelihood decoding;probability;random code;weight distribution;error statistics;maximum likelihood decoding;parity check codes;random codes;},
  Timestamp                = {2011.11.15}
}

@Article{Barak2007,
  Title                    = {Lower Bounds on the Error Rate of LDPC Code Ensembles},
  Author                   = {Barak, O. and Burshtein, D.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {4225-4236},
  Volume                   = {53},

  Doi                      = {10.1109/TIT.2007.907448},
  File                     = {:PDF\\04373388.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {error statistics;maximum likelihood decoding;parity check codes;random codes;LDPC code ensembles;error exponent;error rate;graph connectivity;low-definition parity-check codes;lower bounds;maximum-likelihood decoding;probability;random code;weight distribution;Australia;Error analysis;Error probability;Information theory;Maximum likelihood decoding;Parity check codes;Polynomials;Signal processing;Signal processing algorithms;Upper bound;Error exponents;low-density parity-check (LDPC) codes;saddle point;weight (distance) distribution},
  Timestamp                = {2014.12.24}
}

@Article{5485013,
  Title                    = {Codes in Permutations and Error Correction for Rank Modulation},
  Author                   = {Barg, A. and Mazumdar, A.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {3158-3165},
  Volume                   = {56},

  Abstract                 = {Codes for rank modulation have been recently proposed as a means of protecting flash memory devices from errors. We study basic coding theoretic problems for such codes, representing them as subsets of the set of permutations of n elements equipped with the Kendall tau distance. We derive several lower and upper bounds on the size of codes. These bounds enable us to establish the exact scaling of the size of optimal codes for large values of n. We also show the existence of codes whose size is within a constant factor of the sphere packing bound for any fixed number of errors.},
  Doi                      = {10.1109/TIT.2010.2048455},
  File                     = {:PDF\\05485013.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {error correction;error correction codes;modulation coding;Kendall tau distance;coding theoretic problem;error correction;flash memory device;optimal codes;permutation;rank modulation;sphere packing bound;Aging;Error correction codes;Flash memory;Functional analysis;Hamming distance;Modulation coding;Protection;Statistics;Upper bound;Writing;Bose–Chowla theorem;Kendall tau distance;flash memory;inversion;rank permutation codes},
  Timestamp                = {2015.04.01}
}

@InProceedings{1216697,
  Title                    = {Fast decoding algorithm for LDPC over GF(2q)},
  Author                   = {Barnault, L. and Declercq, D.},
  Booktitle                = {Information Theory Workshop, 2003. Proceedings. 2003 IEEE},
  Year                     = {2003},
  Month                    = {March},
  Pages                    = {70-73},

  Abstract                 = {We present a modification of belief propagation that enables us to decode LDPC codes defined on high order Galois fields with a complexity that scales as p log2 (p), p being the field order. With this low complexity algorithm, we are able to decode GF(2q) LDPC codes up to a field order value of 256. We show by simulation that ultra-sparse regular LDPC codes in GF(64) and GF(256) exhibit very good performance.},
  Doi                      = {10.1109/ITW.2003.1216697},
  File                     = {:PDF\\01216697.pdf:PDF},
  Keywords                 = {Galois fields;computational complexity;decoding;parity check codes;LDPC codes;belief propagation;complexity;fast decoding algorithm;high order Galois fields;Belief propagation;Binary codes;Decoding;Equations;Galois fields;Parity check codes;Sparse matrices;Tensile stress;Turbo codes;Vectors},
  Timestamp                = {2015.04.21}
}

@InProceedings{1200390,
  Title                    = {Hardware implementation issues of a BMS decoding approach for AG based codes},
  Author                   = {Belkoura, Z. and de B Naviner, L.A.},
  Booktitle                = {Wireless Communications and Networking, 2003. WCNC 2003. 2003 IEEE},
  Year                     = {2003},
  Month                    = {March},
  Pages                    = {448-453 vol.1},
  Volume                   = {1},

  Abstract                 = {Algebraic-geometry (AG) family of codes contains sequences with excellent asymptotic behaviour, but only few pieces of work have treated their hardware implementation. In this paper, we investigate an algorithm for decoding AG codes from the hardware feasibility point of view. We modify the original strategy in order to obtain a new structure more suitable for hardware implementation.},
  Doi                      = {10.1109/WCNC.2003.1200390},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01200390.pdf:PDF},
  ISSN                     = {1525-3511},
  Keywords                 = {algebraic geometric codes;codecs;decoding;error correction codes;AG based codes;BMS decoding approach;Berlekamp-Massey-Sakata algorithm;algebraic geometry;code family;error correcting codes;hardware implementation issues;Algebra;Communication channels;Concatenated codes;Decoding;Error correction;Galois fields;Geometry;Hardware;Linear code;Publishing},
  Timestamp                = {2014.03.25}
}

@Article{1624642,
  Title                    = {Upper bounds on the rate of LDPC codes as a function of minimum distance},
  Author                   = {Ben-Haim, Y. and Litsyn, S.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {may},
  Number                   = {5},
  Pages                    = { 2092 - 2100},
  Volume                   = {52},

  Abstract                 = { New upper bounds on the rate of low-density parity-check (LDPC) codes as a function of the minimum distance of the code are derived. The bounds apply to regular LDPC codes, and sometimes also to right-regular LDPC codes. Their derivation is based on combinatorial arguments and linear programming. The new bounds improve upon the previous bounds due to Burshtein et al. It is proved that at least for high rates, regular LDPC codes with full-rank parity-check matrices have worse relative minimum distance than the one guaranteed by the Gilbert-Varshamov bound.},
  Doi                      = {10.1109/TIT.2006.872972},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01624642.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { LDPC; combinatorial argument; linear programming; low-density parity-check code; minimum distance; combinatorial mathematics; linear programming; minimum principle; parity check codes;},
  Timestamp                = {2011.11.15}
}

@Article{springerlink:10.1007/BF02189233,
  Title                    = {Multicanonical recursions},
  Author                   = {Berg, Bernd},
  Journal                  = {Journal of Statistical Physics},
  Year                     = {1996},
  Note                     = {10.1007/BF02189233},
  Pages                    = {323-342},
  Volume                   = {82},

  Abstract                 = {The problem of calculating multicanonical parameters recursively is discussed. I describe in detail a computational implementation which has worked reasonably well in practice.},
  Affiliation              = {Florida State University Department of Physics 32306 Tallahassee FL 32306 Tallahassee FL},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\10.1007_BF02189233.pdf:PDF},
  ISSN                     = {0022-4715},
  Issue                    = {1},
  Keyword                  = {Physics and Astronomy},
  Publisher                = {Springer Netherlands},
  Timestamp                = {2012.03.08},
  Url                      = {http://dx.doi.org/10.1007/BF02189233}
}

@Article{Berg1998982,
  Title                    = {Algorithmic aspects of multicanonical simulations},
  Author                   = {Bernd A. Berg},
  Journal                  = {Nuclear Physics B - Proceedings Supplements},
  Year                     = {1998},
  Note                     = {Proceedings of the XVth International Symposium on Lattice Field Theory},
  Number                   = {3},
  Pages                    = {982 - 984},
  Volume                   = {63},

  Abstract                 = {Monte Carlo (MC) simulations of many systems, can be considerably speeded up by using multicanonical or related methods. I shall focus on two aspects: (i) Opinions about the optimal choice of weights. (ii) Recursive weight factor estimates.},
  Doi                      = {10.1016/S0920-5632(97)00962-6},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Berg1998982.pdf:PDF},
  ISSN                     = {0920-5632},
  Timestamp                = {2012.03.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0920563297009626}
}

@Article{PhysRevLett.69.2292,
  Title                    = {New approach to spin-glass simulations},
  Author                   = {Berg, Bernd A. and Celik, Tarik},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {1992},

  Month                    = {Oct},
  Pages                    = {2292--2295},
  Volume                   = {69},

  Doi                      = {10.1103/PhysRevLett.69.2292},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.69.2292.pdf:PDF},
  Issue                    = {15},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.03.09},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevLett.69.2292}
}

@Article{PhysRevB.47.497,
  Title                    = {Simulation of an ensemble with varying magnetic field: A numerical determination of the order-order interface tension in the \textit{D} =2 Ising model},
  Author                   = {Berg, B. A. and Hansmann, U. and Neuhaus, T.},
  Journal                  = {Phys. Rev. B},
  Year                     = {1993},

  Month                    = {Jan},
  Pages                    = {497--500},
  Volume                   = {47},

  Doi                      = {10.1103/PhysRevB.47.497},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\simulation_ensemble_varying_magnetic_field_numerical_determination_order_order_interface_tension_d2_Ising_model_1993.pdf:PDF},
  Issue                    = {1},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.03.26},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevB.47.497}
}

@Article{Berg1991249,
  Title                    = {Multicanonical algorithms for first order phase transitions},
  Author                   = {Bernd A. Berg and Thomas Neuhaus},
  Journal                  = {Physics Letters B},
  Year                     = {1991},
  Number                   = {2},
  Pages                    = {249 - 253},
  Volume                   = {267},

  Abstract                 = {Monte Carlo simulations are discussed for systems of volume V = Ld which undergo a first order phase transition in the finite volume limit. Conventional canonical, local Monte Carlo algorithms suffer from exponentially fast slowing down 竕・2 exp (cLd竏・). Here we present a class of multicanonical Monte Carlo algorithms which can reduce the slowing down to a quadratic power law 竕・2.},
  Doi                      = {10.1016/0370-2693(91)91256-U},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Berg1991249.pdf:PDF},
  ISSN                     = {0370-2693},
  Url                      = {http://www.sciencedirect.com/science/article/pii/037026939191256U}
}

@Article{1055088,
  Title                    = {Goppa codes},
  Author                   = { Berlekamp, E.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1973},

  Month                    = sep,
  Number                   = {5},
  Pages                    = { 590 - 592},
  Volume                   = {19},

  Abstract                 = { Goppa described a new class of linear noncyclic error-correcting codes in [1] and [2]. This paper is a summary of Goppa's work, which is not yet available in English.^1We prove the four most important properties of Goppa codes. 1) There existq-ary Goppa codes with lengths and redundancies comparable to BCH codes. For the same redundancy, the Goppa code is typically one digit longer. 2) All Goppa codes have an algebraic decoding algorithm which will correct up to a certain number of errors, comparable to half the designed distance of BCH codes. 3) For binary Goppa codes, the algebraic decoding algorithm assumes a special form. 4) Unlike primitive BCH codes, which are known to have actual distances asymptotically equal to their designed distances, long Goppa codes have actual minimum distances much greater than twice the number of errors, which are guaranteed to be correctable by the algebraic decoding algorithm. In fact, long irreducible Goppa codes asymptotically meet the Gilbert bound.},
  Doi                      = {10.1109/TIT.1973.1055088},
  File                     = {:PDF\\Goppa_Codes.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Goppa codes;}
}

@InProceedings{Berlekamp_NONBINARY_BCH_DECODING,
  Title                    = {NONBINARY BCH DECODING},
  Author                   = {E. R. Berlekamp},
  Booktitle                = {1967 International Symposium on Information Theory, San Remeo, Italy},
  Year                     = {1967},

  File                     = {:PDF\\Berlekamp_ISMS_1966_502.pdf:PDF}
}

@Article{bethe1935,
  Title                    = {Statistical Theory of Superlattices},
  Author                   = {H. A. Bethe},
  Journal                  = {Proceedings of The Royal Society London A},
  Year                     = {1935},

  Month                    = {July},
  Number                   = {871},
  Pages                    = {552-575},
  Volume                   = {150},

  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Proc. R. Soc. Lond. A-1935-Bethe-552-75.pdf:PDF},
  Timestamp                = {2014.06.23}
}

@Book{biggs:1993,
  Title                    = {Algebraic Graph Theory},
  Author                   = {Biggs, N.},
  Publisher                = {Cambridge University Press},
  Year                     = {1993},
  Edition                  = {2nd},

  Added-at                 = {2011-08-05T18:21:23.000+0200},
  Biburl                   = {http://www.bibsonomy.org/bibtex/24f5faaac03d4009ed8b489ef250ac235/ulpsch},
  Interhash                = {4c87ee829665dbb45d227f7643cc632a},
  Intrahash                = {4f5faaac03d4009ed8b489ef250ac235},
  Keywords                 = {d3 graphtheory},
  Timestamp                = {2011-08-05T18:21:23.000+0200}
}

@Book{Monte_Carlo_Simulation_in_Statistical_Physics,
  Title                    = {Monte Carlo Simulation in Statistical Physics: An Introduction},
  Author                   = {Kurt Binder and Dieter W. Heermann},
  Publisher                = {Springer Verlag},
  Year                     = {2010},
  Edition                  = {5th},
  Month                    = {aug},

  Abstract                 = {Monte Carlo Simulation in Statistical Physics deals with the computer simulation of many-body systems in condensed-matter physics and related fields of physics, chemistry and beyond, to traffic flows, stock market fluctuations, etc.). Using random numbers generated by a computer, probability distributions are calculated, allowing the estimation of the thermodynamic properties of various systems. This book describes the theoretical background to several variants of these Monte Carlo methods and gives a systematic presentation from which newcomers can learn to perform such simulations and to analyze their results. The fifth edition covers Classical as well as Quantum Monte Carlo methods. Furthermore a new chapter on the sampling of free-energy landscapes has been added. To help students in their work a special web server has been installed to host programs and discussion groups (http://wwwcp.tphys.uni-heidelberg.de). Prof. Binder was awarded the Berni J. Alder CECAM Award for Computational Physics 2001 as well as the Boltzmann Medal in 2007.},
  ISBN                     = {978-3-642-03162-5},
  Keywords                 = {Computational algorithms, Computer simulation and modeling of many-body systems, Sampling methods, Textbook on Monte Carlo Method, Textbook on statistical physics},
  Timestamp                = {2012.03.22},
  Url                      = {http://www.springer.com/physics/theoretical%2C+mathematical+%26+computational+physics/book/978-3-642-03162-5}
}

@Book{bishop2006pattern,
  Title                    = {Pattern recognition and machine learning},
  Author                   = {Bishop, Christopher M},
  Publisher                = {springer},
  Year                     = {2006},

  Timestamp                = {2015.07.30}
}

@Article{720550,
  Title                    = {Algebraic-geometry codes},
  Author                   = {Blake, I. and Heegard, C. and Hoholdt, T. and Wei, V.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1998},

  Month                    = oct,
  Number                   = {6},
  Pages                    = {2596 -2618},
  Volume                   = {44},

  Abstract                 = {The theory of error-correcting codes derived from curves in an algebraic geometry was initiated by the work of Goppa as generalizations of Bose-Chaudhuri-Hocquenghem (BCH), Reed-Solomon (RS), and Goppa codes. The development of the theory has received intense consideration since that time and the purpose of the paper is to review this work. Elements of the theory of algebraic curves, at a level sufficient to understand the code constructions and decoding algorithms, are introduced. Code constructions from particular classes of curves, including the Klein quartic, elliptic, and hyperelliptic curves, and Hermitian curves, are presented. Decoding algorithms for these classes of codes, and others, are considered. The construction of classes of asymptotically good codes using modular curves is also discussed},
  Doi                      = {10.1109/18.720550},
  File                     = {:NMP\\ユニット管理\\refereces\\PDF\\00720550.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {BCH code;Bose-Chaudhuri-Hocquenghem codes;Goppa codes;Hermitian curves;Klein quartic curves;RS codes;Reed-Solomon codes;algebraic curves;algebraic-geometry codes;asymptotically good codes;code constructions;decoding algorithms;elliptic curves;error-correcting codes;hyperelliptic curves;modular curves;review;BCH codes;Goppa codes;Reed-Solomon codes;algebraic geometric codes;decoding;error correction codes;reviews;}
}

@Article{Blake19791,
  Title                    = {Coding with permutations },
  Author                   = {Ian F. Blake and Gerard Cohen and Mikhail Deza},
  Journal                  = {Information and Control },
  Year                     = {1979},
  Number                   = {1},
  Pages                    = {1 - 19},
  Volume                   = {43},

  Doi                      = {http://dx.doi.org/10.1016/S0019-9958(79)90076-7},
  File                     = {:PDF\\1-s2.0-S0019995879900767-main.pdf:PDF},
  ISSN                     = {0019-9958},
  Timestamp                = {2015.04.01},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0019995879900767}
}

@Article{987093,
  Title                    = {A 690-mW 1-Gb/s 1024-b, rate-1/2 low-density parity-check code decoder},
  Author                   = {Blanksby, A.J. and Howland, C.J.},
  Journal                  = {Solid-State Circuits, IEEE Journal of},
  Year                     = {2002},

  Month                    = {mar},
  Number                   = {3},
  Pages                    = {404 -412},
  Volume                   = {37},

  Abstract                 = {A 1024-b, rate-1/2, soft decision low-density parity-check (LDPC) code decoder has been implemented that matches the coding gain of equivalent turbo codes. The decoder features a parallel architecture that supports a maximum throughput of 1 Gb/s while performing 64 decoder iterations. The parallel architecture enables rapid convergence in the decoding algorithm to be translated into low decoder switching activity resulting in a power dissipation of only 690 mW from a 1.5-V supply},
  Doi                      = {10.1109/4.987093},
  File                     = {:PDF\\00987093.pdf:PDF},
  ISSN                     = {0018-9200},
  Keywords                 = {1 Gbit/s;1.5 V;1024 bit;690 mW;CMOS digital integrated circuits;coding gain;decoder iterations;decoder switching activity;decoding algorithm;low-density parity-check code decoder;parallel architecture;power dissipation;soft decision;throughput;CMOS digital integrated circuits;iterative decoding;parallel architectures;},
  Timestamp                = {2011.04.22}
}

@Article{Bolthausen2014,
  Title                    = {An Iterative Construction of Solutions of the TAP Equations for the Sherrington-Kirkpatrick Model},
  Author                   = {Bolthausen, Erwin},
  Journal                  = {Communications in Mathematical Physics},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {333-366},
  Volume                   = {325},

  Doi                      = {10.1007/s00220-013-1862-3},
  File                     = {:PDF\\10.1007Fs00220-013-1862-3.pdf:PDF},
  ISSN                     = {0010-3616},
  Language                 = {English},
  Publisher                = {Springer Berlin Heidelberg},
  Timestamp                = {2015.02.16},
  Url                      = {http://dx.doi.org/10.1007/s00220-013-1862-3}
}

@InProceedings{4035991,
  Title                    = {Upper Bounds on the Error Exponents of LDPC Code Ensembles},
  Author                   = {Burshtein, D. and Barak, O.},
  Booktitle                = {Information Theory, 2006 IEEE International Symposium on},
  Year                     = {2006},
  Month                    = {july},
  Pages                    = {401 -405},

  Abstract                 = {We consider the ensemble of regular LDPC codes and use recent concentration results on the distance spectrum to derive upper bounds on the error exponent of a randomly chosen code from the ensemble. These bounds hold with some confidence level that approaches one as the connectivity of the graph increases. We show that the bounds can be used to obtain the true error exponent over some range of channel parameter values, with the above confidence level},
  Doi                      = {10.1109/ISIT.2006.261699},
  File                     = {:\\\\homepd\\pd6\\ユニット管理\\refereces\\PDF\\04035991.pdf:PDF},
  Keywords                 = {LDPC code ensembles;channel parameter values;distance spectrum;error exponents;channel coding;parity check codes;random codes;},
  Timestamp                = {2011.11.17}
}

@InProceedings{5592887,
  Title                    = {Improved linear programming decoding and bounds on the minimum distance of LDPC codes},
  Author                   = {Burshtein, D. and Goldenberg, I.},
  Booktitle                = {Information Theory Workshop (ITW), 2010 IEEE},
  Year                     = {2010},
  Month                    = {302010-sept.3},
  Pages                    = {1 -5},

  Abstract                 = {We propose a technique for improving LP decoding, based on the merging of check nodes. This technique can be applied to standard as well as generalized LDPC codes. Furthermore, we show how a recently-discovered linear-complexity LP decoder can be used to derive non-trivial lower bounds on the minimum distance of specific LDPC codes, with complexity that exhibits quadratic growth with respect to the block length. This bound can be refined using the check node merging technique. The lower bound on the minimum distance is shown to be an upper bound on the fractional distance of the code.},
  Doi                      = {10.1109/CIG.2010.5592887},
  File                     = {:PDF\\Improved_Linear_Programming_Decoding_and_Bounds_on_the_Minimum_Distance_of_LDPC_Codes.pdf:PDF},
  Keywords                 = {LDPC codes;check node merging technique;improved linear programming decoding;linear complexity LP decoder;low-density parity check codes;linear codes;linear programming;parity check codes;}
}

@Article{6491477,
  Title                    = {Polar Write Once Memory Codes},
  Author                   = {Burshtein, D. and Strugatski, A},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {5088-5101},
  Volume                   = {59},

  Abstract                 = {A coding scheme for write once memory (WOM) using polar codes is presented. It is shown that the scheme achieves the capacity region of noiseless WOMs when an arbitrary number of multiple writes is permitted. The encoding and decoding complexities scale as O(N log N), where N is the blocklength. For N sufficiently large, the error probability decreases subexponentially in N. The results can be generalized from binary to generalized WOMs, described by an arbitrary directed acyclic graph, using nonbinary polar codes. In the derivation, we also obtain results on the typical distortion of polar codes for lossy source coding. Some simulation results with finite length codes are presented.},
  Doi                      = {10.1109/TIT.2013.2255732},
  File                     = {:PDF\\06491477.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {communication complexity;decoding;encoding;error statistics;arbitrary directed acyclic graph;decoding complexities scale;encoding complexities scale;error probability;finite length codes;lossy source coding;noiseless WOM;nonbinary polar codes;polar write once memory codes;Polar codes;write once memory codes (WOMs)},
  Timestamp                = {2014.08.26}
}

@Article{6928457,
  Title                    = {Error Floor Approximation for LDPC Codes in the AWGN Channel},
  Author                   = {Butler, B.K. and Siegel, P.H.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {7416-7441},
  Volume                   = {60},

  Abstract                 = {This paper addresses the prediction of error floors of low-density parity-check codes transmitted over the additive white Gaussian noise channel. Using a linear state-space model to estimate the behavior of the sum-product algorithm (SPA) decoder in the vicinity of trapping sets (TSs), we study the performance of the SPA decoder in the log-likelihood ratio (LLR) domain as a function of the LLR saturation level. When applied to several widely studied codes, the model accurately predicts a significant decrease in the error floor as the saturation level is allowed to increase. For nonsaturating decoders, however, we find that the state-space model breaks down after a small number of iterations due to the strong correlation of LLR messages. We then revisit Richardson's importance-sampling methodology for estimating error floors due to TSs when those floors are too low for Monte Carlo simulation. We propose modifications that account for the behavior of a nonsaturating decoder and present the resulting error floor estimates for the Margulis code. These estimates are much lower, significantly steeper, and more sensitive to iteration count than those previously reported.},
  Doi                      = {10.1109/TIT.2014.2363832},
  File                     = {:PDF\\06928457.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {AWGN channels;Monte Carlo methods;approximation theory;channel coding;channel estimation;decoding;error correction codes;iterative methods;parity check codes;sampling methods;AWGN channel;LDPC codes;LLR saturation level;Margulis code;Monte Carlo simulation;Richardson importance-sampling methodology;SPA decoder;additive white Gaussian noise channel;error floor approximation;linear state-space model;log-likelihood ratio;low-density parity-check codes;sum-product algorithm;trapping sets;AWGN channels;Approximation methods;Decoding;Error analysis;Iterative decoding;State-space methods;Absorbing set;Margulis code;belief propagation (BP);error floor;linear analysis;low-density parity-check (LDPC) code;near-codeword;sum-product algorithm (SPA) decoding;trapping set},
  Timestamp                = {2015.01.09}
}

@InProceedings{6875261,
  Title                    = {Systematic codes for rank modulation},
  Author                   = {Buzaglo, S. and Yaakobi, E. and Etzion, T. and Bruck, J.},
  Booktitle                = {Information Theory (ISIT), 2014 IEEE International Symposium on},
  Year                     = {2014},
  Month                    = {June},
  Pages                    = {2386-2390},

  Abstract                 = {The goal of this paper is to construct systematic error-correcting codes for permutations and multi-permutations in the Kendall's τ-metric. These codes are important in new applications such as rank modulation for flash memories. The construction is based on error-correcting codes for multi-permutations and a partition of the set of permutations into error-correcting codes. For a given large enough number of information symbols k, and for any integer t, we present a construction for (k + r, k) systematic t-error-correcting codes, for permutations from Sk+r, with less redundancy symbols than the number of redundancy symbols in the codes of the known constructions. In particular, for a given t and for sufficiently large k we can obtain r = t+1. The same construction is also applied to obtain related systematic error-correcting codes for multi-permutations.},
  Doi                      = {10.1109/ISIT.2014.6875261},
  File                     = {:PDF\\06875261.pdf:PDF},
  Keywords                 = {error correction codes;flash memories;Kendall's τ-metric;flash memories;multipermutations;rank modulation;systematic error-correcting codes;Error correction codes;Measurement;Modulation;Redundancy;Systematics;Tin},
  Timestamp                = {2014.09.03}
}

@InProceedings{byers2004fourier,
  Title                    = {Fourier transform decoding of non-binary LDPC codes},
  Author                   = {Byers, Geoffrey J and Takawira, Fambirai},
  Booktitle                = {Proceedings Southern African Telecommunication Networks and Applications Conference},
  Year                     = {2004},

  File                     = {:PDF\\0fcfd508787b56a933000000.pdf:PDF},
  Timestamp                = {2015.05.14}
}

@InProceedings{6176524,
  Title                    = {Error patterns in MLC NAND flash memory: Measurement, characterization, and analysis},
  Author                   = {Yu Cai and Haratsch, E.F. and Mutlu, O. and Ken Mai},
  Booktitle                = {Design, Automation Test in Europe Conference Exhibition (DATE), 2012},
  Year                     = {2012},
  Month                    = {March},
  Pages                    = {521-526},

  Abstract                 = {As NAND flash memory manufacturers scale down to smaller process technology nodes and store more bits per cell, reliability and endurance of flash memory reduce. Wear-leveling and error correction coding can improve both reliability and endurance, but finding effective algorithms requires a strong understanding of flash memory error patterns. To enable such understanding, we have designed and implemented a framework for fast and accurate characterization of flash memory throughout its lifetime. This paper examines the complex flash errors that occur at 30-40nm flash technologies. We demonstrate distinct error patterns, such as cycle-dependency, location-dependency and value-dependency, for various types of flash operations. We analyze the discovered error patterns and explain why they exist from a circuit and device standpoint. Our hope is that the understanding developed from this characterization serves as a building block for new error tolerance algorithms for flash memory.},
  Doi                      = {10.1109/DATE.2012.6176524},
  File                     = {:PDF\\06176524.pdf:PDF},
  ISSN                     = {1530-1591},
  Keywords                 = {NAND circuits;circuit reliability;error correction codes;flash memories;wear;MLC NAND flash memory error pattern;cycle-dependency error pattern;error correction coding;error tolerance algorithm;flash memory endurance reduction;flash memory reliability reduction;location-dependency error pattern;multilevel cell NAND flash memory error pattern;process technology node;size 30 nm to 40 nm;value-dependency error pattern;wear-leveling;Electric fields;Error analysis;Flash memory;Interference;Logic gates;Programming;Threshold voltage;NAND flash;endurance;error correction;error patterns;reliability},
  Timestamp                = {2015.06.16}
}

@InProceedings{6378623,
  Title                    = {Flash correct-and-refresh: Retention-aware error management for increased flash memory lifetime},
  Author                   = {Yu Cai and Yalcin, G. and Mutlu, O. and Haratsch, E.F. and Cristal, A. and Unsal, O.S. and Ken Mai},
  Booktitle                = {Computer Design (ICCD), 2012 IEEE 30th International Conference on},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {94-101},

  Abstract                 = {With the continued scaling of NAND flash and multi-level cell technology, flash-based storage has gained widespread use in systems ranging from mobile platforms to enterprise servers. However, the robustness of NAND flash cells is an increasing concern, especially at nanometer-regime process geometries. NAND flash memory bit error rate increases exponentially with the number of program/erase cycles. Stronger error correcting codes (ECC) can be used to tolerate higher error rates, but these have diminishing returns with increasing P/E cycles and can have prohibitively high power, area, and latency overheads. The goal of this paper is to develop new techniques that can tolerate high bit error rates without requiring prohibitively strong ECC. Our techniques, called Flash Correct-and-Refresh (FCR) exploit the observation that the dominant error source in NAND flash memory is retention errors, caused by flash cells losing charge over time. The key idea is to periodically read, correct, and reprogram (in-place) or remap the stored data before it accumulates more retention errors than can be corrected by simple ECC. Detailed simulations of a solid-state drive (SSD) storage system driven by measured experimental data from error characterization on real flash memory chips show that our techniques provide 46× average lifetime improvement on a variety of workloads at no additional hardware cost. We also find that our techniques achieve lifetime improvements that cannot feasibly be achieved with stronger ECC.},
  Doi                      = {10.1109/ICCD.2012.6378623},
  File                     = {:PDF\\06378623.pdf:PDF},
  ISSN                     = {1063-6404},
  Keywords                 = {NAND circuits;error correction codes;error statistics;flash memories;multivalued logic circuits;ECC;FCR;NAND flash memory;P-E cycle;SSD storage system;bit error rate;dominant error source;enterprise server;error correcting codes;flash correct and refresh;flash memory chip;flash memory lifetime;flash-based storage;mobile platform;multilevel cell technology;nanometer regime process geometry;program-erase cycle;retention aware error management;retention error;solid-state drive;Bit error rate;Error correction codes;Flash memory;Nonvolatile memory;Programming;Threshold voltage;NAND Flash;error correction;multi-level cell (MLC);reliability},
  Timestamp                = {2015.06.10}
}

@InProceedings{1216751,
  Title                    = {A new data compression algorithm for sources with memory based on error correcting codes},
  Author                   = {Caire, G. and Shamai, S. and Verdu, S.},
  Booktitle                = {Information Theory Workshop, 2003. Proceedings. 2003 IEEE},
  Year                     = {2003},
  Month                    = {march-4 april},
  Pages                    = { 291 - 295},

  Abstract                 = { A new fixed-length asymptotically optimal scheme for lossless compression of stationary ergodic tree sources with memory is proposed. Our scheme is based on the concatenation of the Burrows-Wheeler block sorting transform with the syndrome former of a linear error correcting code. Low-density parity-check (LDPC) codes together with belief propagation decoding lead to linear compression and decompression times, and to natural universal implementation of the algorithm.},
  Doi                      = {10.1109/ITW.2003.1216751},
  File                     = {:PDF\\01216751.pdf:PDF},
  Keywords                 = { Burrows-Wheeler block sorting transform; LDPC codes; belief propagation decoding; block error rate; concatenation; data compression; linear code; linear error correcting codes; low-density parity-check codes; memory; stationary ergodic tree sources; syndrome former; decoding; error correction codes; linear codes; parity check codes; source coding; transform coding; trees (mathematics);},
  Timestamp                = {2011.06.08}
}

@InProceedings{carreira2005contrastive,
  Title                    = {On contrastive divergence learning},
  Author                   = {Carreira-Perpinan, Miguel A and Hinton, Geoffrey E},
  Booktitle                = {Proceedings of the tenth international workshop on artificial intelligence and statistics},
  Year                     = {2005},
  Organization             = {Citeseer},
  Pages                    = {33--40},

  File                     = {:PDF\\10.1.1.221.8829.pdf:PDF},
  Timestamp                = {2015.07.30}
}

@InProceedings{1651870,
  Title                    = {A performance improvement and error floor avoidance technique for belief propagation decoding of LDPC codes},
  Author                   = {Cavus, E. and Daneshrad, B.},
  Booktitle                = {Personal, Indoor and Mobile Radio Communications, 2005. PIMRC 2005. IEEE 16th International Symposium on},
  Year                     = {2005},
  Pages                    = {2386-2390 Vol. 4},
  Volume                   = {4},

  Abstract                 = {In this work, we introduce a unique technique that improves the performance of the BP decoding in waterfall and error-floor regions by reversing the decoder failures. Based on the short cycles existing in the bipartite graph, an importance sampling simulation technique is used to identify the bit and check node combinations that are the dominant sources of error events, called trapping sets. Then, the identified trapping sets are used in the decoding process to avoid the pre-known failures and to converge to the transmitted codeword. With a minimal additional decoding complexity, the proposed technique is able to provide performance improvements for short-length LDPC codes and push or avoid error-floor behaviors of longer codes},
  Doi                      = {10.1109/PIMRC.2005.1651870},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01651870.pdf:PDF},
  Keywords                 = {decoding;graph theory;importance sampling;parity check codes;belief propagation decoding;bipartite graph;bit-check node combination;error floor avoidance technique;importance sampling simulation technique;low density parity check;short-length LDPC code;trapping set;Belief propagation;Bipartite graph;Bridges;Discrete event simulation;Floors;Maximum likelihood decoding;Monte Carlo methods;Parity check codes;Performance loss;Signal processing algorithms},
  Timestamp                = {2013.12.20}
}

@Article{1054291,
  Title                    = {Rank permutation group codes based on Kendall's correlation statistic},
  Author                   = {Chadwick, H. and Kurz, L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1969},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {306-315},
  Volume                   = {15},

  Abstract                 = {A coding scheme based on the properties of rank vectors is presented. The new codes are based on the theory of permutation groups by introducing a new notation for the group operation that simplifies the generation and decoding of desirable rank codes. The use of group theory is made possible by the introduction of the Kendall correlation coefficient as a measure of the distance between code words. This technique provides a method for the choice of rank vector code words superior to those that have been proposed in the past. Much of the terminology used in block coding can also be used to describe rank vector codes, but the actual quantities involved are quite different. The rank vector codes discussed in the paper offer the advantage of low sensitivity of the probability of error to the noise distribution because of the nonparametric character of rank vector detection schemes. Bounds that have been verified by extensive computer simulation have been derived for the probability of error.},
  Doi                      = {10.1109/TIT.1969.1054291},
  File                     = {:PDF\\01054291.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Permutation codes;Block codes;Computer errors;Decoding;Delay;Detectors;Genetic mutations;Information theory;Noise generators;Signal detection;Statistics},
  Timestamp                = {2015.04.01}
}

@InProceedings{1031604,
  Title                    = {A low-power Reed-Solomon decoder for STM-16 optical communications},
  Author                   = {Hsie-Chia Chang and Chien-Ching Lin and Chen-Yi Lee},
  Year                     = {2002},
  Pages                    = { 351 - 354},

  Doi                      = {10.1109/APASIC.2002.1031604},
  ISSN                     = { },
  Journal                  = {ASIC, 2002. Proceedings. 2002 IEEE Asia-Pacific Conference on},
  Keywords                 = { 0.25 micron; 2 kbit; 2.5 Gbit/s; CMOS 1P5M standard cells; Chien search circuit terminated mechanism; RS decoder; STM-16 optical communications; data rate; embedded memory; gate counts; key equation solver; low-power Reed-Solomon decoder; modified Berlekamp-Massey algorithm; power dissipation; received codewords; simulation; syndrome calculator; syndrome computation; CMOS integrated circuits; Reed-Solomon codes; circuit simulation; decoding; error correction codes; low-power electronics; optical communication equipment;}
}

@Article{new_serial_BM,
  Title                    = {New Serial Architecture for the Berlekamp-Massey Algorithm},
  Author                   = {Hsic-Chia Chang and Bernard Shung},
  Journal                  = {IEEE Transactions on Communications},
  Year                     = {1999},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {481-483},
  Volume                   = {47},

  File                     = {:PDF\\New_Serial_Architecture_for_the_Berlekamp-Massey_Algorithm.pdf:PDF}
}

@InProceedings{5872253,
  Title                    = {Physical mechanism of HfO2-based bipolar resistive random access memory},
  Author                   = {Huan-Lin Chang and Hsuan-Chih Li and Liu, C.W. and Chen, F. and Tsai, M.-J.},
  Booktitle                = {VLSI Technology, Systems and Applications (VLSI-TSA), 2011 International Symposium on},
  Year                     = {2011},
  Month                    = {april},
  Pages                    = {1 -2},

  Abstract                 = {The (anode) TiN/Ti/HfO2/TiN (cathode) resistive random access memory (RRAM) has shown yield ~100%. Its simple metal-insulator-metal (MIM) structure exhibits great potential for an embedded BEOL memory compatible with the high-k/metal gate CMOS process. There have been many theories of RRAM physical mechanism in the literature. This paper focuses on HfO2-based RRAM and describes a complete physical mechanism from forming, SET/RESET, current conduction, to explanations of various observed phenomena including multilevel, cell size scaling, resistance fluctuation, soft error, and non-abrupt RESRT process. Finally, suggestions for device optimization are given based on the physical model.},
  Doi                      = {10.1109/VTSA.2011.5872253},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05872253.pdf:PDF},
  ISSN                     = {1930-8868},
  Keywords                 = {HfO2-based bipolar resistive random access memory;RRAM;SET/RESET;TiN-Ti-HfO2-TiN;current conduction;device optimization;embedded BEOL memory;high-k/metal gate CMOS process;metal-insulator-metal structure;CMOS integrated circuits;MIM structures;bipolar memory circuits;hafnium compounds;random-access storage;titanium compounds;},
  Timestamp                = {2011.08.28}
}

@Article{5483292,
  Title                    = {A Management Strategy for the Reliability and Performance Improvement of MLC-Based Flash-Memory Storage Systems},
  Author                   = {Yuan-Hao Chang and Tei-Wei Kuo},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {2011},

  Month                    = march,
  Number                   = {3},
  Pages                    = {305 -320},
  Volume                   = {60},

  Abstract                 = {Cost has been a major driving force in the development of the flash-memory technology. Because of this, serious challenges are now faced for future products on reliability and performance requirements. In this work, we propose a management strategy to resolve the reliability and performance problems of many flash-memory products. A three-level address translation architecture with an adaptive block mapping mechanism is proposed to accelerate the address translation process with a limited amount of the RAM usage. Parallelism of operations over multiple chips is also explored with the considerations of the write constraints of advanced multilevel cell flash-memory chips. The capability of the proposed approach is analyzed with reliability considerations and evaluated by experiments over realistic workloads with respect to the reliability and performance improvement.},
  Doi                      = {10.1109/TC.2010.126},
  File                     = {:PDF\\A_Management_Strategy_for_the_Reliability_and_Performance_Improvement_of_MLC-Based_Flash-Memory_Storage_Systems.pdf:PDF},
  ISSN                     = {0018-9340},
  Keywords                 = {MLC based flash memory storage system;RAM;adaptive block mapping mechanism;advanced multilevel cell flash-memory chips;storage management strategy;storage system reliability;three-level address translation architecture;circuit reliability;flash memories;memory architecture;random-access storage;storage management chips;}
}

@InProceedings{5513621,
  Title                    = {Two-dimensional generalized Reed-Solomon codes: A unified framework for quasi-cyclic LDPC codes constructed based on finite fields},
  Author                   = {Chao Chen and Baoming Bai and Xinmei Wang},
  Booktitle                = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium on},
  Year                     = {2010},
  Month                    = june,
  Pages                    = {839 -843},

  Abstract                 = {In this paper, we first propose a general framework for constructing quasi-cyclic low-density parity-check (QC-LDPC) codes based on a two-dimensional (2-D) maximum distance separable (MDS) code. Two classes of QC-LDPC codes are defined, whose parity-check matrices are transposes of each other. We then use a 2-D generalized Reed-Solomon (GRS) code to give a concrete construction. The decoding parity-check matrices have a large number of redundant parity-check equations while their Tanner graphs have a girth of at least 6. The minimum distances of the codes are very respectable as far as LDPC codes are concerned. We further show that many existing constructions of QC-LDPC codes based on finite fields in the literature can be unified under this construction. Experimental studies show that the constructed QC-LDPC codes perform well with the sum-product algorithm (SPA).},
  Doi                      = {10.1109/ISIT.2010.5513621},
  File                     = {:PDF\\Two-Dimentional_Generalaized_Reed-Solomon_Codes_A_Unified_Framework_for_Quasi-Cyclic_LDPC_Codes_Constructed_Based_on_Finite_Field.pdf:PDF},
  Keywords                 = {decoding;finite fields;generalized Reed-Solomon code;low-density parity-check;maximum distance separable code;parity-check matrices;quasi-cyclic LDPC codes;sum-product algorithm;Reed-Solomon codes;decoding;parity check codes;}
}

@Article{5582319,
  Title                    = {Two Low-Complexity Reliability-Based Message-Passing Algorithms for Decoding Non-Binary LDPC Codes},
  Author                   = {Chao-Yu Chen and Qin Huang and Chi-chao Chao and Shu Lin},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {November},
  Number                   = {11},
  Pages                    = {3140-3147},
  Volume                   = {58},

  Abstract                 = {This paper presents two low-complexity reliability-based message-passing algorithms for decoding LDPC codes over non-binary finite fields. These two decoding algorithms require only finite field and integer operations and they provide effective trade-off between error performance and decoding complexity compared to the non-binary sum product algorithm. They are particularly effective for decoding LDPC codes constructed based on finite geometries and finite fields.},
  Doi                      = {10.1109/TCOMM.2010.091310.090327},
  File                     = {:PDF\\05582319.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {decoding;message passing;parity check codes;telecommunication computing;telecommunication network reliability;decoding nonbinary LDPC codes;finite fields;finite geometries;integer operations;low-complexity reliability-based message passing algorithms;nonbinary sum product algorithm;Complexity theory;Decoding;Geometry;Iterative decoding;Reliability;Variable speed drives;Reliability-based message-passing algorithms;fast Fourier transform q-ary sum-product algorithm;non-binary LDPC codes},
  Timestamp                = {2015.04.21}
}

@Article{5467394,
  Title                    = {Advances and Future Prospects of Spin-Transfer Torque Random Access Memory},
  Author                   = {Chen, E. and Apalkov, D. and Diao, Z. and Driskill-Smith, A. and Druist, D. and Lottis, D. and Nikitin, V. and Tang, X. and Watts, S. and Wang, S. and Wolf, S.A. and Ghosh, A.W. and Lu, J.W. and Poon, S.J. and Stan, M. and Butler, W.H. and Gupta, S. and Mewes, C. and Mewes, T. and Visscher, P.B.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2010},

  Month                    = june,
  Number                   = {6},
  Pages                    = {1873 -1878},
  Volume                   = {46},

  Abstract                 = {Spin-transfer torque random access memory (STT-RAM) is a potentially revolutionary universal memory technology that combines the capacity and cost benefits of DRAM, the fast read and write performance of SRAM, the non-volatility of Flash, and essentially unlimited endurance. In order to realize a small cell size, high speed and achieve a fully functional STT-RAM chip, the MgO-barrier magnetic tunnel junctions (MTJ) used as the core storage and readout element must meet a set of performance requirements on switching current density, voltage, magneto-resistance ratio (MR), resistance-area product (RA), thermal stability factor (Â¿) , switching current distribution, read resistance distribution and reliability. In this paper, we report the progress of our work on device design, material improvement, wafer processing, integration with CMOS, and testing for a demonstration STT-RAM test chip, and projections based on modeling of the future characteristics of STT-RAM.},
  Doi                      = {10.1109/TMAG.2010.2042041},
  File                     = {:PDF\\Advances_and_Future_Prospects_of_Spin-Transfer_Torque_Random_Access_Memory.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = {CMOS integration;device design;material improvement;spin-transfer torque random access memory;wafer processing;CMOS integrated circuits;random-access storage;}
}

@Article{1001666,
  Title                    = {Density evolution for two improved BP-Based decoding algorithms of LDPC codes},
  Author                   = {Chen, J. and Fossorier, M.P.C.},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2002},

  Month                    = may,
  Number                   = {5},
  Pages                    = {208 -210},
  Volume                   = {6},

  Abstract                 = {In this letter, we analyze the performance of two improved belief propagation (BP) based decoding algorithms for LDPC codes, namely the normalized BP-based and the offset BP-based algorithms, by means of density evolution. The numerical calculations show that with one properly chosen parameter for each of these two improved BP-based algorithms, performances very close to that of the BP algorithm can be achieved. Simulation results for LDPC codes with code length moderately long validate the proposed optimization},
  Doi                      = {10.1109/4234.1001666},
  File                     = {:PDF\\Density_Evolution_for_Two_Improved_BP-Based_Decoding_Algorithms_of_LDPC_Codes.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = {LDPC codes;belief propagation based decoding algorithms;density evolution;iterative decoding;low-density parity-check codes;normalized BP-based algorithms;numerical calculations;offset BP-based algorithms;AWGN channels;belief maintenance;block codes;iterative decoding;}
}

@Article{1327849,
  Title                    = {A serial-in-serial-out hardware architecture for systematic encoding of Hermitian codes via Gr ouml;bner bases},
  Author                   = {Jia-Ping Chen and Chung-Chin Lu},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {aug},
  Number                   = {8},
  Pages                    = { 1322 - 1332},
  Volume                   = {52},

  Abstract                 = {When a nontrivial permutation of a Hermitian code is given, the code will have a module structure over a polynomial ring of one variable. By exploiting the theory of Gr ouml;bner bases for modules, a novel and elegant systematic encoding scheme for Hermitian codes is proposed by Heegard et al. (1995). The goal of this paper is to develop a serial-in-serial-out hardware architecture, similar to a classical cyclic encoder, for such a systematic encoding scheme. Moreover, we demonstrate that under a specific permutation, the upper bounds of the numbers of memory elements and constant multipliers in the proposed architecture are both proportional to O(n), where n is the length of the Hermitian code. To encode a codeword of length n, this architecture takes n clock cycles without any latency. Therefore, the hardware complexity of the proposed architecture is much less than that of the brute-force systematic encoding by matrix multiplication.},
  Doi                      = {10.1109/TCOMM.2004.833020},
  File                     = {:PDF\\01327849.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = { Grobner bases; Hermitian codes; Reed-Solomon code; algebraic geometry code; brute-force systematic encoding; cyclic encoder; matrix multiplication; module structure; permutation; polynomial ring; serial-in serial-out hardware architecture; systematic encoding; Reed-Solomon codes; algebraic geometric codes; matrix multiplication;},
  Timestamp                = {2011.04.27}
}

@Article{1315899,
  Title                    = {Near-Shannon-limit quasi-cyclic low-density parity-check codes},
  Author                   = {Lei Chen and Jun Xu and Djurdjevic, I. and Lin, S.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {july},
  Number                   = {7},
  Pages                    = { 1038 - 1042},
  Volume                   = {52},

  Abstract                 = { This letter presents two classes of quasi-cyclic low-density parity-check codes that perform close to the Shannon limit.},
  Doi                      = {10.1109/TCOMM.2004.831353},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01315899.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = { algebraic method; array decomposition; iterative decoding; low-density parity-check codes; near-Shannon-limit codes; quasi-cyclic codes; random codes; algebraic geometric codes; cyclic codes; iterative decoding; parity check codes; random codes;},
  Timestamp                = {2011.12.06}
}

@InProceedings{6804242,
  Title                    = {Increasing flash memory lifetime by dynamic voltage allocation for constant mutual information},
  Author                   = {Tsung-Yi Chen and Williamson, A.R. and Wesel, R.D.},
  Booktitle                = {Information Theory and Applications Workshop (ITA), 2014},
  Year                     = {2014},
  Month                    = {Feb},
  Pages                    = {1-5},

  Abstract                 = {The read channel in Flash memory systems degrades over time because the Fowler-Nordheim tunneling used to apply charge to the floating gate eventually compromises the integrity of the cell because of tunnel oxide degradation. While degradation is commonly measured in the number of program/erase cycles experienced by a cell, the degradation is proportional to the number of electrons forced into the floating gate and later released by the erasing process. By managing the amount of charge written to the floating gate to maintain a constant read-channel mutual information, Flash lifetime can be extended. This paper proposes an overall system approach based on information theory to extend the lifetime of a flash memory device. Using the instantaneous storage capacity of a noisy flash memory channel, our approach allocates the read voltage of flash cell dynamically as it wears out gradually over time. A practical estimation of the instantaneous capacity is also proposed based on soft information via multiple reads of the memory cells.},
  Doi                      = {10.1109/ITA.2014.6804242},
  File                     = {:PDF\\06804242.pdf:PDF},
  Keywords                 = {flash memories;Fowler-Nordheim tunneling;cell integrity;constant mutual information;dynamic voltage allocation;erasing process;flash memory lifetime;floating gate;memory cells;noisy flash memory channel;program-erase cycles;read-channel mutual information;soft information;tunnel oxide degradation;Ash;Flash memories;Histograms;Logic gates;Mutual information;Noise;Threshold voltage},
  Timestamp                = {2015.06.10}
}

@Article{5954141,
  Title                    = {Hardware Implementation of a Backtracking-Based Reconfigurable Decoder for Lowering the Error Floor of Quasi-Cyclic LDPC Codes},
  Author                   = {Xiaoheng Chen and Jingyu Kang and Shu Lin and Akella, V.},
  Journal                  = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {dec. },
  Number                   = {12},
  Pages                    = {2931 -2943},
  Volume                   = {58},

  Abstract                 = {Emerging applications such as flash-based storage systems and 10 gigabit Ethernet require that there is no error floor even at bit error rates as low as 10-12 or so. It has been found that trapping sets are responsible for the error floors of many LDPC codes with AWGN channels. This paper presents a hardware based backtracking scheme to break the trapping sets at runtime for lowering the error floor of quasi-cyclic LDPC codes. Backtracking is implemented as a self-contained module that can be interfaced to any generic reconfigurable iterative decoder for QC-LDPC codes. The backtracking module and a reconfigurable decoder are implemented with a FPGA and an 180 nm standard cell library. The results indicate that the overhead of backtracking is modest - about 5% in terms of logic and 13% in terms of memory for the first level backtracking and 14% in terms of logic and 46% in terms of memory for a two-level backtracking scheme. Furthermore, it is shown that the increase in latency due to backtracking is modest in the average case and can be controlled by the system designer by choosing the appropriate values for the number of trials and the number of iterations of the backtracking module.},
  Doi                      = {10.1109/TCSI.2011.2158712},
  File                     = {:\\\\homepd\\pd6u\\ユニット管理\\refereces\\PDF\\05954141.pdf:PDF},
  ISSN                     = {1549-8328},
  Keywords                 = {AWGN channels;Ethernet;FPGA;backtracking module;bit error rates;error floor;flash-based storage systems;iteration number;quasicyclic LDPC codes;reconfigurable decoder;reconfigurable iterative decoder;self-contained module;size 180 nm;standard cell library;AWGN channels;backtracking;cyclic codes;error statistics;field programmable gate arrays;iterative decoding;local area networks;parity check codes;},
  Timestamp                = {2012.03.29}
}

@Article{5982105,
  Title                    = {Efficient Configurable Decoder Architecture for Nonbinary Quasi-Cyclic LDPC Codes},
  Author                   = {Xiaoheng Chen and Shu Lin and Akella, V.},
  Journal                  = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {188-197},
  Volume                   = {59},

  Abstract                 = {Nonbinary LDPC codes are effective in combating burst errors. This paper presents an efficient architecture for implementing nonbinary LDPC decoders. The Galois field power representation is used to organize the a priori, a posteriori, and extrinsic messages involved in decoding. The power representation in conjunction with the barrel shifter and multithreaded pipelining yields an efficient implementation. The proposed decoder is configurable, in the sense that a single decoder can be used to decode any code of a given field size. The decoder supports both regular and irregular nonbinary QC-LDPC codes. Using a practical metric of throughput per unit area, the proposed implementation outperforms the best implementations published in research literature to date.},
  Doi                      = {10.1109/TCSI.2011.2161416},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05982105.pdf:PDF},
  ISSN                     = {1549-8328},
  Keywords                 = {Galois fields;binary codes;cyclic codes;decoding;error correction codes;parity check codes;Galois field power representation;barrel shifter;burst errors;configurable decoder architecture;decoding;irregular nonbinary QC-LDPC codes;multithreaded pipelining;nonbinary LDPC codes;nonbinary LDPC decoders;nonbinary quasi-cyclic LDPC codes;Complexity theory;Computer architecture;Decoding;Iterative decoding;Logic gates;Multiplexing;Configurable;VLSI design;decoder architecture;low-density parity-check (LDPC) codes;min-max;nonbinary},
  Timestamp                = {2013.05.28}
}

@InProceedings{4716279,
  Title                    = {Reduced complexity and improved performance decoding algorithm for nonbinary LDPC codes over GF(q)},
  Author                   = {Xin Chen and Aidong Men},
  Booktitle                = {Communication Technology, 2008. ICCT 2008. 11th IEEE International Conference on},
  Year                     = {2008},
  Month                    = {Nov},
  Pages                    = {406-409},

  Abstract                 = {Low-density parity-check (LDPC) codes over finite fields GF(q) have a better performance than those of the binary low-density parity-check codes, at short and medium block lengths, but the decoder of GF(q)-LDPC has more complexity. In this paper, we present a new reduced-complexity belief propagation (BP) decoding algorithm for GF(q)-LDPC codes by analyzing the reliability of code symbols. Instead of update all the variable nodes during iteration, we just update variable nodes which are mostly to be in an error. The algorithm also shows a property to reduce the effects of oscillating on decoding. The simulations prove that we could improve performance and reduce complexity at the same time for short and medium GF(q)-LDPC codes.},
  Doi                      = {10.1109/ICCT.2008.4716279},
  File                     = {:PDF\\04716279.pdf:PDF},
  Keywords                 = {Galois fields;iterative decoding;parity check codes;Galois fields;LDPC codes;decoder;decoding algorithm;iterative decoding;low-density parity-check codes;reduced-complexity belief propagation decoding algorithm;Algorithm design and analysis;Belief propagation;Convergence;Error correction;Fast Fourier transforms;Galois fields;Iterative decoding;Medical services;Parity check codes;Scheduling algorithm;GF(q)-LDPC;belief propagation (BP);decoding algorithm;reduced-complexity},
  Timestamp                = {2015.05.13}
}

@InProceedings{Chen2003,
  Title                    = {A FPGA and ASIC implementation of rate 1/2, 8088-b irregular low density parity check decoder},
  Author                   = {Yanni Chen and Hocevar, D.},
  Booktitle                = {Global Telecommunications Conference, 2003. GLOBECOM '03. IEEE},
  Year                     = {2003},
  Month                    = {dec.},
  Pages                    = { 113 - 117 Vol.1},
  Volume                   = {1},

  Abstract                 = {This paper presents an implementation of irregular low density parity check decoder using both FPGA and ASIC. The considered low density parity check code has code rate 1/2, codeword length of 8088 bits and parallel factor of 24. The partly parallel structure, memory management, message alignment and addressing generation schemes needed to realize the underlying graph connectivity will be discussed. With the target FPGA device Xilinx XC2V8000 and maximum number of 25 iterations, the information decoding throughput could achieve up to 40 Mbps. By using the same configuration and Texas Instruments' GS-40 0.11 mu;m ASIC process technology, decoder data rate of 188 Mbps could be achieved for this decoder.},
  Doi                      = {10.1109/GLOCOM.2003.1258213},
  File                     = {:PDF\\01258213.pdf:PDF},
  Keywords                 = { ASIC; FPGA; graph connectivity; information decoding; low density parity check decoder; memory management; message alignment; parallel structure; application specific integrated circuits; decoding; field programmable gate arrays; parallel architectures; parity check codes; storage management;},
  Timestamp                = {2011.04.22}
}

@Article{1291433,
  Title                    = {Small area parallel Chien search architectures for long BCH codes},
  Author                   = {Yanni Chen and Parhi, K.K.},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {may.},
  Number                   = {5},
  Pages                    = { 545 - 549},
  Volume                   = {12},

  Doi                      = {10.1109/TVLSI.2004.826203},
  File                     = {:PDF\\Small_Area_Parallel_Chien_Search_Architecture_for_Long_BCH_Codes.pdf:PDF},
  ISSN                     = {1063-8210},
  Keywords                 = { BCH codes; Bose-Chaud-huri-Hochquenghem codes; Chien search hardware complexity; finite field multiplier; group matching scheme; iterative matching algorithm; parallel Chien search architectures; BCH codes; iterative methods;}
}

@Article{1304967,
  Title                    = {Overlapped message passing for quasi-cyclic low-density parity check codes},
  Author                   = {Chen, Yanni and Parhi, K.K.},
  Journal                  = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  Year                     = {2004},
  Number                   = {6},
  Pages                    = {1106-1113},
  Volume                   = {51},

  Abstract                 = {In this paper, a systematic approach is proposed to develop a high throughput decoder for quasi-cyclic low-density parity check (LDPC) codes, whose parity check matrix is constructed by circularly shifted identity matrices. Based on the properties of quasi-cyclic LDPC codes, the two stages of belief propagation decoding algorithm, namely, check node update and variable node update, could be overlapped and thus the overall decoding latency is reduced. To avoid the memory access conflict, the maximum concurrency of the two stages is explored by a novel scheduling algorithm. Consequently, the decoding throughput could be increased by about twice assuming dual-port memory is available.},
  Doi                      = {10.1109/TCSI.2004.826194},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01304967.pdf:PDF},
  ISSN                     = {1549-8328},
  Keywords                 = {decoding;matrix algebra;message passing;parity check codes;scheduling;belief propagation decoding;check node update;decoding latency;dual-port memory;low-density parity check codes;overlapped message passing;parity check matrix;quasi-cyclic LDPC codes;scheduling algorithm;variable node update;Belief propagation;Decoding;Hardware;Message passing;Parity check codes;Routing;Sparse matrices;Throughput;Turbo codes;Very large scale integration;High throughput;LDPC;MP;codes;low-density parity check;overlapped message passing;quasi-cyclic codes},
  Timestamp                = {2013.07.31}
}

@InProceedings{1405303,
  Title                    = {Instanton method of post-error-correction analytical evaluation},
  Author                   = {Chernyak, V. and Chertkov, M. and Stepanov, M. and Vasic, B.},
  Booktitle                = {Information Theory Workshop, 2004. IEEE},
  Year                     = {2004},
  Month                    = {Oct},
  Pages                    = {220-224},

  Doi                      = {10.1109/ITW.2004.1405303},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01405303.pdf:PDF},
  Keywords                 = {block codes;error correction codes;error statistics;linear codes;tree codes;bit error rate;closed form expression;error code performance;error correction;instanton calculus;linear block codes;tree graphical model;Bit error rate;Block codes;Calculus;Graphical models;Iterative algorithms;Iterative decoding;Parity check codes;Performance analysis;Physics;Tree graphs},
  Timestamp                = {2014.03.05}
}

@Article{Chien,
  Title                    = {Cyclic decoding procedures for Bose-Chaudhuri-Hocquenghem codes},
  Author                   = {R. Chien},
  Journal                  = {IEEE Transactions on Information Theory},
  Year                     = {1964},
  Pages                    = {357-363},
  Volume                   = {10},

  File                     = {:PDF\\Chien_Cyclic_Decoding_Procedures_for_Bose-Chaundhuri-Hocquenghen_Codes.pdf:PDF},
  Issue                    = {4}
}

@Article{5174515,
  Title                    = {Instanton-based techniques for analysis and reduction of error floors of LDPC codes},
  Author                   = {Chilappagari, S.K. and Chertkov, Michael and Stepanov, M.G. and Vasic, B.},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {2009},

  Month                    = {August},
  Number                   = {6},
  Pages                    = {855-865},
  Volume                   = {27},

  Abstract                 = {We describe a family of instanton-based optimization methods developed recently for the analysis of the error floors of low-density parity-check (LDPC) codes. Instantons are the most probable configurations of the channel noise which result in decoding failures. We show that the general idea and the respective optimization technique are applicable broadly to a variety of channels, discrete or continuous, and variety of sub-optimal decoders. Specifically, we consider: iterative belief propagation (BP) decoders, Gallager type decoders, and linear programming (LP) decoders performing over the additive white Gaussian noise channel (AWGNC) and the binary symmetric channel (BSC). The instanton analysis suggests that the underlying topological structures of the most probable instanton of the same code but different channels and decoders are related to each other. Armed with this understanding of the graphical structure of the instanton and its relation to the decoding failures, we suggest a method to construct codes whose Tanner graphs are free of these structures, and thus have less significant error floors.},
  Doi                      = {10.1109/JSAC.2009.090804},
  ISSN                     = {0733-8716},
  Keywords                 = {AWGN channels;coding errors;linear programming;optimisation;parity check codes;Gallager type decoder;LDPC codes;additive white Gaussian noise channel;binary symmetric channel;channel noise;error floor reduction;instanton based optimization;iterative belief propagation decoder;linear programming decoder;low density parity check code;Algorithm design and analysis;Belief propagation;Bit error rate;Error analysis;Iterative algorithms;Iterative decoding;Linear programming;Message passing;Optimization methods;Parity check codes;Low-density parity-check codes, Error Floor, Iterative Decoding, Linear Programming Decoding, Instantons, Pseudo-Codewords, Trapping Sets},
  Timestamp                = {2014.03.04}
}

@Article{5895058,
  Title                    = {An Efficient Instanton Search Algorithm for LP Decoding of LDPC Codes Over the BSC},
  Author                   = {Chilappagari, S.K. and Chertkov, Michael and Vasic, B.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {4417-4426},
  Volume                   = {57},

  Abstract                 = {We consider linear programming (LP) decoding of a fixed low-density parity-check (LDPC) code over the binary symmetric channel (BSC). The LP decoder fails when it outputs a pseudo-codeword which is not equal to the transmitted codeword. We design an efficient algorithm termed the Instanton Search Algorithm (ISA) which generates an error vector called the BSC-instanton. We prove that: (a) the LP decoder fails for any error pattern with support that is a superset of the support of an instanton; (b) for any input, the ISA outputs an instanton in the number of steps upper-bounded by twice the number of errors in the input error vector. We then find the number of unique instantons of different sizes for a given LDPC code by running the ISA sufficient number of times.},
  Doi                      = {10.1109/TIT.2011.2146670},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05895058.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {binary codes;channel coding;linear predictive coding;linear programming;parity check codes;search problems;BSC-instanton search algorithm;ISA output;LDPC codes;LP decoder;LP decoding;binary symmetric channel code;error pattern;fixed low density parity check code;input error vector;linear programming decoding;pseudo codeword;transmitted codeword;Algorithm design and analysis;Decoding;Iterative decoding;Signal to noise ratio;Support vector machines;Binary symmetric channel (BSC);error-floor;linear programming decoding;low-density parity-check (LDPC) codes;pseudo-codewords},
  Timestamp                = {2014.03.04}
}

@Article{5437420,
  Title                    = {On Trapping Sets and Guaranteed Error Correction Capability of LDPC Codes and GLDPC Codes},
  Author                   = {Chilappagari, S.K. and Nguyen, D.V. and Vasic, B. and Marcellin, M.W.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {1600-1611},
  Volume                   = {56},

  Abstract                 = {The relation between the girth and the guaranteed error correction capability of ?? -left-regular low-density parity-check (LDPC) codes when decoded using the bit flipping (serial and parallel) algorithms is investigated. A lower bound on the size of variable node sets which expand by a factor of at least 3 ??/4 is found based on the Moore bound. This bound, combined with the well known expander based arguments, leads to a lower bound on the guaranteed error correction capability. The decoding failures of the bit flipping algorithms are characterized using the notions of trapping sets and fixed sets. The relation between fixed sets and a class of graphs known as cage graphs is studied. Upper bounds on the guaranteed error correction capability are then established based on the order of cage graphs. The results are extended to left-regular and right-uniform generalized LDPC codes. It is shown that this class of generalized LDPC codes can correct a linear number of worst case errors (in the code length) under the parallel bit flipping algorithm when the underlying Tanner graph is a good expander. A lower bound on the size of variable node sets which have the required expansion is established.},
  Doi                      = {10.1109/TIT.2010.2040962},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05437420.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {error correction codes;parallel algorithms;parity check codes;??-left-regular low-density parity-check codes;GLDPC Codes;LDPC Codes;Moore bound;Tanner graph;cage graphs;error correction capability;parallel bit flipping algorithm;variable node sets;Error correction;Error correction codes;Graph theory;Information theory;Iterative algorithms;Iterative decoding;Message passing;Parity check codes;Upper bound;Bit flipping algorithms;error correction capability;fixed sets;generalized low-density parity-check (LDPC) codes;low-density parity-check (LDPC) codes;trapping sets},
  Timestamp                = {2013.11.08}
}

@InProceedings{4024284,
  Title                    = {Error Floors of LDPC Codes on the Binary Symmetric Channel},
  Author                   = {Chilappagari, S.K. and Sankaranarayanan, S. and Vasic, B.},
  Booktitle                = {Communications, 2006. ICC '06. IEEE International Conference on},
  Year                     = {2006},
  Month                    = {june },
  Pages                    = {1089 -1094},
  Volume                   = {3},

  Abstract                 = {In this paper, we propose a semi-analytical method to compute error floors of LDPC codes on the binary symmetric channel decoded iteratively using the Gallager B algorithm. The error events of the decoder are characterized using combinatorial objects called trapping sets, originally defined by Richardson. In general, trapping sets are characteristic of the graphical representation of a code. We study the structure of trapping sets and explore their relation to graph parameters such as girth and vertex degrees. Using the proposed method, we compute error floors of regular structured and random LDPC codes with column weight three.},
  Doi                      = {10.1109/ICC.2006.254892},
  File                     = {:PDF\\04024284.pdf:PDF},
  ISSN                     = {8164-9547},
  Keywords                 = {AWGN;Additive white noise;Bipartite graph;Computer errors;Degradation;Error analysis;Iterative algorithms;Iterative decoding;Parity check codes;Signal to noise ratio;},
  Timestamp                = {2013.01.25}
}

@Article{2439,
  Title                    = {An analysis of the DC and small-signal AC performance of the tunnel emitter transistor},
  Author                   = {Chu, K.M. and Pulfrey, D.L.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1988},

  Month                    = {feb},
  Number                   = {2},
  Pages                    = {188 -194},
  Volume                   = {35},

  Abstract                 = {A model to describe the I-V characteristics of the tunnel emitter transistor (the TETRAN) is developed. It is based on a general model for tunneling in metal thin-insulator semiconductor structures. The model is used to compute typical magnitudes for the parameters appearing in the small-signal hybrid- pi; equivalent circuit of this device. From these it is predicted that the cutoff frequency for realistic TETRANs based on Al/SiO2/n-Si structures is about 1 GHz. This is considerably less than the values recently predicted for a related device, the BICFET, which is similar to the TETRAN},
  Doi                      = {10.1109/16.2439},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00002439.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {Al-SiO2-Si;DC performance;I-V characteristics;MIS structure;TETRAN;cutoff frequency;model;small-signal AC performance;small-signal hybrid- pi; equivalent circuit;tunnel emitter transistor;equivalent circuits;insulated gate field effect transistors;semiconductor device models;tunnelling;},
  Timestamp                = {2011.09.15}
}

@PhdThesis{10-1-1-133-2426,
  Title                    = {On the Construction of Some Capacity-Approaching Coding Schemes},
  Author                   = {Sae-Young Chung},
  School                   = {Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology},
  Year                     = {2000},
  Month                    = {September},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\10-1-1-133-2426.pdf:PDF},
  Timestamp                = {2011.12.02}
}

@Article{905935,
  Title                    = {On the design of low-density parity-check codes within 0.0045 dB of the Shannon limit},
  Author                   = {Sae-Young Chung and Forney, G.D., Jr. and Richardson, T.J. and Urbanke, R.},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2001},

  Month                    = {feb },
  Number                   = {2},
  Pages                    = {58 -60},
  Volume                   = {5},

  Abstract                 = {We develop improved algorithms to construct good low-density parity-check codes that approach the Shannon limit very closely. For rate 1/2, the best code found has a threshold within 0.0045 dB of the Shannon limit of the binary-input additive white Gaussian noise channel. Simulation results with a somewhat simpler code show that we can achieve within 0.04 dB of the Shannon limit at a bit error rate of 10/sup -6/ using a block length of 10/sup 7/.},
  Doi                      = {10.1109/4234.905935},
  File                     = {:PDF\\00905935.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = {AWGN;Additive white noise;Bit error rate;Decoding;H infinity control;Laboratories;Parity check codes;Quantization;Sum product algorithm;Turbo codes;AWGN channels;block codes;decoding;error statistics;linear programming;AWGN channel;Shannon limit;binary-input additive white Gaussian noise channel;bit error rate;block length;code construction;iterative linear programming;low-density parity-check codes;rate 1/2 code;simulation results;sum-product decoding;},
  Timestamp                = {2013.01.17}
}

@Article{910580,
  Title                    = {Analysis of sum-product decoding of low-density parity-check codes using a Gaussian approximation},
  Author                   = {Sae-Young Chung and Richardson, T.J. and Urbanke, R.L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {feb},
  Number                   = {2},
  Pages                    = {657 -670},
  Volume                   = {47},

  Abstract                 = {Density evolution is an algorithm for computing the capacity of low-density parity-check (LDPC) codes under message-passing decoding. For memoryless binary-input continuous-output additive white Gaussian noise (AWGN) channels and sum-product decoders, we use a Gaussian approximation for message densities under density evolution to simplify the analysis of the decoding algorithm. We convert the infinite-dimensional problem of iteratively calculating message densities, which is needed to find the exact threshold, to a one-dimensional problem of updating the means of the Gaussian densities. This simplification not only allows us to calculate the threshold quickly and to understand the behavior of the decoder better, but also makes it easier to design good irregular LDPC codes for AWGN channels. For various regular LDPC codes we have examined, thresholds can be estimated within 0.1 dB of the exact value. For rates between 0.5 and 0.9, codes designed using the Gaussian approximation perform within 0.02 dB of the best performing codes found so far by using density evolution when the maximum variable degree is 10. We show that by using the Gaussian approximation, we can visualize the sum-product decoding algorithm. We also show that the optimization of degree distributions can be understood and done graphically using the visualization},
  Doi                      = {10.1109/18.910580},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00910580.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {AWGN channels;Gaussian approximation;Gaussian densities;additive white Gaussian noise channels;code rates;degree distributions;density evolution algorithm;infinite-dimensional problem;irregular LDPC codes;low-density parity-check codes;maximum variable degree;memoryless binary-input continuous-output channels;message-passing decoding;optimization;regular LDPC codes;sum-product decoders;sum-product decoding algorithm;thresholds;visualization;AWGN channels;approximation theory;decoding;error detection codes;evolutionary computation;memoryless systems;optimisation;},
  Timestamp                = {2011.12.02}
}

@Article{1057221,
  Title                    = {Linear binary code for write-once memories (Corresp.)},
  Author                   = {Cohen, G. and Godlewski, P. and Merkx, F.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1986},

  Month                    = {Sep},
  Number                   = {5},
  Pages                    = {697-700},
  Volume                   = {32},

  Abstract                 = {An application of error-correcting codes to "write-once" memories (WOM's) as defined by Rivest and Shamir is studied. Large classes of "WOM codes" that are easily decodable are obtained. In particular, a construction allowing three successive writings of 11 bits on 23 positions is derived from the Golay code.},
  Doi                      = {10.1109/TIT.1986.1057221},
  File                     = {:PDF\\01057221.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Error-correction coding;Golay coding;Linear coding;Write-once memories;Binary codes;Decoding;Encoding;Error correction codes;Error probability;Laser beams;Linear code;Optical sensors;Structural beams;Writing},
  Timestamp                = {2015.06.08}
}

@Article{0605051,
  Title                    = {A General Method for Finding Low Error Rates of LDPC Codes},
  Author                   = {Chad A. Cole and Stephen and G. Wilson and Eric. K. Hall and Thomas R. Giallorenzi},
  Journal                  = {arxiv:cs/0605051},
  Year                     = {2006},

  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\0605051.pdf:PDF},
  Timestamp                = {2014.04.07}
}

@Book{978-94-010-2196-8,
  Title                    = {Advanced Combinatorics},
  Author                   = {Louis Comtet},
  Publisher                = {Springer},
  Year                     = {1974},

  File                     = {:PDF\\3A978-94-010-2196-8.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.04.04}
}

@Article{0305-4470-27-23-013,
  Title                    = {Order-parameter flow in the SK spin glass. I. Replica symmetry},
  Author                   = {A C C Coolen and D Sherrington},
  Journal                  = {Journal of Physics A: Mathematical and General},
  Year                     = {1994},
  Number                   = {23},
  Pages                    = {7687},
  Volume                   = {27},

  Abstract                 = {We present a theory to describe the dynamics of the Sherrington-Kirkpatrick spin-glass with (sequential) Glauber dynamics in terms of deterministic flow equations for macroscopic parameters. Two transparent assumptions allow us to close the macroscopic laws. Replica theory enters as a tool in the calculation of the time-dependent local field distribution. The theory produces, in a natural way, dynamical generalizations of the AT- and zero-entropy lines and of Parisi's order-parameter function P(q). In equilibrium we recover the standard results from equilibrium statistical mechanics. In this paper we make the replica-symmetric ansatz, as a first step towards calculating the order-parameter flow. Numerical simulations support our assumptions and suggest that our equations describe the shape of the local field distribution and the macroscopic dynamics reasonably well in the region where replica symmetry is stable.},
  File                     = {:PDF\\9406104.pdf:PDF},
  Timestamp                = {2015.02.23},
  Url                      = {http://stacks.iop.org/0305-4470/27/i=23/a=013}
}

@Book{Cox:2007:IVA:1204670,
  Title                    = {Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra, 3/e (Undergraduate Texts in Mathematics)},
  Author                   = {Cox, David A. and Little, John and O'Shea, Donal},
  Publisher                = {Springer-Verlag New York, Inc.},
  Year                     = {2007},

  Address                  = {Secaucus, NJ, USA},

  ISBN                     = {0387356509},
  Timestamp                = {2011.09.24}
}

@Book{GBBIB448,
  Title                    = {Using Algebraic Geometry},
  Author                   = {D. Cox and J. Little and D. O'Shea},
  Publisher                = {Springer},
  Year                     = {2005},
  Edition                  = {2nd},

  Abstract                 = {This book is an introduction to Gr\"{o}bner bases and resultants, which are two of the main tools used in computational algebraic geometry and commutative algebra. It also discusses local methods and syzygies, and gives applications to integer programming, polynomial splines and algebraic coding theory.},
  Language                 = {English},
  Length                   = {558},
  Timestamp                = {2012.06.07},
  Url                      = {http://www.cs.amherst.edu/~dac/uag.html}
}

@Article{AJP2004vol72no10pp1294,
  Title                    = {A new approach to Monte Carlo simulations in statistical physics: Wang-Landau sampling},
  Author                   = {D. P. Landau, Shan-Ho Tsai and M. Exler},
  Journal                  = {American Journal of Physics},
  Year                     = {2004},

  Month                    = {oct},
  Number                   = {10},
  Pages                    = {1294--1302},
  Volume                   = {72},

  Abstract                 = {We describe a Monte Carlo algorithm for doing simulations in classical statistical physics in a different way. Instead of sampling the probability distribution at a fixed temperature, a random walk is performed in energy space to extract an estimate for the density of states. The probability can be computed at any temperature by weighting the density of states by the appropriate Boltzmann factor. Thermodynamic properties can be determined from suitable derivatives of the partition function and, unlike “standard” methods, the free energy and entropy can also be computed directly. To demonstrate the simplicity and power of the algorithm, we apply it to models exhibiting first-order or second-order phase transitions.},
  Doi                      = {10.1119/1.1707017},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\AJP2004vol72no10pp1294.pdf:PDF},
  Timestamp                = {2012.03.27},
  Url                      = {http://ajp.aapt.org/resource/1/ajpias/v72/i10/p1294_s1?isAuthorized=no}
}

@Article{ETT:ETT1091,
  Title                    = {An algorithm for the computation of the minimum distance of LDPC codes},
  Author                   = {Daneshgaran, Fred and Laddomada, Massimiliano and Mondin, Marina},
  Journal                  = {European Transactions on Telecommunications},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {57--62},
  Volume                   = {17},

  Abstract                 = {The evaluation of the minimum distance of low-density parity-check (LDPC) codes remains an open problem due to the rather large dimension of the parity check matrix H associated with any practical code. In this article, we propose an effective modification of the error impulse (EI) technique for computation of the minimum distance of the LDPC codes. The EI method is successfully applied to sub-optimum decoding algorithms such as the iterative MAP decoding algorithm for turbo codes. We present novel modifications and extensions of this method to the sub-optimum iterative sum-product algorithm for LDPC codes. The performance of LDPC codes may be limited by pseudo-codewords. There are, however, cases when the LDPC decoder behaves as a maximum-likelihood (ML) decoder. This is specially so for randomly constructed LDPC codes operating at medium to high SNR values. In such cases, estimation of the minimum distance dm using the error-impulse method can be useful to assess asymptotic performance. In short, apart from theoretical interest in achievable dm, the technique is useful in checking whether an LDPC code is poor by virtue of having a low dm. But, if a code has a high dm, simulations would still be needed to assess real performance. Copyright ﾂｩ 2006 AEIT.},
  Doi                      = {10.1002/ett.1091},
  File                     = {:PDF\\an_algorithm_estimation_min_dist_ldpc4e08.pdf:PDF},
  ISSN                     = {1541-8251},
  Publisher                = {John Wiley \& Sons, Ltd.},
  Timestamp                = {2015.01.21},
  Url                      = {http://dx.doi.org/10.1002/ett.1091}
}

@InProceedings{1465805,
  Title                    = {Multi-Gbit/sec low density parity check decoders with reduced interconnect complexity},
  Author                   = {Darabiha, A. and Carusone, A.C. and Kschischang, F.R.},
  Booktitle                = {Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium on},
  Year                     = {2005},
  Month                    = {may},
  Pages                    = { 5194 - 5197 Vol. 5},

  Abstract                 = {A 3.2-Gbit/sec 2048-bit parallel LDPC decoder is implemented in a 0.18 mu;m CMOS process. We employ two new techniques to address the interconnect problem: A broadcasting technique reduces the total amount of check-to-variable interconnect wires by more than 40%. A hierarchical placement algorithm places the variable and check nodes in the top-level hierarchy of the design and reduces the maximum wire length by up to 50%.},
  Doi                      = {10.1109/ISCAS.2005.1465805},
  File                     = {:PDF\\01465805.pdf:PDF},
  Keywords                 = { 0.18 micron; 2048 bit; 3.2 Gbit/s; CMOS; broadcasting technique; check nodes placement; check-to-variable interconnect wire reduction; hierarchical placement algorithm; low density parity check decoders; maximum wire length reduction; parallel LDPC decoder; random parity-check matrix; reduced interconnect complexity decoders; variable nodes placement; CMOS logic circuits; decoding; integrated circuit interconnections; integrated circuit layout; parallel processing; parity check codes;},
  Timestamp                = {2011.04.22}
}

@Article{681360,
  Title                    = {Low-density parity check codes over GF(q)},
  Author                   = {Davey, M.C. and MacKay, D.},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {1998},

  Month                    = {jun.},
  Number                   = {6},
  Pages                    = {165 -167},
  Volume                   = {2},

  Doi                      = {10.1109/4234.681360},
  File                     = {:PDF\\Low-Density_Parity_Check_Codes_over_GF(q).pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = {GF(q);binary Gaussian channels;binary codes;binary symmetric channels;bit error probability;error-correction;low-density parity check codes;near-Shannon limit performance;probabilistic decoding algorithm;rate 1/4 code;Galois fields;Gaussian channels;binary sequences;channel coding;decoding;error correction codes;probability;sparse matrices;}
}

@InProceedings{706440,
  Title                    = {Low density parity check codes over GF(q)},
  Author                   = {Davey, M.C. and MacKay, D.J.C.},
  Booktitle                = {Information Theory Workshop, 1998},
  Year                     = {1998},
  Month                    = {Jun},
  Pages                    = {70-71},

  Abstract                 = {Binary low density parity check (LDPC) codes have been shown to have near Shannon limit performance when decoded using a probabilistic decoding algorithm. The analogous codes defined over finite fields GF(q) of order q>2 show significantly improved performance. We present the results of Monte Carlo simulations of the decoding of infinite LDPC codes which can be used to obtain good constructions for finite codes. We also present empirical results for the Gaussian channel including a rate 1/4 code with bit error probability of 10-4 at Eb /N0=-0.05 dB},
  Doi                      = {10.1109/ITW.1998.706440},
  File                     = {:PDF\\00706440.pdf:PDF},
  Keywords                 = {Galois fields;Gaussian channels;Monte Carlo methods;channel coding;decoding;error correction codes;error statistics;GF(q);Gaussian channel;LDPC codes;Monte Carlo simulations;Shannon limit performance;binary low density parity check codes;bit error probability;finite codes;finite fields;low density parity check codes;performance;probabilistic decoding algorithm;rate 1/4 code;Belief propagation;Error correction codes;Error probability;Galois fields;Gaussian channels;Iterative decoding;Laboratories;Parity check codes;Sparse matrices;Symmetric matrices},
  Timestamp                = {2015.04.21}
}

@PhdThesis{davey_phd,
  Title                    = {Error-correction using Low-Density Parity-Check Codes},
  Author                   = {Matthew C. Davey},
  School                   = {University of Cambridge},
  Year                     = {1999},

  File                     = {:PDF\\davey_phd.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.05.15}
}

@Article{1480701,
  Title                    = {Standardized terminology for oxide charges associated with thermally oxidized silicon},
  Author                   = {Deal, B.E.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1980},

  Month                    = mar,
  Number                   = {3},
  Pages                    = { 606 - 608},
  Volume                   = {27},

  Abstract                 = { Standarized terminology for oxide charges associated with the thermally oxidized silicon system is presented. This terminology is recommended by a committee established by the Electronics Division of the Electrochemical Society and the IEEE Semiconductor Interface Specialists Conference. All engineers and scientists concerned with oxide charges in silicon semiconductor applications are urged to adopt this terminology.},
  Doi                      = {10.1109/T-ED.1980.19908},
  File                     = {:PDF\\Standardized_Terminology_for_Oxide_Charges_Associated_with_Thermally_Oxidized_Silicon.pdf:PDF},
  ISSN                     = {0018-9383}
}

@Article{4155118,
  Title                    = {Decoding Algorithms for Nonbinary LDPC Codes Over GF (q) },
  Author                   = {Declercq, D. and Fossorier, M.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {633-643},
  Volume                   = {55},

  Abstract                 = {In this letter, we address the problem of decoding nonbinary low-density parity-check (LDPC) codes over finite fields GF(q), with reasonable complexity and good performance. In the first part of the letter, we recall the original belief propagation (BP) decoding algorithm and its Fourier domain implementation. We show that the use of tensor notations for the messages is very convenient for the algorithm description and understanding. In the second part of the letter, we introduce a simplified decoder which is inspired by the min-sum decoder for binary LDPC codes. We called this decoder extended min-sum (EMS). We show that it is possible to greatly reduce the computational complexity of the check-node processing by computing approximate reliability measures with a limited number of values in a message. By choosing appropriate correction factors or offsets, we show that the EMS decoder performance is quite good, and in some cases better than the regular BP decoder. The optimal values of the factor and offset correction are obtained asymptotically with simulated density evolution. Our simulations on ultra-sparse codes over very-high-order fields show that nonbinary LDPC codes are promising for applications which require low frame-error rates for small or moderate codeword lengths. The EMS decoder is a good candidate for practical hardware implementations of such codes},
  Doi                      = {10.1109/TCOMM.2007.894088},
  File                     = {:PDF\\04155118.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {Fourier analysis;Galois fields;computational complexity;iterative decoding;parity check codes;Fourier domain;GF;belief propagation decoding;check-node processing;codeword lengths;decoder extended min-sum;decoding algorithms;frame-error rates;min-sum decoder;nonbinary LDPC codes;nonbinary low-density parity-check codes;ultra-sparse codes;Belief propagation;Computational complexity;Fourier transforms;Galois fields;Iterative algorithms;Iterative decoding;Medical services;Parity check codes;Table lookup;Tensile stress;Complexity reduction;iterative decoder;nonbinary low-density parity-check (LDPC) codes}
}

@Article{5145891,
  Title                    = {Impact ionization, trap creation, degradation, and breakdown in silicon dioxide films on silicon},
  Author                   = {DiMaria, D. J. and Cartier, E. and Arnold, D.},
  Journal                  = {Journal of Applied Physics},
  Year                     = {1993},

  Month                    = apr,
  Number                   = {7},
  Pages                    = {3367 -3384},
  Volume                   = {73},

  Abstract                 = {Degradation of silicon dioxide films is shown to occur primarily near interfaces with contacting metals or semiconductors. This deterioration is shown to be accountable through two mechanisms triggered by electron heating in the oxide conduction band. These mechanisms are trap creation and band #x2010;gap ionization by carriers with energies exceeding 2 and 9 eV with respect to the bottom of the oxide conduction band, respectively. The relationship of band #x2010;gap ionization to defect production and subsequent degradation is emphasized. The dependence of the generated sites on electric field, oxide thickness, temperature, voltage polarity, and processing for each mechanism is discussed. A procedure for separating and studying these two generation modes is also discussed. A unified model from simple kinetic relationships is developed and compared to the experimental results. Destructive breakdown of the oxide is shown to be correlated with #x2018; #x2018;effective #x2019; #x2019; interface softening due to the total defect generation caused by both mechanisms.},
  Doi                      = {10.1063/1.352936},
  File                     = {:PDF\\JAP_Vol73_P3367.pdf:PDF},
  ISSN                     = {0021-8979}
}

@InProceedings{Divsalar99asimple,
  Title                    = {A simple tight bound on error probability of block codes with application to turbo codes},
  Author                   = {D. Divsalar},
  Booktitle                = {TMO Progr. Rep},
  Year                     = {1999},
  Pages                    = {42--139},

  File                     = {:PDF\\10.1.1.313.6161.pdf:PDF},
  Timestamp                = {2015.01.09}
}

@Article{1256740,
  Title                    = {Upper bounds to error probabilities of coded systems beyond the cutoff rate},
  Author                   = {Divsalar, D. and Biglieri, Ezio},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {2011-2018},
  Volume                   = {51},

  Abstract                 = {A family of upper bounds to error probabilities of coded systems was recently proposed by D. Divsalar (see IEEE Communication Theory Workshop, 1999; JPL TMO Prog. Rep. 42-139, 1999). These bounds are valid for transmission over the additive white Gaussian noise channel, and require only the knowledge of the weight spectrum of the code words. After illustrating these bounds, we extend them to fading channels. Contrary to the union bound, our bounds maintain their effectiveness below the signal-to-noise ratio (SNR) at which the cutoff rate of the channel equals the rate of the code. Some applications are shown. First, we derive upper bounds to the minimum SNR necessary to achieve zero error probability as the code block length increases to infinity. Next, we use our bounds to predict the performance of turbo codes and low-density parity-check codes.},
  Doi                      = {10.1109/TCOMM.2003.820746},
  File                     = {:PDF\\01256740.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {AWGN channels;binary codes;error statistics;fading channels;linear codes;maximum likelihood decoding;parity check codes;turbo codes;AWGN channel;SNR;additive white Gaussian noise channel;channel cutoff rate;code word weight spectrum;error probability upper bounds;fading channels;linear binary codes;low-density parity-check codes;maximum-likelihood decoding;signal-to-noise ratio;turbo codes;AWGN;Additive white noise;Binary codes;Error probability;Fading;Maximum likelihood decoding;Parity check codes;Signal to noise ratio;Turbo codes;Upper bound},
  Timestamp                = {2015.01.09}
}

@Article{1214058,
  Title                    = {A class of low-density parity-check codes constructed based on Reed-Solomon codes with two information symbols},
  Author                   = {Djurdjevic, I. and Jun Xu and Abdel-Ghaffar, K. and Shu Lin},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2003},

  Month                    = july,
  Number                   = {7},
  Pages                    = { 317 - 319},
  Volume                   = {7},

  Abstract                 = { This letter presents an algebraic method for constructing regular low-density parity-check (LDPC) codes based on Reed-Solomon codes with two information symbols. The construction method results in a class of LDPC codes in Gallager's original form. Codes in this class are free of cycles of length 4 in their Tanner graphs and have good minimum distances. They perform well with iterative decoding.},
  Doi                      = {10.1109/LCOMM.2003.814716},
  File                     = {:PDF\\A_Class_of_Low-Density_Parity-Check_Codes_Constructed_Based_on_Reed-Solomon_Codes_With_Two_Information_Symbols.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = { LDPC codes; Reed-Solomon codes; Tanner graphs; information symbols; iterative decoding; low-density parity-check codes; minimum distances; Reed-Solomon codes; graph theory; iterative decoding; parity check codes;}
}

@Article{5174520,
  Title                    = {Predicting error floors of structured LDPC codes: deterministic bounds and estimates},
  Author                   = {Dolecek, L. and Lee, P. and Zhengya Zhang and Anantharam, V. and Nikolic, B. and Wainwright, M.},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {2009},
  Number                   = {6},
  Pages                    = {908-917},
  Volume                   = {27},

  Abstract                 = {The error-correcting performance of low-density parity check (LDPC) codes, when decoded using practical iterative decoding algorithms, is known to be close to Shannon limits for codes with suitably large blocklengths. A substantial limitation to the use of finite-length LDPC codes is the presence of an error floor in the low frame error rate (FER) region. This paper develops a deterministic method of predicting error floors, based on high signal-to-noise ratio (SNR) asymptotics, applied to absorbing sets within structured LDPC codes. The approach is illustrated using a class of array-based LDPC codes, taken as exemplars of high-performance structured LDPC codes. The results are in very good agreement with a stochastic method based on importance sampling which, in turn, matches the hardware-based experimental results. The importance sampling scheme uses a mean-shifted version of the original Gaussian density, appropriately centered between a codeword and a dominant absorbing set, to produce an unbiased estimator of the FER with substantial computational savings over a standard Monte Carlo estimator. Our deterministic estimates are guaranteed to be a lower bound to the error probability in the high SNR regime, and extend the prediction of the error probability to as low as 10-30. By adopting a channel-independent viewpoint, the usefulness of these results is demonstrated for both the standard Gaussian channel and a channel with mixture noise.},
  Doi                      = {10.1109/JSAC.2009.090809},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05174520.pdf:PDF},
  ISSN                     = {0733-8716},
  Keywords                 = {Gaussian channels;Gaussian processes;block codes;channel capacity;error correction codes;error statistics;importance sampling;iterative decoding;parity check codes;stochastic processes;Gaussian channel;Gaussian density;SNR asymptotics;Shannon limits;array-based LDPC codes;deterministic bounds;error floor prediction;error probability;error-correcting performance;finite blocklength;finite-length LDPC codes;frame error rate region;importance sampling;iterative decoding algorithm;low-density parity check codes;signal-to-noise ratio;stochastic method;structured LDPC codes;Code standards;Error analysis;Error probability;Gaussian channels;Iterative algorithms;Iterative decoding;Monte Carlo methods;Parity check codes;Signal to noise ratio;Stochastic processes;LDPC codes; belief propagation; hardware emulation; error floor; importance sampling; near-codeword; trapping set; absorbing set; pseudocodeword.},
  Timestamp                = {2013.12.17}
}

@Article{5629456,
  Title                    = {On the Use of Soft-Decision Error-Correction Codes in nand Flash Memory},
  Author                   = {Guiqiang Dong and Ningde Xie and Tong Zhang},
  Journal                  = {Circuits and Systems I: Regular Papers, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {feb. },
  Number                   = {2},
  Pages                    = {429 -439},
  Volume                   = {58},

  Abstract                 = {As technology continues to scale down, NAND Flash memory has been increasingly relying on error-correction codes (ECCs) to ensure the overall data storage integrity. Although advanced ECCs such as low-density parity-check (LDPC) codes can provide significantly stronger error-correction capability over BCH codes being used in current practice, their decoding requires soft-decision log-likelihood ratio (LLR) information. This results in two critical issues. First, accurate calculation of LLR demands fine-grained memory-cell sensing, which nevertheless tends to incur implementation overhead and access latency penalty. Hence, it is critical to minimize the fine-grained memory sensing precision. Second, accurate calculation of LLR also demands the availability of a memory-cell threshold-voltage distribution model. As the major source for memory-cell threshold-voltage distribution distortion, cell-to-cell interference must be carefully incorporated into the model. However, these two critical issues have not been ever addressed in the open literature. This paper attempts to address these open issues. We derive mathematical formulations to approximately model the threshold-voltage distribution of memory cells in the presence of cell-to-cell interference, based on which the calculation of LLRs is mathematically formulated. This paper also proposes a nonuniform memory sensing strategy to reduce the memory sensing precision and, thus, sensing latency while still maintaining good error-correction performance. In addition, we investigate these design issues under the scenario when we can also sense interfering cells and hence explicitly estimate cell-to-cell interference strength. We carry out extensive computer simulations to demonstrate the effectiveness and involved tradeoffs, assuming the use of LDPC codes in 2-bits/cell NAND Flash memory.},
  Doi                      = {10.1109/TCSI.2010.2071990},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05629456.pdf:PDF},
  ISSN                     = {1549-8328},
  Keywords                 = {BCH codes;ECC;LDPC codes;LLR information;NAND flash emory;access latency penalty;cell-to-cell interference;computer simulations;data storage integrity;decoding;error-correction capability;error-correction performance;fine-grained memory sensing precision;fine-grained memory-cell sensing;implementation overhead;interfering cells;low-density parity-check codes;mathematical formulations;memory-cell threshold-voltage distribution distortion;memory-cell threshold-voltage distribution model;nonuniform memory sensing strategy;sensing latency;soft-decision error-correction codes;soft-decision log-likelihood ratio information;BCH codes;NAND circuits;decoding;error correction codes;flash memories;interference;parity check codes;},
  Timestamp                = {2012.04.25}
}

@InProceedings{Dong:2008:CME:1391469.1391610,
  Title                    = {Circuit and microarchitecture evaluation of 3D stacking magnetic RAM (MRAM) as a universal memory replacement},
  Author                   = {Dong, Xiangyu and Wu, Xiaoxia and Sun, Guangyu and Xie, Yuan and Li, Helen and Chen, Yiran},
  Booktitle                = {Proceedings of the 45th annual Design Automation Conference},
  Year                     = {2008},

  Address                  = {New York, NY, USA},
  Pages                    = {554--559},
  Publisher                = {ACM},
  Series                   = {DAC '08},

  Acmid                    = {1391610},
  Doi                      = {http://doi.acm.org/10.1145/1391469.1391610},
  File                     = {:PDF\\p554-dong.pdf:PDF},
  ISBN                     = {978-1-60558-115-6},
  Keywords                 = {3D stacking, MRAM},
  Location                 = {Anaheim, California},
  Numpages                 = {6},
  Timestamp                = {2011.06.03},
  Url                      = {http://doi.acm.org/10.1145/1391469.1391610}
}

@Article{1057299,
  Title                    = {On the equivalence between Berlekamp's and Euclid's algorithms (Corresp.)},
  Author                   = { Dornstetter, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1987},

  Month                    = may,
  Number                   = {3},
  Pages                    = { 428 - 431},
  Volume                   = {33},

  Doi                      = {10.1109/TIT.1987.1057299},
  File                     = {:PDF\\On_the_Equivalence_Between_Berlekamp's_and_Euclid's_Algorithms.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Division; Polynomials;}
}

@InProceedings{6284712,
  Title                    = {Trade-offs between instantaneous and total capacity in multi-cell flash memories},
  Author                   = {En Gad, E. and Anxiao Jiang and Bruck, J.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on},
  Year                     = {2012},
  Month                    = {July},
  Pages                    = {990-994},

  Abstract                 = {The limited endurance of flash memories is a major design concern for enterprise storage systems. We propose a method to increase it by using relative (as opposed to fixed) cell levels and by representing the information with Write Asymmetric Memory (WAM) codes. Overall, our new method enables faster writes, improved reliability as well as improved endurance by allowing multiple writes between block erasures. We study the capacity of the new WAM codes with relative levels, where the information is represented by multiset permutations induced by the charge levels, and show that it achieves the capacity of any other WAM codes with the same number of writes. Specifically, we prove that it has the potential to double the total capacity of the memory. Since capacity can be achieved only with cells that have a large number of levels, we propose a new architecture that consists of multi-cells - each an aggregation of a number of floating gate transistors.},
  Doi                      = {10.1109/ISIT.2012.6284712},
  File                     = {:PDF\\06284712.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {flash memories;integrated circuit design;integrated circuit reliability;write-once storage;WAM codes;block erasures;charge levels;endurance improvement;enterprise storage systems;floating gate transistors;information representation;instantaneous rate;memory capacity;multicell flash memory design;multiset permutations;relative cell levels;reliability improvement;total capacity rate;write asymmetric memory codes;Ash;Decoding;Memory management;Microprocessors;Modulation;Transistors},
  Timestamp                = {2015.03.26}
}

@Article{6531662,
  Title                    = {Generalized Gray Codes for Local Rank Modulation},
  Author                   = {En Gad, E. and Langberg, M. and Schwartz, M. and Bruck, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {6664-6673},
  Volume                   = {59},

  Abstract                 = {We consider the local rank-modulation scheme, in which a sliding window going over a sequence of real-valued variables induces a sequence of permutations. Local rank-modulation is a generalization of the rank-modulation scheme, which has been recently suggested as a way of storing information in flash memory. We study gray codes for the local rank-modulation scheme in order to simulate conventional multilevel flash cells while retaining the benefits of rank modulation. Unlike the limited scope of previous works, we consider code constructions for the entire range of parameters including the code length, sliding-window size, and overlap between adjacent windows. We show that the presented codes have asymptotically optimal rate. We also provide efficient encoding, decoding, and next-state algorithms.},
  Doi                      = {10.1109/TIT.2013.2268534},
  File                     = {:PDF\\06531662.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Gray codes;decoding;flash memories;code length;decoding;encoding;flash memory;generalized gray code;information storing;local rank-modulation scheme;multilevel flash cell;next-state algorithm;real-valued variable;sliding-window size;Ash;Context;Educational institutions;Modulation;Programming;Reflective binary codes;Vectors;Flash memory;gray code;local rank modulation;permutations;rank modulation},
  Timestamp                = {2015.03.31}
}

@Article{5959206,
  Title                    = {Constant-Weight Gray Codes for Local Rank Modulation},
  Author                   = {En Gad, E. and Langberg, M. and Schwartz, M. and Bruck, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {7431-7442},
  Volume                   = {57},

  Abstract                 = {We consider the local rank-modulation (LRM) scheme in which a sliding window going over a sequence of real-valued variables induces a sequence of permutations. LRM is a generalization of the rank-modulation scheme, which has been recently suggested as a way of storing information in flash memory. We study constant-weight Gray codes for the LRM scheme in order to simulate conventional multilevel flash cells while retaining the benefits of rank modulation. We present a practical construction of codes with asymptotically-optimal rate and weight asymptotically half the length, thus having an asymptotically-optimal charge difference between adjacent cells. Next, we turn to examine the existence of optimal codes by specifically studying codes of weight 2 and 3. In the former case, we upper bound the code efficiency, proving that there are no such asymptotically-optimal cyclic codes. In contrast, for the latter case we construct codes which are asymptotically-optimal. We conclude by providing necessary conditions for the existence of cyclic and cyclic optimal Gray codes.},
  Doi                      = {10.1109/TIT.2011.2162570},
  File                     = {:PDF\\05959206.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Gray codes;cyclic codes;flash memories;modulation coding;LRM scheme;asymptotically-optimal charge difference;asymptotically-optimal cyclic code;code efficiency;constant-weight Gray code;conventional multilevel flash memory cell simulation;cyclic optimal Gray codes;local rank-modulation scheme;permutation sequence;upper bound;Demodulation;Flash memory;Permutations;Reflective binary codes;Flash memory;gray code;local rank modulation;permutations;rank modulation},
  Timestamp                = {2015.03.31}
}

@InProceedings{6033837,
  Title                    = {A practical approach to polar codes},
  Author                   = {Eslami, A and Pishro-Nik, H.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on},
  Year                     = {2011},
  Month                    = {July},
  Pages                    = {16-20},

  Abstract                 = {In this paper, we study polar codes from a practical point of view. In particular, we study concatenated polar codes and rate-compatible polar codes. First, we propose a concatenation scheme including polar codes and Low-Density Parity-Check (LDPC) codes. We will show that our proposed scheme outperforms conventional concatenation schemes formed by LDPC and Reed-Solomon (RS) codes. We then study two rate-compatible coding schemes using polar codes. We will see that polar codes can be designed as universally capacity achieving rate-compatible codes over a set of physically degraded channels. We also study the effect of puncturing on polar codes to design rate-compatible codes.},
  Doi                      = {10.1109/ISIT.2011.6033837},
  File                     = {:PDF\\06033837.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {concatenated codes;parity check codes;Reed-Solomon codes;concatenation scheme;low-density parity-check codes;polar codes;rate-compatible coding schemes;Belief propagation;Bit error rate;Complexity theory;Decoding;Encoding;Parity check codes;Concatenated Codes;Polar Codes;Rate-Compatible Codes},
  Timestamp                = {2014.08.26}
}

@Article{springerlink:10.1007_s00200-009-0095-3,
  Title                    = {Generalized quasi-cyclic codes: structural properties and code construction},
  Author                   = {Esmaeili, M. and Yari, S.},
  Journal                  = {Applicable Algebra in Engineering, Communication and Computing},
  Year                     = {2009},
  Note                     = {10.1007/s00200-009-0095-3},
  Pages                    = {159-173},
  Volume                   = {20},

  Abstract                 = {Generalized quasi-cyclic (GQC) codes are defined by generator matrices comprised of circulant matrices of lengths not necessarily identical. A decomposition of these codes is given by using the Chinese reminder theorem. The focus is to characterize ρ -generator GQC codes in details. A good lower bound on the minimum distance of such a code in terms of the minimum distance of the constituent codes is given. Construction methods are given and a set of GQC codes is provided that from minimum distance perspective are optimal codes among the known linear codes having the same length and dimension.},
  Affiliation              = {Isfahan University of Technology Department of Mathematical Sciences 84156-83111 Isfahan Iran},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink10.1007_s00200-009-0095-3.pdf:PDF},
  ISSN                     = {0938-1279},
  Issue                    = {2},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Berlin / Heidelberg},
  Timestamp                = {2011.10.13},
  Url                      = {http://dx.doi.org/10.1007/s00200-009-0095-3}
}

@Article{179340,
  Title                    = {Decoding algebraic-geometric codes up to the designed minimum distance},
  Author                   = {Feng, G.-L. and Rao, T.R.N.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1993},

  Month                    = {jan},
  Number                   = {1},
  Pages                    = {37 -45},
  Volume                   = {39},

  Abstract                 = {A simple decoding procedure for algebraic-geometric codes C Omega;(D,G) is presented. This decoding procedure is a generalization of Peterson's decoding procedure for the BCH codes. It can be used to correct any [(d*-1)/2] or fewer errors with complexity O(n3), where d * is the designed minimum distance of the algebraic-geometric code and n is the codelength},
  Doi                      = {10.1109/18.179340},
  File                     = {:PDF\\00179340.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {algebraic-geometric codes;complexity;decoding procedure;error correction;minimum distance;computational complexity;decoding;error correction codes;},
  Timestamp                = {2011.06.14}
}

@InProceedings{510063,
  Title                    = {Algebraic geometry codes from Reed Solomon codes},
  Author                   = {Liu Feng and Ebel, W.J.},
  Booktitle                = {Southeastcon '96. 'Bringing Together Education, Science and Technology'., Proceedings of the IEEE},
  Year                     = {1996},
  Month                    = apr,
  Pages                    = {231 -237},

  Abstract                 = {This paper discusses algebraic geometry codes by their construction, parametric properties, and decoding algorithms. Comparisons between algebraic geometry codes and Reed Solomon codes are made to provide a deeper understanding of the algebraic geometry codes. Also, a worked example of algebraic geometry codes is given},
  Doi                      = {10.1109/SECON.1996.510063},
  File                     = {:\\\\yrlshare.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00510063.pdf:PDF},
  Keywords                 = {Reed Solomon codes;algebraic geometry codes;code construction;decoding algorithms;error correcting codes;parametric properties;Reed-Solomon codes;algebraic geometric codes;decoding;error correction codes;}
}

@Article{PhysRev.185.832,
  Title                    = {Bounded and Inhomogeneous Ising Models. I. Specific-Heat Anomaly of a Finite Lattice},
  Author                   = {Ferdinand, Arthur E. and Fisher, Michael E.},
  Journal                  = {Phys. Rev.},
  Year                     = {1969},

  Month                    = {Sep},
  Pages                    = {832--846},
  Volume                   = {185},

  Doi                      = {10.1103/PhysRev.185.832},
  File                     = {:PDF\\PhysRev.185.832.pdf:PDF},
  Issue                    = {2},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.07.25},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRev.185.832}
}

@Article{PhysRevLett.63.1195,
  Title                    = {Optimized Monte Carlo data analysis},
  Author                   = {Ferrenberg, Alan M. and Swendsen, Robert H.},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {1989},

  Month                    = {Sep},
  Pages                    = {1195--1198},
  Volume                   = {63},

  Doi                      = {10.1103/PhysRevLett.63.1195},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.63.1195.pdf:PDF},
  Issue                    = {12},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.04.03},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevLett.63.1195}
}

@Article{PhysRevLett.61.2635,
  Title                    = {New Monte Carlo technique for studying phase transitions},
  Author                   = {Ferrenberg, Alan M. and Swendsen, Robert H.},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {1988},

  Month                    = {Dec},
  Pages                    = {2635--2638},
  Volume                   = {61},

  Doi                      = {10.1103/PhysRevLett.61.2635},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.61.2635.pdf:PDF},
  Issue                    = {23},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.04.03},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevLett.61.2635}
}

@Book{feynman_statistical_mechanics,
  Title                    = {ファインマン統計力学},
  Author                   = {Richard P. Feynman},
  Publisher                = {シュプリンガー・ジャパン株式会社},
  Year                     = {2011},

  ISBN                     = {978-4-431-100683},
  Timestamp                = {2012.03.15}
}

@Article{1056918,
  Title                    = {Generalized 'write-once' memories},
  Author                   = {Fiat, A. and Shamir, Adi},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1984},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {470-480},
  Volume                   = {30},

  Abstract                 = {Storage media such as digital optical discs, PROM's, or punched cards consist of a number of write-once bit positions (WIT's); each WIT initially contains a "0" that may later be irreversibly overwritten with a "r'. Rivest and Shamir have shown that such write-once memories (WOM's) can be reused very efficiently. Generalized WOM's are considered, in which the basic storage element has more than two possible states and the legal state transitions are described by an arbitrary directed acyclic graph. The capabilities of such memories depend on the depth of the graphs rather than on their size, and the decision problem associated with the generalized WOM's in NP-hard even for 3 -ary symbols rewritten several times or multiple values rewritten once.},
  Doi                      = {10.1109/TIT.1984.1056918},
  File                     = {:PDF\\01056918.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Graph theory;Memory management;Decoding;Gain;H infinity control;Law;Legal factors;Mathematics;Optical microscopy;Repeaters;Surface emitting lasers;Writing},
  Timestamp                = {2015.06.08}
}

@InCollection{Forrest1995,
  Title                    = {Storage Capacity and Learning in Ising-Spin Neural Networks},
  Author                   = {Forrest, Bruce M. and Wallace, David J.},
  Booktitle                = {Models of Neural Networks I},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {1995},
  Editor                   = {Domany, Eytan and van Hemmen, J.Leo and Schulten, Klaus},
  Pages                    = {129-156},
  Series                   = {Physics of Neural Networks},

  Doi                      = {10.1007/978-3-642-79814-6_3},
  File                     = {:PDF\\978-3-642-79814-6.pdf:PDF},
  ISBN                     = {978-3-642-79816-0},
  Timestamp                = {2015.07.21},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-79814-6_3}
}

@Article{1317123,
  Title                    = {Quasicyclic low-density parity-check codes from circulant permutation matrices},
  Author                   = {Fossorier, M.P.C.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2004},

  Month                    = aug,
  Number                   = {8},
  Pages                    = { 1788 - 1793},
  Volume                   = {50},

  Abstract                 = { In this correspondence, the construction of low-density parity-check (LDPC) codes from circulant permutation matrices is investigated. It is shown that such codes cannot have a Tanner graph representation with girth larger than 12, and a relatively mild necessary and sufficient condition for the code to have a girth of 6, 8,10, or 12 is derived. These results suggest that families of LDPC codes with such girth values are relatively easy to obtain and, consequently, additional parameters such as the minimum distance or the number of redundant check sums should be considered. To this end, a necessary condition for the codes investigated to reach their maximum possible minimum Hamming distance is proposed.},
  Doi                      = {10.1109/TIT.2004.831841},
  File                     = {:PDF\\Quasi-Cyclic_Low-Density_Parity-Check_Codes_From_Circulant_Permutation_Matrices.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Hamming distance; LDPC code; QC; Tanner graph representation; circulant permutation matrix; girth value; iterative decoding; low-density parity-check; quasicyclic codes; Hamming codes; cyclic codes; iterative decoding; matrix algebra; parity check codes;}
}

@Article{768759,
  Title                    = {Reduced complexity iterative decoding of low-density parity check codes based on belief propagation},
  Author                   = {Fossorier, M.P.C. and Mihaljevic, M. and Imai, H.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {1999},

  Month                    = may,
  Number                   = {5},
  Pages                    = {673 -680},
  Volume                   = {47},

  Abstract                 = {Two simplified versions of the belief propagation algorithm for fast iterative decoding of low-density parity check codes on the additive white Gaussian noise channel are proposed. Both versions are implemented with real additions only, which greatly simplifies the decoding complexity of belief propagation in which products of probabilities have to be computed. Also, these two algorithms do not require any knowledge about the channel characteristics. Both algorithms yield a good performance-complexity trade-off and can be efficiently implemented in software as well as in hardware, with possibly quantized received values},
  Doi                      = {10.1109/26.768759},
  File                     = {:PDF\\Reduced_Complexity_Iterative_Decoding_of_Low-Density_Parity_Check_Codes_Based_on_Belief_Propagation.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {additive white Gaussian noise channel;belief propagation algorithm;binary code;block code;decoding complexity;fast iterative decoding;hardware;low-density parity check codes;performance-complexity trade-off;probabilities product;quantized received values;reduced complexity iterative decoding;software;AWGN channels;belief maintenance;binary codes;block codes;computational complexity;error detection codes;iterative decoding;probability;}
}

@Book{friedman2009elements,
  Title                    = {The elements of statistical learning},
  Author                   = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  Publisher                = {Springer series in statistics Springer, Berlin},
  Year                     = {2009},
  Volume                   = {1},

  File                     = {:PDF\\978-0-387-84858-7.pdf:PDF},
  Timestamp                = {2015.07.30}
}

@InProceedings{4024287,
  Title                    = {On Gaussian Approximation for Density Evolution of Low-Density Parity-Check Codes},
  Author                   = {Minyue Fu},
  Booktitle                = {Communications, 2006. ICC '06. IEEE International Conference on},
  Year                     = {2006},
  Month                    = {june },
  Pages                    = {1107 -1112},
  Volume                   = {3},

  Abstract                 = {This paper is concerned with density evolution for iterative decoding of low-density parity-check (LDPC) codes. We first study the problem of density evolution computation for regular LDPC codes. For this, we propose a simple computational algorithm based on the ergodicity theory. This method is shown to match very well with explicit calculations of density functions. The second problem we study is about the approach of Gaussian approximation to density evolution. We point out that it is inappropriate to use the mean of the density only to model the iterative decoding process. Instead, both the mean and variance are needed for Gaussian approximation. Finally, we consider the problem of density evolution for irregular LDPC codes. For this, we extend the density evolution algorithm for regular LDPC codes to irregular LDPC codes. We then illustrate that Gaussian approximation is also valid provided that the degree distributions are not wide. A dynamic model is also presented based on Gaussian approximation.},
  Doi                      = {10.1109/ICC.2006.254895},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04024287.pdf:PDF},
  ISSN                     = {8164-9547},
  Timestamp                = {2011.12.02}
}

@InProceedings{1523458,
  Title                    = {An algebraic method for constructing efficiently encodable irregular LDPC codes},
  Author                   = {Fujita, H. and Ohata, M. and Sakaniwa, K.},
  Booktitle                = {Information Theory, 2005. ISIT 2005. Proceedings. International Symposium on},
  Year                     = {2005},
  Month                    = sept.,
  Pages                    = {855 -859},

  Abstract                 = {In this paper we propose an algebraic construction of efficiently encodable irregular LDPC codes. The proposed irregular LDPC codes have not only an efficient encoding algorithm but also guaranteed minimum distances. Simulation results show that the proposed codes perform well compared to randomly constructed irregular LDPC codes},
  Doi                      = {10.1109/ISIT.2005.1523458},
  File                     = {:PDF\\01523458.pdf:PDF},
  Keywords                 = {algebraic construction;encoding algorithm;irregular LDPC codes;algebraic codes;matrix algebra;parity check codes;}
}

@InProceedings{1365312,
  Title                    = {An efficient encoding method for LDPC codes based on cyclic shift},
  Author                   = {Fujita, H. and Sakaniwa, K.},
  Booktitle                = {Information Theory, 2004. ISIT 2004. Proceedings. International Symposium on},
  Year                     = {2004},
  Month                    = june-2 # july,
  Pages                    = { 276},

  Abstract                 = { Low-density parity-check (LDPC) codes are one of the most promising next generation error correcting codes and many investigations shows that LDPC codes suitable for many hardware implementation. Although randomly constructed LDPC codes are usually encoded by using generator matrix, this method requires quadratic time complexity and is not easy to implement. This work presents the encoding of array-type LDPC codes and a special class of Sridhara-Fuja-Tanner (SFT) codes by division circuits as cyclic codes, which are very easy to implement.},
  Doi                      = {10.1109/ISIT.2004.1365312},
  File                     = {:PDF\\An_Efficient_Encoding_Method_for_LDPC_Codes_Based_on_Cyclic_Shift.pdf:PDF},
  Keywords                 = { SFT codes; Sridhara-Fuja-Tanner codes; array-type LDPC codes; cyclic codes; division circuit; error correcting code; low-density parity-check code; cyclic codes; error correction codes; matrix algebra; parity check codes;}
}

@Book{978-3-540-00539-1,
  Title                    = {Representation Theory A First Course},
  Author                   = {Wiliam Fulton and Joe Harries},
  Publisher                = {Springer},
  Year                     = {2004},

  File                     = {:PDF\\3A978-1-4612-0979-9.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.04.04}
}

@InProceedings{4036049,
  Title                    = {Generalized Construction of Quasi-Cyclic Regular LDPC Codes Based on Permutation Matrices},
  Author                   = {Gabidulin, E. and Moinian, A. and Honary, B.},
  Booktitle                = {Information Theory, 2006 IEEE International Symposium on},
  Year                     = {2006},
  Month                    = {july},
  Pages                    = {679 -683},

  Abstract                 = {A new approach is proposed for constructing regular low-density parity-check (LDPC) codes based on tensor product of matrices. In this paper, first a general construction method of regular LDPC codes exploiting permutation matrices is described. Constructed codes have a quasi-cyclic structure with no short cycles of length 4 in their Tanner graph, hence simple encoding while maintaining good performance is achieved. The paper also demonstrates a generalized design, which covers a large family of LDPC codes and number of other construction methods. The new generalized LDPC codes are defined by a small number of parameters and cover a large set of code lengths and rates. Using these codes, LDPC matrices of any column weight and row weight can be constructed. Performance of these codes under iterative decoding compares well with other well-structured as well as random LDPC codes},
  Doi                      = {10.1109/ISIT.2006.261871},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04036049.pdf:PDF},
  Keywords                 = {Tanner graph;code lengths;generalized construction;iterative decoding;low-density parity-check codes;matrix tensor product;permutation matrices;quasi-cyclic regular LDPC codes;quasi-cyclic structure;random LDPC codes;cyclic codes;iterative decoding;matrix algebra;parity check codes;tensors;},
  Timestamp                = {2012.04.17}
}

@InProceedings{6089490,
  Title                    = {Non-binary WOM-codes for multilevel flash memories},
  Author                   = {Gabrys, R. and Yaakobi, E. and Dolecek, L. and Siegel, P.H. and Vardy, A and Wolf, J.K.},
  Booktitle                = {Information Theory Workshop (ITW), 2011 IEEE},
  Year                     = {2011},
  Month                    = {Oct},
  Pages                    = {40-44},

  Abstract                 = {A Write-Once Memory (WOM)-code is a coding scheme that allows information to be written in a memory block multiple times, but in a way that the stored values are not decreased across writes. This work studies non-binary WOM-codes with applications to flash memory. We present two constructions of non-binary WOM-codes that leverage existing high sum-rate WOM-codes defined over smaller alphabets. In many instances, these constructions provide the highest known sum-rates of the non-binary WOM-codes. In addition, we introduce a new class of codes, called level distance WOM-codes, which mitigate the difficulty of programming a flash memory cell by eliminating all small-magnitude level increases. We show how to construct such codes and state an upper bound on their sum-rate.},
  Doi                      = {10.1109/ITW.2011.6089490},
  File                     = {:PDF\\06089490.pdf:PDF},
  Keywords                 = {codes;flash memories;write-once storage;level distance WOM-codes;memory block;multilevel flash memories;non-binary write-once memory codes;Ash;Conferences;Decoding;Encoding;Upper bound;Vectors},
  Timestamp                = {2014.09.03}
}

@InProceedings{6620317,
  Title                    = {Rank-modulation rewriting codes for flash memories},
  Author                   = {Gad, E.E. and Yaakobi, E. and Anxiao Jiang and Bruck, J.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2013 IEEE International Symposium on},
  Year                     = {2013},
  Month                    = {July},
  Pages                    = {704-708},

  Abstract                 = {Current flash memory technology is focused on cost minimization of the stored capacity. However, the resulting approach supports a relatively small number of write-erase cycles. This technology is effective for consumer devices (smart-phones and cameras) where the number of write-erase cycles is small, however, it is not economical for enterprise storage systems that require a large number of lifetime writes. Our proposed approach for alleviating this problem consists of the efficient integration of two key ideas: (i) improving reliability and endurance by representing the information using relative values via the rank modulation scheme and (ii) increasing the overall (lifetime) capacity of the flash device via rewriting codes, namely, performing multiple writes per cell before erasure. We propose a new scheme that combines rank-modulation with rewriting. The key benefits of the new scheme include: (i) the ability to store close to 2 bits per cell on each write, and rewrite the memory close to q times, where q is the number of levels in each cell, and (ii) efficient encoding and decoding algorithms that use the recently proposed polar WOM codes.},
  Doi                      = {10.1109/ISIT.2013.6620317},
  File                     = {:PDF\\06620317.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {flash memories;minimisation;consumer devices;cost minimization;current flash memory technology;decoding algorithms;encoding algorithms;enterprise storage systems;flash device;flash memories;polar WOM codes;rank modulation rewriting codes;rank modulation scheme;smart cameras;smart-phones;stored capacity;write erase cycles;Ash;Decoding;Encoding;Modulation;Silicon;Vectors;Writing},
  Timestamp                = {2014.09.03}
}

@InProceedings{6875111,
  Title                    = {Polar coding for noisy write-once memories},
  Author                   = {Gad, Eyal En and Li, Yue and Kliewer, Joerg and Langberg, Michael and Jiang, Anxiao and Bruck, Jehoshua},
  Booktitle                = {Information Theory (ISIT), 2014 IEEE International Symposium on},
  Year                     = {2014},
  Month                    = {June},
  Pages                    = {1638-1642},

  Abstract                 = {We consider the noisy write-once memory (WOM) model to capture the behavior of data-storage devices such as flash memories. The noisy WOM is an asymmetric channel model with non-causal state information at the encoder. We show that a nesting of non-linear polar codes achieves the corresponding Gelfand-Pinsker bound with polynomial complexity.},
  Doi                      = {10.1109/ISIT.2014.6875111},
  File                     = {:PDF\\06875111.pdf:PDF},
  Timestamp                = {2014.08.26}
}

@Article{1057683,
  Title                    = {Low-density parity-check codes},
  Author                   = {Gallager, R.},
  Journal                  = {Information Theory, IRE Transactions on},
  Year                     = {1962},

  Month                    = {jan.},
  Number                   = {1},
  Pages                    = {21 -28},
  Volume                   = {8},

  Doi                      = {10.1109/TIT.1962.1057683},
  File                     = {:PDF\\Low-Density_Parity-Check_Codes_Gallager.pdf:PDF},
  ISSN                     = {0096-1000},
  Keywords                 = {Error-correcting codes;Parity checks;}
}

@Book{Gallager:1968:ITR:578869,
  Title                    = {Information Theory and Reliable Communication},
  Author                   = {Gallager, Robert G.},
  Publisher                = {John Wiley \& Sons, Inc.},
  Year                     = {1968},

  Address                  = {New York, NY, USA},

  ISBN                     = {0471290483},
  Timestamp                = {2014.08.01}
}

@Book{GallagerMITbook,
  Title                    = {Low-Density Parity Check Codes},
  Author                   = {Robert G. Gallager},
  Publisher                = {THE MIT PRESS},
  Year                     = {1963},

  File                     = {:PDF\\gallager_LDPC_book.pdf:PDF},
  Timestamp                = {2012.01.13}
}

@Book{Singular_Introduction_to_Commutative_Algebra,
  Title                    = {A Singular Introduction to Commutative Algebra},
  Author                   = {Gert-Martin Greuel, Gerhard Pfister},
  Publisher                = {Springer},
  Year                     = {2008},

  Abstract                 = {This substantially enlarged second edition aims to lead a further stage in the computational revolution in commutative algebra. This is the first handbook/tutorial to extensively deal with SINGULAR. Among the book’s most distinctive features is a new, completely unified treatment of the global and local theories. Another feature of the book is its breadth of coverage of theoretical topics in the portions of commutative algebra closest to algebraic geometry, with algorithmic treatments of almost every topic.},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\A_SINGULAR_Introduction_to_Commutative_Algebra.pdf:PDF},
  ISBN                     = {978-3540735410},
  Timestamp                = {2013.06.03}
}

@InProceedings{996673,
  Title                    = {Stress induced leakage current and bulk oxide trapping: temperature evolution},
  Author                   = {Ghidini, G. and Sebastiani, A. and Brazzelli, D.},
  Booktitle                = {Reliability Physics Symposium Proceedings, 2002. 40th Annual},
  Year                     = {2002},
  Pages                    = { 415 - 416},

  Doi                      = {10.1109/RELPHY.2002.996673},
  ISSN                     = { },
  Keywords                 = { 10 nm; 8 nm; CMOS process; SILC temperature evolution; STI; bulk oxide trapping; degraded thick oxide conduction; dual-gate technology; fixed field stationary SILC; flash cell scaling; flat area capacitors; gate leakage current; multiple trap assisted tunneling; shallow trench isolation; stable charge measurement; stable charge position; stress induced leakage current; thermal cycling; tunnel oxide thickness; tunneling front model; CMOS integrated circuits; annealing; electric charge; electron traps; flash memories; integrated circuit measurement; isolation technology; leakage currents; stress effects; tunnelling;}
}

@Article{1054126,
  Title                    = {On a class of majority-logic decodable cyclic codes},
  Author                   = { Goethals, J.-M. and Delsarte, P.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1968},

  Month                    = mar,
  Number                   = {2},
  Pages                    = { 182 - 188},
  Volume                   = {14},

  Abstract                 = { A new infinite class of cyclic codes is studied. Codes of this class can be decoded in a step-by-step manner, using` majority logic. Some previously known codes fall in this class, and thus admit simpler decoding procedures. As random error-correcting codes, the codes are nearly as powerful as the Bose-Chaudhuri codes.},
  Doi                      = {10.1109/TIT.1968.1054126},
  File                     = {:PDF\\On_a_Class_of_Majority-Logic_Decodable_Cyclic_Codes.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Cyclic codes; Information theory group; Majority logic decoding;}
}

@Article{1671829,
  Title                    = {The Relationship Between Two Fast Fourier Transforms},
  Author                   = {Good, I.J.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1971},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {310-317},
  Volume                   = {C-20},

  Abstract                 = {The purpose of this note is to show as clearly as possible the mathematical relationship between the two basic fast methods used for the calculation of discrete Fourier transforms and to generalize one of the methods a little further. This method applies to all those linear transformations whose matrices are expressible as direct products.},
  Doi                      = {10.1109/T-C.1971.223236},
  File                     = {:PDF\\310-317.pdf:PDF},
  ISSN                     = {0018-9340},
  Keywords                 = {Algorithms, circulices, direct product of matrices, discrete Fourier transforms, fast Fourier transforms, frequency analysis, Hadamard transform, harmonic analysis, multidimensional linear transformation, sparse matrices.;Discrete Fourier transforms;Equations;Fast Fourier transforms;Flexible printed circuits;Fourier transforms;Frequency;Harmonic analysis;Multidimensional systems;Sparse matrices;Vectors;Algorithms, circulices, direct product of matrices, discrete Fourier transforms, fast Fourier transforms, frequency analysis, Hadamard transform, harmonic analysis, multidimensional linear transformation, sparse matrices.},
  Timestamp                = {2015.05.15}
}

@Article{4155120,
  Title                    = {FFT-Based BP Decoding of General LDPC Codes Over Abelian Groups},
  Author                   = {Goupil, A. and Colas, M. and Gelle, G. and Declercq, D.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {644-649},
  Volume                   = {55},

  Abstract                 = {We introduce a wide class of low-density parity-check (LDPC) codes, large enough to include LDPC codes over finite fields, rings, or groups, as well as some nonlinear codes. A belief-propagation decoding procedure with the same complexity as for the decoding of LDPC codes over finite fields is also presented. Moreover, an encoding procedure is developed},
  Doi                      = {10.1109/TCOMM.2007.894089},
  File                     = {:PDF\\04155120.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {decoding;fast Fourier transforms;nonlinear codes;parity check codes;Abelian groups;FFT-based BP decoding;LDPC codes;belief-propagation decoding;low-density parity-check codes;nonlinear codes;Belief propagation;Communications Society;Decoding;Encoding;Equations;Galois fields;Linear code;Parity check codes;Turbo codes;Fourier transform (FT);generalized low-density parity-check (LDPC) codes;group},
  Timestamp                = {2015.05.13}
}

@Article{Gross1984431,
  Title                    = {The simplest spin glass },
  Author                   = {D.J. Gross and M. Mezard},
  Journal                  = {Nuclear Physics B },
  Year                     = {1984},
  Number                   = {4},
  Pages                    = {431 - 452},
  Volume                   = {240},

  Abstract                 = {We study a system of Ising spins with quenched random infinite ranged p-spin interactions. For p → ∞, we can solve this model exactly either by a direct microcanonical argument, or through the introduction of replicas and Parisi's ultrametric ansatz for replica symmetry breaking, or by means of \{TAP\} mean field equations. Although the model is extremely simple it retains the characteristic features of a spin glass. We use it to confirm the methods that have been applied in more complicated situations and to explicitlu exhibit the structure of the spin glass phase. },
  Doi                      = {http://dx.doi.org/10.1016/0550-3213(84)90237-2},
  File                     = {:PDF\\Gross1984431.pdf:PDF},
  ISSN                     = {0550-3213},
  Timestamp                = {2015.02.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0550321384902372}
}

@Article{Gupta:2009:DFT:1508284.1508271,
  Title                    = {DFTL: A Flash Translation Layer Employing Demand-based Selective Caching of Page-level Address Mappings},
  Author                   = {Gupta, Aayush and Kim, Youngjae and Urgaonkar, Bhuvan},
  Journal                  = {SIGPLAN Not.},
  Year                     = {2009},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {229--240},
  Volume                   = {44},

  Acmid                    = {1508271},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1508284.1508271},
  File                     = {:PDF\\p229-gupta.pdf:PDF},
  ISSN                     = {0362-1340},
  Issue_date               = {March 2009},
  Keywords                 = {flash management, flash translation layer, storage system},
  Numpages                 = {12},
  Publisher                = {ACM},
  Timestamp                = {2015.06.15},
  Url                      = {http://doi.acm.org/10.1145/1508284.1508271}
}

@Article{782097,
  Title                    = {Improved decoding of Reed-Solomon and algebraic-geometry codes},
  Author                   = {Guruswami, V. and Sudan, M.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {sep},
  Number                   = {6},
  Pages                    = {1757 -1767},
  Volume                   = {45},

  Abstract                 = {Given an error-correcting code over strings of length n and an arbitrary input string also of length n, the list decoding problem is that of finding all codewords within a specified Hamming distance from the input string. We present an improved list decoding algorithm for decoding Reed-Solomon codes. The list decoding problem for Reed-Solomon codes reduces to the following ldquo;curve-fitting rdquo; problem over a field F: given n points ((xi middot;yi))i=1 n, xi, yi isin;F, and a degree parameter k and error parameter e, find all univariate polynomials p of degree at most k such that yi=p(xi) for all but at most e values of i isin;(1,...,n). We give an algorithm that solves this problem for e lt;n- radic;(kn), which improves over the previous best result, for every choice of k and n. Of particular interest is the case of k/n gt;1/3, where the result yields the first asymptotic improvement in four decades. The algorithm generalizes to solve the list decoding problem for other algebraic codes, specifically alternant codes (a class of codes including BCH codes) and algebraic-geometry codes. In both cases, we obtain a list decoding algorithm that corrects up to n- radic;(n(n-d')) errors, where n is the block length and d' is the designed distance of the code. The improvement for the case of algebraic-geometry codes extends the methods of Shokrollahi and Wasserman (see in Proc. 29th Annu. ACM Symp. Theory of Computing, p.241-48, 1998) and improves upon their bound for every choice of n and d'. We also present some other consequences of our algorithm including a solution to a weighted curve-fitting problem, which may be of use in soft-decision decoding algorithms for Reed-Solomon codes},
  Doi                      = {10.1109/18.782097},
  File                     = {:PDF\\00782097.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {BCH codes;Hamming distance;Reed-Solomon codes;algebraic-geometry codes;alternant codes;asymptotic improvement;block length;bound;code distance;codewords;curve-fitting problem;degree parameter;error parameter;error-correcting code;input string;list decoding algorithm;polynomial time algorithm;soft-decision decoding algorithms;string length;univariate polynomials;weighted curve-fitting problem;BCH codes;Reed-Solomon codes;algebraic geometric codes;coding errors;curve fitting;decoding;},
  Timestamp                = {2011.04.18}
}

@Article{4202100,
  Title                    = {Layered BP Decoding for Rate-Compatible Punctured LDPC Codes},
  Author                   = {Jeongseok Ha and Klinc, D. and Jini Kwon and McLaughlin, S.W.},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2007},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {440-442},
  Volume                   = {11},

  Abstract                 = {Rate-compatible punctured LDPC codes have shown to perform well over a wide variety of code rates, both theoretically and practically. However it has been reported that the belief propagation (BP) decoding for these codes converges slower than for unpunctured codes. Layered BP algorithm is a modified BP algorithm that accelerates the decoding convergence by means of sequential scheduling of check node updates. In this letter, we propose an efficient scheduling of check node updates for rate-compatible punctured LDPC codes that performs well. We show that the convergence speed of the proposed scheduling outperforms conventional (random) scheduling and conventional BP decoding. Performance improvements become more distinctive with the growing fraction of punctured bits},
  Doi                      = {10.1109/LCOMM.2007.061962},
  File                     = {:PDF\\04202100.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = {decoding;parity check codes;scheduling;belief propagation decoding;check node update;layered BP algorithm;low density parity check codes;rate-compatible punctured LDPC code;sequential scheduling;Acceleration;Belief propagation;Bit error rate;Communication systems;Convergence;Costs;Iterative decoding;Parity check codes;Scheduling algorithm;Throughput},
  Timestamp                = {2015.05.22}
}

@Article{476233,
  Title                    = {Effective construction of algebraic geometry codes},
  Author                   = {Hache, G. and Le Brigand, D.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1995},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {1615 -1628},
  Volume                   = {41},

  Abstract                 = {We intend to show that algebraic geometry codes may be constructed easily using blowing-up theory for any projective plane algebraic curve. Our paper is based on a paper by Le Brigand and Risler (1988). We try to be as explicit as possible},
  Doi                      = {10.1109/18.476233},
  File                     = {:PDF\\Effective_Construction_of_Algebraic_Geometry_Codes.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {algebraic geometry codes;blowing-up theory;code construction;geometric Goppa codes;projective plane algebraic curve;Goppa codes;algebraic geometric codes;}
}

@Article{hagenauer1999decoding,
  Title                    = {Decoding and Equalization with Analog Non-linear Networks},
  Author                   = {Hagenauer, Joachim and Offer, Elke and Measson, Cyril and M{\"o}rz, Matthias},
  Journal                  = {European Transactions on Telecommunications},
  Year                     = {1999},
  Number                   = {6},
  Pages                    = {659--680},
  Volume                   = {10},

  File                     = {:PDF\\10.1.1.34.9534.pdf:PDF},
  Publisher                = {Wiley Online Library},
  Timestamp                = {2015.05.15}
}

@Article{PhysRev_123_85,
  Title                    = {Tunneling from an Independent-Particle Point of View},
  Author                   = {Harrison, Walter A.},
  Journal                  = {Phys. Rev.},
  Year                     = {1961},

  Month                    = {Jul},
  Pages                    = {85--89},
  Volume                   = {123},

  Doi                      = {10.1103/PhysRev.123.85},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRev_123_85.pdf:PDF},
  Issue                    = {1},
  Publisher                = {American Physical Society},
  Timestamp                = {2011.09.15},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRev.123.85}
}

@Article{1055198,
  Title                    = {On the structure of generalized finite-geometry codes},
  Author                   = { Hartmann, C. and Ducey, J. and Rudolph, L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1974},

  Month                    = {mar},
  Number                   = {2},
  Pages                    = { 240 - 252},
  Volume                   = {20},

  Abstract                 = { Some new results on the structure of generalized finite-geometry codes are presented.},
  Doi                      = {10.1109/TIT.1974.1055198},
  File                     = {:PDF\\01055198.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Geometry codes;},
  Timestamp                = {2011.04.19}
}

@Article{476247,
  Title                    = {Systematic encoding via Gr\"{o}bner bases for a class of algebraic-geometric Goppa codes},
  Author                   = {Heegard, C. and Little, J. and Saints, K.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1995},

  Month                    = {nov},
  Number                   = {6},
  Pages                    = {1752 -1761},
  Volume                   = {41},

  Abstract                 = {Any linear code with a nontrivial automorphism has the structure of a module over a polynomial ring. The theory of Grobner bases for modules gives a compact description and implementation of a systematic encoder. We present examples of algebraic-geometric Goppa codes that can be encoded by these methods, including the one-point Hermitian codes },
  Doi                      = {10.1109/18.476247},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00476247.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Grobner bases;algebraic-geometric Goppa codes;linear code;modules;nontrivial automorphism;one-point Hermitian codes;polynomial ring;systematic encoding;Goppa codes;algebraic geometric codes;linear algebra;linear codes;polynomials;},
  Timestamp                = {2012.04.27}
}

@Article{Herlihy:1993:TMA:173682.165164,
  Title                    = {Transactional memory: architectural support for lock-free data structures},
  Author                   = {Herlihy, Maurice and Moss, J. Eliot B.},
  Journal                  = {SIGARCH Comput. Archit. News},
  Year                     = {1993},

  Month                    = {May},
  Pages                    = {289--300},
  Volume                   = {21},

  Acmid                    = {165164},
  Address                  = {New York, NY, USA},
  Doi                      = {http://doi.acm.org/10.1145/173682.165164},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\p289-herlihy.pdf:PDF},
  ISSN                     = {0163-5964},
  Issue                    = {2},
  Issue_date               = {May 1993},
  Numpages                 = {12},
  Publisher                = {ACM},
  Timestamp                = {2011.11.25},
  Url                      = {http://doi.acm.org/10.1145/173682.165164}
}

@Article{Hess2002425,
  Title                    = {Computing Riemann–Roch Spaces in Algebraic Function Fields and Related Topics},
  Author                   = {F. Hess},
  Journal                  = {Journal of Symbolic Computation},
  Year                     = {2002},
  Number                   = {4},
  Pages                    = {425 - 445},
  Volume                   = {33},

  Abstract                 = {We develop a simple and efficient algorithm to compute Riemann–Roch spaces of divisors in general algebraic function fields which does not use the Brill–Noether method of adjoints or any series expansions. The basic idea also leads to an elementary proof of the Riemann–Roch theorem. We describe the connection to the geometry of numbers of algebraic function fields and develop a notion and algorithm for divisor reduction. An important application is to compute in the divisor class group of an algebraic function field.},
  Doi                      = {10.1006/jsco.2001.0513},
  File                     = {:PDF\\Hess2002425.pdf:PDF},
  ISSN                     = {0747-7171},
  Timestamp                = {2012.05.17},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0747717101905139}
}

@Article{887869,
  Title                    = {On the equivalence of the Berlekamp-Massey and the Euclidean algorithms for decoding},
  Author                   = {Heydtmann, A.E. and Jensen, J.M.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2000},

  Month                    = nov,
  Number                   = {7},
  Pages                    = {2614 -2624},
  Volume                   = {46},

  Doi                      = {10.1109/18.887869},
  File                     = {:PDF\\On_the_Equivalence_of_the_Berlekamp-Massey_and_the_Euclidean_Algorithms_for_Decoding.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Berlekamp-Massey algorithm;Euclidean algorithm;Reed Solomon codes;alternant codes;arithmetic;decoding;error coevaluator;error evaluator;error locator;fundamental iterative algorithm;key equation;syndrome matrix;error analysis;iterative decoding;matrix algebra;}
}

@Article{hinton2002training,
  Title                    = {Training products of experts by minimizing contrastive divergence},
  Author                   = {Hinton, Geoffrey E},
  Journal                  = {Neural computation},
  Year                     = {2002},
  Number                   = {8},
  Pages                    = {1771--1800},
  Volume                   = {14},

  File                     = {:PDF\\nccd.pdf:PDF},
  Publisher                = {MIT Press},
  Timestamp                = {2015.07.30}
}

@Article{hinton2006fast,
  Title                    = {A fast learning algorithm for deep belief nets},
  Author                   = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  Journal                  = {Neural computation},
  Year                     = {2006},
  Number                   = {7},
  Pages                    = {1527--1554},
  Volume                   = {18},

  File                     = {:PDF\\fastnc.pdf:PDF},
  Publisher                = {MIT Press},
  Timestamp                = {2015.07.29}
}

@InProceedings{4895509,
  Title                    = {Approximate examination of trapping sets of LDPC codes using the probabilistic algorithm},
  Author                   = {Hirotomo, M. and Konishi, Y. and Morii, M.},
  Booktitle                = {Information Theory and Its Applications, 2008. ISITA 2008. International Symposium on},
  Year                     = {2008},
  Month                    = {dec.},
  Pages                    = {1 -6},

  Abstract                 = {The performance of LDPC codes decoded by iterative algorithms depends on the structural properties of their underlying Tanner graphs. For general memoryless channels, error patterns dominating the bit and frame error probabilities at the error floor region are termed trapping sets. In this paper, we propose an effective method for finding small-size trapping sets of LDPC codes. In the proposed method, a probabilistic algorithm to find low-weight codewords is applied to finding small trapping sets of LDPC codes. Furthermore, we show numerical results of examining small trapping sets of (504, 252) and (1008, 504) LDPC codes.},
  Doi                      = {10.1109/ISITA.2008.4895509},
  File                     = {:PDF\\04895509.pdf:PDF},
  Keywords                 = {Bit error rate;Error analysis;Error probability;Information theory;Iterative algorithms;Iterative decoding;Linear code;Memoryless systems;Parity check codes;Sparse matrices;error statistics;graph theory;iterative decoding;parity check codes;LDPC codes;Tanner graphs;error floor region;error patterns;frame error probabilities;general memoryless channels;iterative algorithms;low-weight codewords;probabilistic algorithm;trapping sets;},
  Timestamp                = {2013.02.05}
}

@InProceedings{5654357,
  Title                    = {Detailed evaluation of error floors of LDPC codes using the probabilistic algorithm},
  Author                   = {Hirotomo, M. and Morii, M.},
  Booktitle                = {Information Theory and its Applications (ISITA), 2010 International Symposium on},
  Year                     = {2010},
  Month                    = {oct},
  Pages                    = {513 -518},

  Abstract                 = {The performance of LDPC codes decoded by iterative algorithm depends on the structural properties of their underlying Tanner graphs. For discrete memoryless channels, error patterns dominating the frame error rate (FER) in the error floor region are termed trapping set. We have shown approximate examinations of small trapping sets of LDPC codes using the probabilistic algorithm. In this paper, we propose an efficient method for the detailed evaluation of the FER of LDPC codes in the error floor region. The accuracy of the FER lies on the failure probability of the number of trapping sets determined by our probabilistic algorithm and the FER estimated by the importance sampling.},
  Doi                      = {10.1109/ISITA.2010.5654357},
  File                     = {:PDF\\05654357.pdf:PDF},
  Keywords                 = {Charge carrier processes;Decoding;Monte Carlo methods;Parity check codes;Probabilistic logic;Sparse matrices;Vectors;error statistics;parity check codes;LDPC code;error floor region;frame error rate;importance sampling;low density parity check code;probabilistic algorithm;tanner graph;},
  Timestamp                = {2013.01.25}
}

@InProceedings{6970813,
  Title                    = {Syndrome decoding of symbol-pair codes},
  Author                   = {Hirotomo, M. and Takita, M. and Morii, M.},
  Booktitle                = {Information Theory Workshop (ITW), 2014 IEEE},
  Year                     = {2014},
  Month                    = {Nov},
  Pages                    = {162-166},

  Abstract                 = {Cassuto and Blaum proposed new error correcting codes which are called symbol-pair codes. They gave a coding framework for channels whose outputs are overlapping pairs of symbols in storage applications. It is called symbol-pair read channel. The pair distance and pair error are used in symbol-pair read channel. Cassuto et al. and showed Yaakobi et al. presented decoding algorithms for symbol-pair codes. However, their decoding algorithms cannot always correct errors whose number is not more than half the minimum pair distance. In this paper, we propose a new decoding algorithm using the syndrome of symbol-pair codes. In addition, we show that the proposed algorithm can correct all pair errors within the pair error correcting capability.},
  Doi                      = {10.1109/ITW.2014.6970813},
  File                     = {:PDF\\06970813.pdf:PDF},
  ISSN                     = {1662-9019},
  Keywords                 = {decoding;error correction codes;coding framework;error correcting code;pair distance;pair error correcting capability;storage application;symbol-pair code;symbol-pair read channel;syndrome decoding algorithm;DH-HEMTs;Decoding;Educational institutions;Error correction codes;Hamming distance;Measurement;Vectors},
  Timestamp                = {2015.06.09}
}

@InProceedings{1204466,
  Title                    = {LDPC code construction with flexible hardware implementation},
  Author                   = {Hocevar, D.E.},
  Booktitle                = {Communications, 2003. ICC '03. IEEE International Conference on},
  Year                     = {2003},
  Month                    = may,
  Pages                    = { 2708 - 2712 vol.4},
  Volume                   = {4},

  Abstract                 = { This paper presents an LDPC code construction technique for irregular codes of various block sizes and code rates, thus obtaining the performance benefit of irregular distributions. It represents an extension of the methods presented by Sridhara et al. [2001] for regular codes. More importantly, an efficient and practical decoder architecture is also presented that achieves flexibility in block size and code rate for this broad family of codes, a capability not present in other approaches. This decoder can also achieve a high degree of parallelism, thus exploiting one of the benefits of belief propagation.},
  Doi                      = {10.1109/ICC.2003.1204466},
  File                     = {:PDF\\LDPC_Code_Construction_with_Flexible_Hardware_Implementation.pdf:PDF},
  Keywords                 = { LDPC code construction technique; addressing; belief propagation; block sizes; code rates; decoder architecture; error correction performance; flexible hardware decoder; irregular codes; irregular distributions; parallelism; routing; codecs; error correction codes; iterative decoding; parity check codes;}
}

@Article{29471,
  Title                    = {Systolic Gaussian elimination over GF(p) with partial pivoting},
  Author                   = {Hochet, B. and Quinton, P. and Robert, Y.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1989},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {1321 -1324},
  Volume                   = {38},

  Abstract                 = {A systolic architecture is proposed for the triangularization by means of the Gaussian elimination algorithm of large dense n times;n matrices over GF(p), where p is a prime number. The solution of large dense linear systems over GF(p) is the major computational step in various algorithms issuing from arithmetic number theory and computer algebra. The proposed architecture implements the elimination with partial pivoting, although the operation of the array remains purely systolic. Extension of the array to the complete solution of a linear system Ax=b over GF(p) is also considered},
  Doi                      = {10.1109/12.29471},
  File                     = {:PDF\\Systolic_Gaussian_Elimination_over_GF(p)_with_Partial_Pivoting.pdf:PDF},
  ISSN                     = {0018-9340},
  Keywords                 = {arithmetic number theory;computer algebra;large dense linear systems;partial pivoting;prime number;systolic Gaussian elimination;systolic architecture;triangularization;digital arithmetic;number theory;parallel architectures;}
}

@Article{476214,
  Title                    = {On the decoding of algebraic-geometric codes},
  Author                   = {Hoholdt, T. and Pellikaan, R.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1995},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1589-1614},
  Volume                   = {41},

  Abstract                 = {This paper provides a survey of the existing literature on the decoding of algebraic-geometric codes. Definitions, theorems, and cross references will be given. We show what has been done, discuss what still has to be done, and pose some open problems},
  Doi                      = {10.1109/18.476214},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00476214.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Goppa codes;algebraic geometric codes;decoding;reviews;Goppa codes;algebraic-geometric codes;algorithm;decoding;definitions;theorems;Concrete;Decoding;Elliptic curves;Error correction;Galois fields;Helium;Mathematics;Poles and zeros;Reed-Solomon codes;Strontium},
  Timestamp                = {2014.05.21}
}

@Article{hopfield1982neural,
  Title                    = {Neural networks and physical systems with emergent collective computational abilities},
  Author                   = {Hopfield, John J},
  Journal                  = {Proceedings of the national academy of sciences},
  Year                     = {1982},
  Number                   = {8},
  Pages                    = {2554--2558},
  Volume                   = {79},

  File                     = {:PDF\\PNAS-1982-Hopfield-2554-8.pdf:PDF},
  Publisher                = {National Acad Sciences},
  Timestamp                = {2015.07.30}
}

@InProceedings{6970903,
  Title                    = {Local rank modulation for flash memories},
  Author                   = {Horovitz, M. and Etzion, T.},
  Booktitle                = {Information Theory Workshop (ITW), 2014 IEEE},
  Year                     = {2014},
  Month                    = {Nov},
  Pages                    = {606-610},

  Abstract                 = {Local rank modulation scheme was suggested recently for representing information in flash memories in order to overcome drawbacks of rank modulation. For 0 <; s ≤ t ≤ n with s divides n, an (s, t, n)-LRM scheme is a local rank modulation scheme where the n cells are locally viewed cyclically through a sliding window of size t resulting in a sequence of small permutations which requires less comparisons and less distinct values. The gap between two such windows equals to s. In this work, encoding, decoding, and asymptotic enumeration of the (1, 3, n)-LRM scheme is studied. The techniques which are suggested have some generalizations for (1, t, n)-LRM, t > 3, but the proofs will become more complicated. The enumeration problem is presented also as a purely combinatorial problem. Finally, we prove the conjecture that the size of a constant weight (1, 2, n)-LRM Gray code with weight two is at most 2n.},
  Doi                      = {10.1109/ITW.2014.6970903},
  File                     = {:PDF\\06970903.pdf:PDF},
  ISSN                     = {1662-9019},
  Keywords                 = {combinatorial mathematics;flash memories;LRM gray code;asymptotic enumeration;combinatorial problem;decoding;encoding;flash memories;local rank modulation;Decoding;Encoding;Law;Manganese;Modulation;Reflective binary codes},
  Timestamp                = {2015.03.31}
}

@Article{924876,
  Title                    = {Performance analysis and code optimization of low density parity-check codes on Rayleigh fading channels},
  Author                   = {Jilei Hou and Siegel, P.H. and Milstein, L.B.},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {2001},

  Month                    = {may},
  Number                   = {5},
  Pages                    = {924 -934},
  Volume                   = {19},

  Abstract                 = {A numerical method has been presented to determine the noise thresholds of low density parity-check (LDPC) codes that employ the message passing decoding algorithm on the additive white Gaussian noise (AWGN) channel. In this paper, we apply the technique to the uncorrelated flat Rayleigh fading channel. Using a nonlinear code optimization technique, we optimize irregular LDPC codes for such a channel. The thresholds of the optimized irregular LDPC codes are very close to the Shannon limit for this channel. For example, at rate one-half, the optimized irregular LDPC code has a threshold only 0.07 dB away from the capacity of the channel. Furthermore, we compare simulated performance of the optimized irregular LDPC codes and turbo codes on a land mobile channel, and the results indicate that at a block size of 3072, irregular LDPC codes can outperform turbo codes over a wide range of mobile speeds},
  Doi                      = {10.1109/49.924876},
  File                     = {:PDF\\00924876.pdf:PDF},
  ISSN                     = {0733-8716},
  Keywords                 = {AWGN;Additive white noise;Channel capacity;Decoding;Fading;Gaussian noise;Message passing;Parity check codes;Performance analysis;Turbo codes;AWGN channels;Rayleigh channels;channel capacity;error correction codes;error detection codes;land mobile radio;nonlinear codes;optimisation;turbo codes;AWGN channel;Shannon limit;additive white Gaussian noise channel;block size;channel capacity;channel coding;code thresholds;error correcting codes;land mobile channel;low density parity-check codes;message passing decoding algorithm;mobile speeds;noise thresholds;nonlinear code optimization;numerical method;optimized irregular LDPC codes;performance analysis;simulated performance;turbo codes;uncorrelated flat Rayleigh fading channel;},
  Timestamp                = {2013.01.21}
}

@Article{5737850,
  Title                    = {Write Activity Minimization for Nonvolatile Main Memory Via Scheduling and Recomputation},
  Author                   = {Jingtong Hu and Wei-Che Tseng and Xue, C.J. and Qingfeng Zhuge and Yingchao Zhao and Sha, E.H.-M.},
  Journal                  = {Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {584-592},
  Volume                   = {30},

  Abstract                 = {Nonvolatile memories such as Flash memory, phase change memory (PCM), and magnetic random access memory (MRAM) have many desirable characteristics for embedded systems to employ them as main memory. However, there are two common challenges we need to answer before we can apply nonvolatile memory as main memory practically. First, nonvolatile memory has limited write/erase cycles compared to DRAM. Second, a write operation is slower than a read operation on nonvolatile memory. These two challenges can be answered by reducing the number of write activities on nonvolatile main memory. In this paper, we proposed two optimization techniques, write-aware scheduling and recomputation, to minimize write activities on nonvolatile memory. With the proposed techniques, we can both speed up the completion time of programs and extend nonvolatile memory's lifetime. The experimental results show that the proposed techniques can reduce the number of write activities on nonvolatile memory by 55.71% on average. Thus, the lifetime of nonvolatile memory is extended to 2.5 times as long as before on average. The completion time of programs can be reduced by 56.67% on systems with NOR Flash memory and by 47.63% on systems with NAND Flash memory on average.},
  Doi                      = {10.1109/TCAD.2010.2097307},
  File                     = {:PDF\\05737850.pdf:PDF},
  ISSN                     = {0278-0070},
  Keywords                 = {MRAM devices;embedded systems;flash memories;processor scheduling;DRAM;MRAM;NAND Flash memory;PCM;embedded systems;flash memory;magnetic random access memory;nonvolatile main memory;phase change memory;write activity minimization;Embedded systems;Flash memory;Nonvolatile memory;Optimal scheduling;Random access memory;Schedules;System-on-a-chip;Data recomputation;MRAM;SPM;flash memory;nonvolatile memory;phase change memory;scheduling},
  Timestamp                = {2015.06.10}
}

@InProceedings{5521139,
  Title                    = {Minimizing write activities to non-volatile memory via scheduling and recomputation},
  Author                   = {Jingtong Hu and Xue, C.J. and Wei-Che Tseng and Qingfeng Zhuge and Sha, E.H.-M.},
  Booktitle                = {Application Specific Processors (SASP), 2010 IEEE 8th Symposium on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {101-106},

  Abstract                 = {Non-volatile memories, such as flash memory, Phase Change Memory (PCM), and Magnetic Random Access Memory (MRAM), have many desirable characteristics for embedded DSP systems to employ them as main memory. These characteristics include low-cost, shock-resistivity, non-volatility, power-economy and high density. However, there are two common challenges we need to answer before we can apply non-volatile memory as main memory practically. First, non-volatile memory has limited write/erase cycles compared to DRAM. Second, a write operation is slower than a read operation on non-volatile memory. These two challenges can be answered by reducing the number of write activities on non-volatile main memory. In this paper, we propose two optimization techniques, write-aware scheduling and recomputation, to minimize write activities on non-volatile memory. With the proposed techniques, we can both speed up the completion time of programs and extend non-volatile memory's lifetime. The experimental results show that the proposed techniques can reduce the number of write activities on non-volatile memory by 55.71% on average. Thus, the lifetime of non-volatile memory is extend to 2.5 times as long as before on average. The completion time of programs can be reduced by 55.32% on systems with NOR flash memory and by 40.69% on systems with NAND flash memory on average.},
  Doi                      = {10.1109/SASP.2010.5521139},
  File                     = {:PDF\\05521139.pdf:PDF},
  Keywords                 = {flash memories;random-access storage;NAND flash memory;embedded DSP system;magnetic random access memory;non-volatile memory;non-volatility;phase change memory;power economy;recomputation;shock resistivity;write activities;write-aware scheduling;Computer science;Digital signal processing;Embedded system;Flash memory;Nonvolatile memory;Phase change materials;Phase change memory;Random access memory;Read-write memory;Scanning probe microscopy},
  Timestamp                = {2015.06.10}
}

@InProceedings{5555509,
  Title                    = {A note on relationship between algebraic geometric codes and LDPC codes},
  Author                   = {Wanbao Hu and Huaping Cai and Yanxia Wu and Zhen Wang},
  Booktitle                = {Signal Processing Systems (ICSPS), 2010 2nd International Conference on},
  Year                     = {2010},
  Month                    = july,
  Pages                    = {V1-56 -V1-58},
  Volume                   = {1},

  Abstract                 = {Low-density parity-check (LDPC) codes constructed by a sparse parity-check matrix are of very fast encoding and decoding algorithms. Another kind of codes, which improved the well-known Gilbert-Varshamov bound, are algebraic geometry codes (Goppa geometry codes) from algebraic curves over finite fields. In the note, we analyze their characteristic of the two class of codes and show that the algebraic geometric codes are seldom LDPC codes.},
  Doi                      = {10.1109/ICSPS.2010.5555509},
  File                     = {:\\\\homepd\\pd6\\ユニット管理\\refereces\\PDF\\05555509.pdf:PDF},
  Keywords                 = {Gilbert-Varshamov bound;LDPC code;algebraic curves;algebraic geometric code;decoding algorithm;encoding algorithm;low density parity check code;sparse parity check matrix;algebraic geometric codes;decoding;parity check codes;sparse matrices;}
}

@InProceedings{4410966,
  Title                    = {Error Floor Estimation of Long LDPC Codes on Partial Response Channels},
  Author                   = {Xinde Hu and Kumar, B.V.K.V. and Zongwang Li and Barndt, R.},
  Booktitle                = {Global Telecommunications Conference, 2007. GLOBECOM '07. IEEE},
  Year                     = {2007},
  Pages                    = {259-264},

  Abstract                 = {The presence of error floor in low density parity check (LDPC) codes is of great concern for potential applications of LDPC codes to data storage channels, which require the error correcting code (ECC) to maintain the near-capacity error correcting performance at frame error rate as low as 10-12. In order to investigate the error floor of LDPC codes under partial response channels used in data storage systems, we propose a new estimation method combining analytical tools and simulation, based on the concept of trapping sets. The definition of trapping sets is based on the dominant error patterns observed in the decoding process. The goal is to accurately estimate the error rate in the error floor region for certain types of LDPC codes under the partial response channel and further extend the frame error rate down to 10-14 or lower. Towards this goal, we first use field programmable gate array (FPGA) hardware simulation to find the trapping sets that cause the decoding failure in the error floor region. For each trapping set, we extract the parameters which are key to the decoding failure rate caused by this trapping set. Then we use a much simpler in situ hardware simulation with these parameters to obtain the conditional decoding failure rate. By considering all the trapping sets we find, we obtain the overall frame error rate in the error floor region. The estimation results for a length -4623 QC-LDPC code under the EPR4 channel are within 0.3 dB of the direct simulation results. In addition, this method allows us to estimate the frame error rate of a LDPC code down to 10-14 or lower.},
  Doi                      = {10.1109/GLOCOM.2007.56},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04410966.pdf:PDF},
  Keywords                 = {error correction codes;field programmable gate arrays;parity check codes;FPGA;LDPC codes;data storage systems;error correcting code;error floor estimation;field programmable gate array;low density parity check codes;partial response channels;Data storage systems;Decoding;Error analysis;Error correction codes;Estimation error;Field programmable gate arrays;Hardware;Memory;Parity check codes;Partial response channels},
  Timestamp                = {2013.10.10}
}

@Article{4069025,
  Title                    = {Evaluation of Low-Density Parity-Check Codes on Perpendicular Magnetic Recording Model},
  Author                   = {Hu, X. and Kumar, B. V. K. V.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2007},

  Month                    = feb.,
  Number                   = {2},
  Pages                    = {727 -732},
  Volume                   = {43},

  Abstract                 = {Low-density parity-check (LDPC) codes have shown superior error-correcting performance in a variety of data storage system studies, including traditional longitudinal magnetic recording systems. However, perpendicular magnetic recording systems (of increasing interest) exhibit impairments different from longitudinal magnetic recording systems, and thus present new challenges for error-correcting codes. In this effort, we evaluate a structured LDPC code using a perpendicular magnetic recording channel model that includes impairments such as transition noise, nonlinear transition shift, transition percolation, and baseline wander (BLW). The channel model, as well as the LDPC encoder and the decoder are implemented in field-programmable gate array (FPGA) hardware. The LDPC coded system is evaluated down to bit error rate (BER) of 10-12 and frame error rate (FER) of 10-8. The impact of individual impairments on coding performance is studied separately. The soft output Viterbi algorithm (SOVA) + LDPC system maintains its superior error-correcting performance under the perpendicular recording channel},
  Doi                      = {10.1109/TMAG.2006.888370},
  File                     = {:PDF\\Evaluation_of_Low-Density_Parity-Check_Codes_on_Perpendicular_Magnetic_Recording_Model.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = {BER;FPGA;LDPC codes;baseline wander;bit error rate;channel model;data storage system;error-correcting codes;field-programmable gate array;frame error rate;low-density parity-check codes;nonlinear transition shift;perpendicular magnetic recording model;soft output Viterbi algorithm;transition noise;transition percolation;Viterbi decoding;error correction codes;error statistics;field programmable gate arrays;parity check codes;perpendicular magnetic recording;}
}

@Article{5467506,
  Title                    = {Error Floor Estimation of Long LDPC Codes on Magnetic Recording Channels},
  Author                   = {Xinde Hu and Zongwang Li and Kumar, B.V.K.V. and Barndt, R.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1836-1839},
  Volume                   = {46},

  Abstract                 = {The presence of error floor in low density parity check (LDPC) codes is of great concern for potential applications of LDPC codes to data storage channels, which require the code to maintain the near capacity error correcting performance down to frame error rates of 10-12. In order to investigate the error floor of LDPC codes under magnetic recording channels used in data storage systems, we extended the trapping set based estimation method to predict the error floor under magnetic recording channels. The goal is to accurately estimate the error rates in the error floor region for certain types of LDPC codes under the partial response channel. First, we use field-programmable gate array (FPGA) hardware simulation to find the trapping sets that cause the decoding failure in the error floor region. For each trapping set, we extract the parameters that are key to the decoding failure caused by this trapping set. Then we use a much simpler in situ simulation with these parameters to obtain the conditional decoding failure rate. By considering all the trapping sets, we obtain the overall frame error rate in the error floor region. The estimation results under the magnetic recording channel are within 0.3 dB of the direct simulation results.},
  Doi                      = {10.1109/TMAG.2010.2040026},
  File                     = {:PDF\\05467506.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = {decoding;error analysis;field programmable gate arrays;importance sampling;magnetic recording;parity check codes;partial response channels;LDPC codes;data storage systems;decoding failure;error floor estimation;field-programmable gate array hardware simulation;importance sampling;low density parity check;magnetic recording channels;trapping set;Data storage systems;Decoding;Error analysis;Error correction codes;Estimation error;Field programmable gate arrays;Magnetic recording;Memory;Parity check codes;Partial response channels;Error floor;importance sampling;low density parity check (LDPC);magnetic recording;trapping set},
  Timestamp                = {2015.01.09}
}

@InProceedings{1312605,
  Title                    = {On the computation of the minimum distance of low-density parity-check codes},
  Author                   = {Xiao-Yu Hu and Fossorier, M.P.C. and Eleftheriou, E.},
  Booktitle                = {Communications, 2004 IEEE International Conference on},
  Year                     = {2004},
  Month                    = june,
  Pages                    = { 767 - 771 Vol.2},
  Volume                   = {2},

  Abstract                 = { Low-density parity-check (LDPC) codes in their broader-sense definition are linear codes whose parity-check matrices have fewer 1s than 0s. Finding their minimum distance is therefore in general an NP-hard problem. We propose a randomized algorithm called nearest nonzero codeword search (NNCS) approach to tackle this problem for iteratively decodable LDPC codes. The principle of the NNCS approach is to search codewords locally around the all-zero codeword perturbed by minimal noise, anticipating that the resultant nearest nonzero codewords will most likely contain the minimum-Hamming- weight codeword whose Hamming weight is equal to the minimum distance of the linear code. This approach has its roots in Berrou et al.'s error-impulse method and a form of Fossorier's list decoding for LDPC codes.},
  Doi                      = {10.1109/ICC.2004.1312605},
  File                     = {:PDF\\On_the_Computation_of_the_Minimum_Distance_of_Low-Density_Parity-Check_Codes.pdf:PDF},
  ISSN                     = { },
  Keywords                 = { LDPC codes; NP-hard problem; all-zero codeword; error-impulse method; linear codes; list decoding; low-density parity-check codes; minimum-Hamming- weight codeword; nearest nonzero codeword search; parity-check matrices; decoding; linear codes; optimisation; parity check codes;}
}

@Article{5605922,
  Title                    = {Large-Girth Nonbinary QC-LDPC Codes of Various Lengths},
  Author                   = {Jie Huang and Lei Liu and Wuyang Zhou and Shengli Zhou},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {december },
  Number                   = {12},
  Pages                    = {3436 -3447},
  Volume                   = {58},

  Abstract                 = {In this paper, we construct nonbinary quasi-cyclic low-density parity-check (QC-LDPC) codes whose parity check matrices consist of an array of square sub-matrices which are either zero matrices or circulant permutation matrices. We propose a novel method to design the shift offset values of the circulant permutation sub-matrices, so that the code length can vary while maintaining a large girth. Extensive Monte Carlo simulations demonstrate that the obtained codes of a wide range of rates (from 1/2 to 8/9) with length from 1000 to 10000 bits have very good performance over both AWGN and Rayleigh fading channels. Furthermore, the proposed method is extended to design multiple nonbinary QC-LDPC codes simultaneously where each individual code can achieve large girth with variable lengths. The proposed codes are appealing to practical adaptive systems where the block length and code rate need to be adaptively adjusted depending on traffic characteristics and channel conditions.},
  Doi                      = {10.1109/TCOMM.2010.101210.090757},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05605922.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {AWGN channels;Monte Carlo simulations;Rayleigh fading channels;adaptive systems;block length;channel conditions;circulant permutation matrices;code rate;large-girth nonbinary QC-LDPC codes;parity check matrices;quasicyclic low-density parity-check codes;square submatrices;traffic characteristics;zero matrices;AWGN channels;Monte Carlo methods;Rayleigh channels;cyclic codes;matrix algebra;parity check codes;},
  Timestamp                = {2011.12.07}
}

@Article{1056864,
  Title                    = {On certain projective geometry codes (Corresp.)},
  Author                   = { Huang, J. and Shiva, S. and Seguin, G.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1984},

  Month                    = {mar},
  Number                   = {2},
  Pages                    = { 385 - 388},
  Volume                   = {30},

  Abstract                 = { LetVbe an(n, k, d)binary projective geometry code withn = (q^{m}-1)/(q - 1), q = 2^{s}, andd geq [(q^{m-r}-1)/(q - 1)] + 1. This code isr-step majority-logic decodable. With reference to the GF(q^{m}) = {0, 1, alpha , alpha^{2} , cdots , alpha^{n(q-1)-1} }, the generator polynomialg(X), ofV, hasalpha^{nu}as a root if and only ifnuhas the formnu = i(q - 1)andmax_{0 leq l lt; s} W_{q}(2^{l} nu) leq (m - r - 1)(q - 1), whereW_{q}(x)indicates the weight of the radix-qrepresentation of the numberx. LetSbe the set of nonzero numbersnu, such thatalpha^{nu}is a root ofg(X). LetC_{1}, C_{2}, cdots, C_{nu}be the cyclotomic cosets such thatSis the union of these cosets. It is clear that the process of findingg(X)becomes simpler if we can find a representative from eachC_{i}, since we can then refer to a table, of irreducible factors, as given by, say, Peterson and Weldon. In this correspondence it was determined that the coset representatives for the cases ofm-r = 2, withs = 2, 3, andm-r=3, withs=2.},
  Doi                      = {10.1109/TIT.1984.1056864},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01056864.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Geometry coding; Majority logic decoding;},
  Timestamp                = {2012.02.07}
}

@Article{Huang19981,
  Title                    = {Counting Points on Curves over Finite Fields},
  Author                   = {Ming-Deh Huang and Doug Ierardi},
  Journal                  = {Journal of Symbolic Computation},
  Year                     = {1998},
  Number                   = {1},
  Pages                    = {1 - 21},
  Volume                   = {25},

  Abstract                 = {We consider the problem of counting the number of points on a plane curve, defined by a homogeneous polynomialF(x,y,z) &#xa0;∈&#xa0;Fq[x,y,z], which are rational over a ground field Fq. More precisely, we show that if we are given a projective plane curve C of degreen, and if C has only ordinary multiple points, then one can compute the number of Fq-rational points on C in randomized time (logq)Δwhere Δ&#xa0;=&#xa0;nO(1). Since our algorithm actually computes the characteristic polynomial of the Frobenius endomorphism on the Jacobian of C, it ,follows that we may also compute (1) the number of Fq-rational points on the smooth projective model of C, (2) the number of Fq-rational points on the Jacobian of C, and (3) the number of Fqm-rational points on C in any given finite extension Fqmof the ground field, each in a similar time bound.},
  Doi                      = {10.1006/jsco.1997.0164},
  File                     = {:PDF\\Huang19981.pdf:PDF},
  ISSN                     = {0747-7171},
  Timestamp                = {2012.05.17},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0747717197901644}
}

@Article{HuangDiaoLinAbdel-Ghaffar2012,
  Title                    = {Cyclic and Quasi-Cyclic LDPC Codes on Constrained Parity-Check Matrices and Their Trapping Sets},
  Author                   = {Qin Huang and Qiuju Diao and Shu Lin and Abdel-Ghaffar, K.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {2648-2671},
  Volume                   = {58},

  Abstract                 = {This paper is concerned with construction and structural analysis of both cyclic and quasi-cyclic codes, particularly low-density parity-check (LDPC) codes. It consists of three parts. The first part shows that a cyclic code given by a parity-check matrix in circulant form can be decomposed into descendant cyclic and quasi-cyclic codes of various lengths and rates. Some fundamental structural properties of these descendant codes are developed, including the characterization of the roots of the generator polynomial of a cyclic descendant code. The second part of the paper shows that cyclic and quasi-cyclic descendant LDPC codes can be derived from cyclic finite-geometry LDPC codes using the results developed in the first part of the paper. This enlarges the repertoire of cyclic LDPC codes. The third part of the paper analyzes the trapping set structure of regular LDPC codes whose parity-check matrices satisfy a certain constraint on their rows and columns. Several classes of finite-geometry and finite-field cyclic and quasi-cyclic LDPC codes with large minimum distances are shown to have no harmful trapping sets of size smaller than their minimum distances. Consequently, their error-floor performances are dominated by their minimum distances.},
  Doi                      = {10.1109/TIT.2011.2179842},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06135499.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {geometric codes;parity check codes;polynomials;constrained parity-check matrices;cyclic descendant code;cyclic finite-geometry LDPC codes;error-floor performances;finite-field cyclic codes;generator polynomial;low-density parity-check codes;quasi-cyclic LDPC codes;trapping sets;Arrays;Decoding;Generators;Matrix decomposition;Null space;Parity check codes;Polynomials;Circulant decomposition;cyclic code;finite-geometry (FG) code;low-density parity-check (LDPC) code;orthogonal parity-check sums;quasi-cyclic (QC) code;row–column (RC)-constrained LDPC code;trapping set},
  Timestamp                = {2014.03.04}
}

@InProceedings{6033698,
  Title                    = {Trapping sets of structured LDPC codes},
  Author                   = {Qin Huang and Qiuju Diao and Shu Lin and Abdel-Ghaffar, K.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on},
  Year                     = {2011},
  Month                    = {July},
  Pages                    = {1086-1090},

  Abstract                 = {THIS PAPER IS ELIGIBLE FOR THE STUDENT PAPER AWARD. This paper analyzes trapping set structure of binary regular LDPC codes whose parity-check matrices satisfy the constraint that no two rows (or two columns) have more than one place where they both have non-zero components, which is called row-column (RC) constraint. For a (γ,ρ)-regular LDPC code whose parity-check matrix satisfies the RC-constraint, its Tanner graph contains no (κ, τ) trapping set with size κ ≤ γ and number τ of odd degree check nodes less than γ. For several classes of RC-constrained regular LDPC codes constructed algebraically, we show that their Tanner graphs contain no trapping sets of sizes smaller than their minimum weights.},
  Doi                      = {10.1109/ISIT.2011.6033698},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06033698.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {binary codes;graph theory;matrix algebra;parity check codes;RC constraint;Tanner graph;parity-check matrices;row-column constraint;structured LDPC codes;trapping sets;Charge carrier processes;Decoding;Geometry;Iterative decoding;Null space;Redundancy},
  Timestamp                = {2014.03.04}
}

@Article{1742-6596-95-1-012005,
  Title                    = {A Monte Carlo algorithm for sampling rare events: application to a search for the Griffiths singularity},
  Author                   = {K Hukushima and Y Iba},
  Journal                  = {Journal of Physics: Conference Series},
  Year                     = {2008},
  Number                   = {1},
  Pages                    = {012005},
  Volume                   = {95},

  Abstract                 = {We develop a recently proposed importance-sampling Monte Carlo algorithm for sampling rare events and quenched variables in random disordered systems. We apply it to a two dimensional bond-diluted Ising model and study the Griffiths singularity which is considered to be due to the existence of rare large clusters. It is found that the distribution of the inverse susceptibility has an exponential tail down to the origin which is considered the consequence of the Griffiths singularity.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\1742-6596_95_1_012005.pdf:PDF},
  Timestamp                = {2012.03.06},
  Url                      = {http://stacks.iop.org/1742-6596/95/i=1/a=012005}
}

@Article{JPSJ.65.1604,
  Title                    = {Exchange Monte Carlo Method and Application to Spin Glass Simulations},
  Author                   = {Koji Hukushima and Koji Nemoto},
  Journal                  = {Journal of the Physical Society of Japan},
  Year                     = {1996},
  Number                   = {6},
  Pages                    = {1604-1608},
  Volume                   = {65},

  Doi                      = {10.1143/JPSJ.65.1604},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\JPSJ.65.1604.pdf:PDF},
  Numpages                 = {4},
  Publisher                = {The Physical Society of Japan},
  Timestamp                = {2014.02.25},
  Url                      = {http://jpsj.ipap.jp/link?JPSJ/65/1604/}
}

@Article{iba_EXTENDED_ENSEMBLE_MONTE_CARLO,
  Title                    = {EXTENDED ENSEMBLE MONTE CARLO},
  Author                   = {YUKITO IBA},
  Journal                  = {International Journal of Modern Physics C},
  Year                     = {2001},

  Month                    = {June},
  Number                   = {5},
  Pages                    = {623-656},
  Volume                   = {12},

  Abstract                 = {"Extended Ensemble Monte Carlo" is a generic term that indicates a set of algorithms, which are now popular in a variety of fields in physics and statistical information processing. Exchange Monte Carlo (Metropolis-Coupled Chain, Parallel Tempering), Simulated Tempering (Expanded Ensemble Monte Carlo) and Multicanonical Monte Carlo (Adaptive Umbrella Sampling) are typical members of this family. Here, we give a cross-disciplinary survey of these algorithms with special emphasis on the great flexibility of the underlying idea. In Sec. 2, we discuss the background of Extended Ensemble Monte Carlo. In Secs. 3, 4 and 5, three types of the algorithms, i.e., Exchange Monte Carlo, Simulated Tempering, Multicanonical Monte Carlo, are introduced. In Sec. 6, we give an introduction to Replica Monte Carlo algorithm by Swendsen and Wang. Strategies for the construction of special-purpose extended ensembles are discussed in Sec. 7. We stress that an extension is not necessary restricted to the space of energy or temperature. Even unphysical (unrealizable) configurations can be included in the ensemble, if the resultant fast mixing of the Markov chain offsets the increasing cost of the sampling procedure. Multivariate (multicomponent) extensions are also useful in many examples. In Sec. 8, we give a survey on extended ensembles with a state space whose dimensionality is dynamically varying. In the appendix, we discuss advantages and disadvantages of three types of extended ensemble algorithms.},
  Doi                      = {10.1142/S0129183101001912},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\iba10_1142_S0129183101001912.pdf:PDF},
  Keywords                 = {Extended Ensemble; Exchange Monte Carlo; Simulated Tempering; Multicanonical Monte Carlo; Replica Monte Carlo; Complexity Ladder; Bridge; Multivariate Extension},
  Timestamp                = {2012.03.14},
  Url                      = {http://www.worldscinet.com/ijmpc/12/1205/S0129183101001912.html}
}

@Article{JPSJ.77.103801,
  Title                    = {Testing Error Correcting Codes by Multicanonical Sampling of Rare Events},
  Author                   = {Yukito Iba and Koji Hukushima},
  Journal                  = {Journal of the Physical Society of Japan},
  Year                     = {2008},
  Number                   = {10},
  Pages                    = {103801},
  Volume                   = {77},

  Doi                      = {10.1143/JPSJ.77.103801},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\JPSJ.77.103801.pdf:PDF;:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\0709.2578v2.pdf:PDF},
  Numpages                 = {3},
  Publisher                = {The Physical Society of Japan},
  Timestamp                = {2012.03.06},
  Url                      = {http://jpsj.ipap.jp/link?JPSJ/77/103801/}
}

@InProceedings{5984637,
  Title                    = {Strain-engineering for high-performance STT-MRAM},
  Author                   = {Iba, Y. and Tsunoda, K. and Lee, Y. M. and Yoshida, C. and Noshiro, H. and Takahashi, A. and Yamazaki, Y. and Nakabayashi, M. and Hatada, A. and Aoki, M. and Sugii, T.},
  Booktitle                = {VLSI Technology (VLSIT), 2011 Symposium on},
  Year                     = {2011},
  Month                    = {june},
  Pages                    = {212 -213},

  Abstract                 = {Strain-engineering using the inverse magnetostrictive effect has been performed to improve the performance of spin transfer torque magnetoresistance random access memory (STT-MRAM). The thermal stability factor E/kBT has been enhanced by 40% without increasing a switching current by controlling the process-induced stress in a free layer in MTJ (magnetic tunnel junctions) with mechanically engineered manufacturing steps.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984637.pdf:PDF},
  ISSN                     = {0743-1562},
  Timestamp                = {2011.08.28}
}

@Article{Ielmini20021749,
  Title                    = {Modeling of anomalous SILC in flash memories based on tunneling at multiple defects},
  Author                   = {Daniele Ielmini and Alessandro S. Spinelli and Andrea L. Lacaita and Alberto Modelli},
  Journal                  = {Solid-State Electronics},
  Year                     = {2002},
  Number                   = {11},
  Pages                    = {1749 - 1756},
  Volume                   = {46},

  Doi                      = {DOI: 10.1016/S0038-1101(02)00144-2},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Ielmini20021749.pdf:PDF},
  ISSN                     = {0038-1101},
  Keywords                 = {Flash memory},
  Timestamp                = {2011.08.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0038110102001442}
}

@Article{jfs280320,
  Title                    = {Some remarks on the number of rational points of algebratic curves over finite fields},
  Author                   = {Yasutaka Ihara},
  Journal                  = {Journal of the Faculty of Science, the University of Tokyo. Sect. 1 A, Mathematics},
  Year                     = {1982},

  Month                    = {February},
  Number                   = {3},
  Pages                    = {721-724},
  Volume                   = {28},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\jfs280320.pdf:PDF},
  ISSN                     = {00408980},
  Publisher                = {Faculty of Science, The University of Tokyo},
  Timestamp                = {2011.07.06},
  Url                      = {http://hdl.handle.net/2261/6319}
}

@InProceedings{5755869,
  Title                    = {Upper Bounds of Block Error Probability of Standard Irregular LDPC Code Ensemble under Maximum Likelihood Decoding},
  Author                   = {Ikegaya, Ryoji and Kasai, Kenta and Shibuya, Tomoharu and Sakaniwa, Kohichi},
  Booktitle                = {Turbo Codes Related Topics; 6th International ITG-Conference on Source and Channel Coding (TURBOCODING), 2006 4th International Symposium on},
  Year                     = {2006},
  Month                    = {April},
  Pages                    = {1-6},

  Abstract                 = {In this paper, we derive the upper bound of the average block error probability of a given standard irregular LDPC code ensemble under the maximum likelihood decoding. Our results are the forms including the case of regular LDPC code ensembles.},
  File                     = {:PDF\\05755869.pdf:PDF},
  Keywords                 = {Decoding;Encoding},
  Timestamp                = {2014.12.24}
}

@Article{5601690,
  Title                    = {Flash-Aware RAID Techniques for Dependable and High-Performance Flash Memory SSD},
  Author                   = {Soojun Im and Dongkun Shin},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {jan. },
  Number                   = {1},
  Pages                    = {80 -92},
  Volume                   = {60},

  Abstract                 = {Solid-state disks (SSDs), which are composed of multiple NAND flash chips, are replacing hard disk drives (HDDs) in the mass storage market. The performances of SSDs are increasing due to the exploitation of parallel I/O architectures. However, reliability remains as a critical issue when designing a large-scale flash storage. For both high performance and reliability, Redundant Arrays of Inexpensive Disks (RAID) storage architecture is essential to flash memory SSD. However, the parity handling overhead for reliable storage is significant. We propose a novel RAID technique for flash memory SSD for reducing the parity updating cost. To reduce the number of write operations for the parity updates, the proposed scheme delays the parity update which must accompany each data write in the original RAID technique. In addition, by exploiting the characteristics of flash memory, the proposed scheme uses the partial parity technique to reduce the number of read operations required to calculate a parity. We evaluated the performance improvements using a RAID-5 SSD simulator. The proposed techniques improved the performance of the RAID-5 SSD by 47 percent and 38 percent on average in comparison to the original RAID-5 technique and the previous delayed parity updating technique, respectively.},
  Doi                      = {10.1109/TC.2010.197},
  File                     = {:PDF\\05601690.pdf:PDF},
  ISSN                     = {0018-9340},
  Keywords                 = {NAND flash chips;RAID storage architecture;RAID-5 SSD simulator;flash memory SSD;flash-aware RAID techniques;hard disk drives;mass storage market;parallel I/O architectures;redundant arrays of inexpensive disks;solid-state disks;RAID;disc drives;flash memories;hard discs;},
  Timestamp                = {2011.05.20}
}

@InProceedings{1523373,
  Title                    = {An LDPCC decoding algorithm based on bowman-levin approximation -comparison with bp and CCCP-},
  Author                   = {Inoue, M. and Komiya, M. and Kabashima, Y.},
  Booktitle                = {Information Theory, 2005. ISIT 2005. Proceedings. International Symposium on},
  Year                     = {2005},
  Month                    = {Sept},
  Pages                    = {444-448},

  Abstract                 = {Belief propagation (BP) and the concave convex procedure (CCCP) are both methods that utilize the Bethe free energy as a cost function and solve information processing tasks. We have developed a new algorithm that also uses the Bethe free energy, but changes the roles of the master variables and the slave variables. This is called the Bowman-Levin (BL) approximation in the domain of statistical physics. When we applied the BL algorithm to decode the Gallager ensemble of short-length regular low-density parity check codes (LDPCC) over an additive white Gaussian noise (AWGN) channel, its average performance was somewhat better than that of either BP or CCCP. This implies that the BL algorithm can also be successfully applied to other problems to which BP or CCCP has already been applied},
  Doi                      = {10.1109/ISIT.2005.1523373},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01523373.pdf:PDF},
  Keywords                 = {AWGN channels;approximation theory;decoding;parity check codes;statistical analysis;AWGN channel;Bethe free energy;Bowman-Levin approximation;LDPCC decoding algorithm;additive white Gaussian noise channel;belief propagation;concave convex procedure;low-density parity check codes;AWGN;Additive white noise;Approximation algorithms;Belief propagation;Cost function;Decoding;Information processing;Master-slave;Parity check codes;Physics},
  Timestamp                = {2014.06.26}
}

@InProceedings{5556126,
  Title                    = {A multi-level-cell spin-transfer torque memory with series-stacked magnetotunnel junctions},
  Author                   = {Ishigaki, T. and Kawahara, T. and Takemura, R. and Ono, K. and Ito, K. and Matsuoka, H. and Ohno, H.},
  Booktitle                = {VLSI Technology (VLSIT), 2010 Symposium on},
  Year                     = {2010},
  Month                    = june,
  Pages                    = {47 -48},

  Abstract                 = {We first report a multi-level-cell (MLC) spin-transfer torque memory (SPRAM) with series-connected magnetotunnel junctions (MTJs). The series MTJs (with different areas) show multi-level resistances by a combination of their magnetization directions. A four-level operation by spin-transfer-torque writing was experimentally demonstrated. A scheme for the write/read operation of the MLC SPRAM was also presented.},
  Doi                      = {10.1109/VLSIT.2010.5556126},
  File                     = {:PDF\\A_Multi-Level-Cell_Spin-Transfer_Torque_Memory_with_Series-Stacked_Magnetotunnel_Junction.pdf:PDF},
  Keywords                 = {SPRAM;magnetization directions;multilevel resistances;multilevel-cell spin-transfer torque memory;series-stacked magnetotunnel junctions;write/read operation;MRAM devices;magnetic tunnelling;magnetisation;}
}

@Article{5571897,
  Title                    = {Rewriting Codes for Joint Information Storage in Flash Memories},
  Author                   = {Anxiao Jiang and Bohossian, V. and Bruck, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {5300-5313},
  Volume                   = {56},

  Abstract                 = {Memories whose storage cells transit irreversibly between states have been common since the start of the data storage technology. In recent years, flash memories have become a very important family of such memories. A flash memory cell has q states-state 0, 1, ..., q-1-and can only transit from a lower state to a higher state before the expensive erasure operation takes place. We study rewriting codes that enable the data stored in a group of cells to be rewritten by only shifting the cells to higher states. Since the considered state transitions are irreversible, the number of rewrites is bounded. Our objective is to maximize the number of times the data can be rewritten. We focus on the joint storage of data in flash memories, and study two rewriting codes for two different scenarios. The first code, called floating code, is for the joint storage of multiple variables, where every rewrite changes one variable. The second code, called buffer code, is for remembering the most recent data in a data stream. Many of the codes presented here are either optimal or asymptotically optimal. We also present bounds to the performance of general codes. The results show that rewriting codes can integrate a flash memory's rewriting capabilities for different variables to a high degree.},
  Doi                      = {10.1109/TIT.2010.2059530},
  File                     = {:PDF\\05571897.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {cache storage;codes;digital storage;flash memories;buffer code;data storage technology;flash memories;floating code;joint information storage;rewriting codes;Ash;Computer aided software engineering;Construction industry;Electron traps;Joints;Memory;Periodic structures;Coding theory;data storage;flash memory},
  Timestamp                = {2015.06.11}
}

@InProceedings{6620390,
  Title                    = {Joint rewriting and error correction in write-once memories},
  Author                   = {Anxiao Jiang and Yue Li and Gad, E.E. and Langberg, M. and Bruck, J.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2013 IEEE International Symposium on},
  Year                     = {2013},
  Month                    = {July},
  Pages                    = {1067-1071},

  Abstract                 = {Both rewriting and error correction are important technologies for non-volatile memories, especially flash memories. However, coding schemes that combine them have been limited. This paper presents a new coding scheme that combines rewriting and error correction for the write-once memory model. Its construction is based on polar codes, and it supports any number of rewrites and corrects a substantial number of errors. The code is analyzed for the binary symmetric channel, and experimental results verify its performance. The results can be extended to multi-level cells and more general noise models.},
  Doi                      = {10.1109/ISIT.2013.6620390},
  File                     = {:PDF\\06620390.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {binary codes;channel coding;error correction codes;flash memories;binary symmetric channel;flash memory;general noise models;joint rewriting and error correction coding scheme;multilevel cells;nonvolatile memory;polar codes;write-once memory model;Decoding;Encoding;Error correction codes;Error probability;Noise;Noise measurement},
  Timestamp                = {2014.09.03}
}

@InProceedings{4595284,
  Title                    = {Rank modulation for flash memories},
  Author                   = {Anxiao Jiang and Mateescu, R. and Schwartz, M. and Bruck, J.},
  Booktitle                = {Information Theory, 2008. ISIT 2008. IEEE International Symposium on},
  Year                     = {2008},
  Month                    = {July},
  Pages                    = {1731-1735},

  Abstract                 = {We explore a novel data representation scheme for multi-level flash memory cells, in which a set of n cells stores information in the permutation induced by the different charge levels of the individual cells. The only allowed charge-placement mechanism is a dasiapush-to-the-toppsila operation which takes a single cell of the set and makes it the top-charged cell. The resulting scheme eliminates the need for discrete cell levels, as well as overshoot errors, when programming cells. We present unrestricted Gray codes spanning all possible n-cell states and using only dasiapush-to-the-toppsila operations, and also construct balanced Gray codes. We also investigate optimal rewriting schemes for translating arbitrary input alphabet into n-cell states which minimize the number of programming operations.},
  Doi                      = {10.1109/ISIT.2008.4595284},
  File                     = {:PDF\\04595284.pdf:PDF},
  Keywords                 = {Gray codes;flash memories;Gray codes;charge-placement mechanism;data representation scheme;discrete cell levels;flash memories;multilevel flash memory cells;n-cell states;optimal rewriting schemes;push-to-the-top operation;rank modulation;Aging;Computer science;Costs;Data engineering;Flash memory;Flash memory cells;Modulation coding;Nonvolatile memory;Postal services;Reflective binary codes},
  Timestamp                = {2015.03.26}
}

@TechReport{etr086,
  Title                    = {Rank modulation for flash memories},
  Author                   = {A. Jiang and R. Mateescu and M. Schwartz and J. Bruck},
  Institution              = {California Institute of Technology},
  Year                     = {2008},

  File                     = {:PDF\\etr086.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.03.27},
  Url                      = {http://www.paradise.caltech.edu/etr.html}
}

@Article{5452201,
  Title                    = {Correcting Charge-Constrained Errors in the Rank-Modulation Scheme},
  Author                   = {Anxiao Jiang and Schwartz, M. and Bruck, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {2112-2120},
  Volume                   = {56},

  Abstract                 = {We investigate error-correcting codes for a the rank-modulation scheme with an application to flash memory devices. In this scheme, a set of n cells stores information in the permutation induced by the different charge levels of the individual cells. The resulting scheme eliminates the need for discrete cell levels, overcomes overshoot errors when programming cells (a serious problem that reduces the writing speed), and mitigates the problem of asymmetric errors. In this paper, we study the properties of error-correcting codes for charge-constrained errors in the rank-modulation scheme. In this error model the number of errors corresponds to the minimal number of adjacent transpositions required to change a given stored permutation to another erroneous one-a distance measure known as Kendall's ?? -distance. We show bounds on the size of such codes, and use metric-embedding techniques to give constructions which translate a wealth of knowledge of codes in the Lee metric to codes over permutations in Kendall's ??-metric. Specifically, the one-error-correcting codes we construct are at least half the ball-packing upper bound.},
  Doi                      = {10.1109/TIT.2010.2043764},
  File                     = {:PDF\\05452201.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {error correction codes;flash memories;modulation;Kendall ??-distance;adjacent transpositions;charge-constrained errors;error-correcting codes;flash memory devices;rank-modulation scheme;Electron traps;Engineering profession;Error correction;Error correction codes;Flash memory;Modulation coding;Nonvolatile memory;Programming profession;Robustness;Upper bound;Error-correcting codes;Kendall's $tau$-metric;flash memory;metric embeddings;permutations;rank modulation},
  Timestamp                = {2015.03.31}
}

@Article{4840630,
  Title                    = {Large Girth Non-Binary LDPC Codes Based on Finite Fields and Euclidean Geometries},
  Author                   = {Xueqin Jiang and Moon Ho Lee},
  Journal                  = {Signal Processing Letters, IEEE},
  Year                     = {2009},

  Month                    = {june },
  Number                   = {6},
  Pages                    = {521 -524},
  Volume                   = {16},

  Abstract                 = {This letter presents an approach to the construction of non-binary low-density parity-check (LDPC) codes based on alpha-multiplied circulant permutation matrices and hyperplanes of two different dimensions in Euclidean geometries. Codes constructed by this method have large girth and high binary column weight when the order of Galois field is high. Simulation results show that these codes perform very well with fast Fourier transform (FFT) based sum-product algorithm (SPA).},
  Doi                      = {10.1109/LSP.2009.2016830},
  File                     = {:\\\\homepd\\pd6\\ユニット管理\\refereces\\PDF\\04840630.pdf:PDF},
  ISSN                     = {1070-9908},
  Keywords                 = {Galois field;alpha-multiplied circulant permutation matrices;euclidean geometries;fast Fourier transform;large girth nonbinary LDPC codes;nonbinary low-density parity-check codes;sum-product algorithm;Galois fields;binary codes;fast Fourier transforms;matrix algebra;parity check codes;},
  Timestamp                = {2011.12.06}
}

@Article{4939346,
  Title                    = {Large girth quasi-cyclic LDPC codes based on the chinese remainder theorem},
  Author                   = {Xueqin Jiang and Moon Ho Lee},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2009},

  Month                    = {may },
  Number                   = {5},
  Pages                    = {342 -344},
  Volume                   = {13},

  Abstract                 = {In this letter, we consider two problems associated with quasi-cyclic low-density parity-check (QC-LDPC) codes. The first is how to extend the code length of a QC-LDPC code without reducing the girth. The second is how to design a QCLDPC code with a prescribed girth easily. We deal with these two problems by using a combining method of QC-LDPC codes via the Chinese Remainder Theorem (CRT). Codes constructed with our proposed method have flexible code lengths, flexible code rates and large girth. Simulation results show that they perform very well with the iterative decoding.},
  Doi                      = {10.1109/LCOMM.2009.082115},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04939346.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = {LDPC codes;chinese remainder theorem;iterative decoding;large girth codes;low-density parity check codes;quasi-cyclic codes;cyclic codes;iterative decoding;parity check codes;},
  Timestamp                = {2011.12.06}
}

@Book{Sarah_Johnson_IterativeErrorCorrection,
  Title                    = {Iterative Error Correction: Turbo, Low-Density Parity-Check and Repeat-Accumulate Codes},
  Author                   = {Sarah Johnson},
  Publisher                = {Cambridge University Press},
  Year                     = {2009},
  Month                    = {November},

  Abstract                 = {Iterative error correction codes have found widespread application in cellular communications, digital video broadcasting and wireless LANs. This self-contained treatment of iterative error correction presents all the key ideas needed to understand, design, implement and analyse these powerful codes. Turbo, low-density parity-check, and repeat-accumulate codes are given equal, detailed coverage, with precise presentations of encoding and decoding procedures. Worked examples are integrated into the text to illuminate each new idea and pseudo-code is included for important algorithms to facilitate the reader's development of the techniques described. For each subject, the treatment begins with the simplest case before generalizing. There is also coverage of advanced topics such as density-evolution and EXIT charts for those readers interested in gaining a deeper understanding of the field. This text is ideal for graduate students in electrical engineering and computer science departments, as well as practitioners in the communications industry.},
  Timestamp                = {2013.02.05}
}

@Book{jordan1998learning,
  Title                    = {Learning in Graphical Models:[proceedings of the NATO Advanced Study Institute...: Ettore Mairona Center, Erice, Italy, September 27-October 7, 1996]},
  Author                   = {Jordan, Michael Irwin},
  Publisher                = {Springer Science \& Business Media},
  Year                     = {1998},
  Volume                   = {89},

  File                     = {:PDF\\978-94-011-5014-9.pdf:PDF},
  Timestamp                = {2015.07.30}
}

@Book{jordan2001graphical,
  Title                    = {Graphical models: Foundations of neural computation},
  Author                   = {Jordan, Michael Irwin and T. J. Sejnowski},
  Publisher                = {MIT press},
  Year                     = {2001},

  Timestamp                = {2015.07.31}
}

@Article{1650378,
  Title                    = {Automorphism groups of some AG codes},
  Author                   = {Joyner, D. and Ksir, A.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {3325-3329},
  Volume                   = {52},

  Abstract                 = {In this correspondence, we show that in many cases, the automorphism group of a curve and the permutation automorphism group of a corresponding AG code are the same. This generalizes a result of Wesemeyer beyond the case of planar curves.},
  Doi                      = {10.1109/TIT.2006.876243},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01650378.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {algebraic geometric codes;AG code;algebraic-geometric code;permutation automorphism group;Algebra;Application software;Codes;Computer errors;Decoding;Encoding;Galois fields;Kernel;Mathematics;Packaging;Algebraic-geometric codes;Riemann–Roch spaces;algebraic curves, code automorphisms},
  Timestamp                = {2014.06.12}
}

@Article{1394085,
  Title                    = {SAGE and coding theory(abstract only)},
  Author                   = {Joyner, David and Miller, Robert},
  Journal                  = {ACM Communication in Compter Algebra},
  Year                     = {2008},
  Number                   = {1-2},
  Pages                    = {74--78},
  Volume                   = {42},

  Address                  = {New York, NY, USA},
  Doi                      = {http://doi.acm.org/10.1145/1394042.1394085},
  ISSN                     = {1932-2240},
  Publisher                = {ACM}
}

@Article{kabashima2004statistical,
  Title                    = {Statistical mechanics of low-density parity-check codes},
  Author                   = {Kabashima, Yoshiyuki and Saad, David},
  Journal                  = {Journal of physics A: mathematical and general},
  Year                     = {2004},
  Number                   = {6},
  Pages                    = {R1},
  Volume                   = {37},

  File                     = {:PDF\\NCRG_2004_002.pdf:PDF},
  Publisher                = {IOP Publishing},
  Timestamp                = {2015.02.03}
}

@Article{kabashima1999statistical,
  Title                    = {Statistical mechanics of error-correcting codes},
  Author                   = {Kabashima, Yoshiyuki and Saad, David},
  Journal                  = {EPL (Europhysics Letters)},
  Year                     = {1999},
  Number                   = {1},
  Pages                    = {97},
  Volume                   = {45},

  File                     = {:PDF\\Europhys_Lett_45(1)_97-103_.pdf:PDF},
  Publisher                = {IOP Publishing},
  Timestamp                = {2015.02.03}
}

@Article{kabashima1998belief,
  Title                    = {Belief propagation vs. TAP for decoding corrupted messages},
  Author                   = {Kabashima, Yoshiyuki and Saad, David},
  Journal                  = {EPL (Europhysics Letters)},
  Year                     = {1998},
  Number                   = {5},
  Pages                    = {668},
  Volume                   = {44},

  File                     = {:PDF\\Europhys_Lett_44(5)_668-674_.pdf:PDF},
  Publisher                = {IOP Publishing},
  Timestamp                = {2015.02.03}
}

@Article{PhysRevE.64.046113,
  Title                    = {Tighter decoding reliability bound for Gallager's error-correcting code},
  Author                   = {Kabashima, Yoshiyuki and Sazuka, Naoya and Nakamura, Kazutaka and Saad, David},
  Journal                  = {Phys. Rev. E},
  Year                     = {2001},

  Month                    = {Sep},
  Pages                    = {046113},
  Volume                   = {64},

  Doi                      = {10.1103/PhysRevE.64.046113},
  File                     = {:PDF\\0010173.pdf:PDF},
  Issue                    = {4},
  Numpages                 = {4},
  Publisher                = {American Physical Society},
  Timestamp                = {2015.02.03},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevE.64.046113}
}

@Article{4137888,
  Title                    = {High-Rate Quasi-Cyclic Low-Density Parity-Check Codes Derived From Finite Affine Planes},
  Author                   = {Kamiya, N.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {april },
  Number                   = {4},
  Pages                    = {1444 -1459},
  Volume                   = {53},

  Abstract                 = {This paper shows that several attractive classes of quasi-cyclic (QC) low-density parity-check (LDPC) codes can be obtained from affine planes over finite fields. One class of these consists of duals of one-generator QC codes. Presented here for codes contained in this class are the exact minimum distance and a lower bound on the multiplicity of the minimum-weight codewords. Further, it is shown that the minimum Hamming distance of a code in this class is equal to its minimum additive white Gaussian noise (AWGN) pseudoweight. Also discussed is a class consisting of codes from circulant permutation matrices, and an explicit formula for the rank of the parity-check matrix is presented for these codes. Additionally, it is shown that each of these codes can be identified with a code constructed from a constacyclic maximum distance separable code of dimension 2. The construction is similar to the derivation of Reed-Solomon (RS)-based LDPC codes presented by Chen and Djurdjevic Experimental results show that a number of high rate QC-LDPC codes with excellent error performance are contained in these classes},
  Doi                      = {10.1109/TIT.2007.892770},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04137888.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {AWGN;additive white Gaussian noise;finite affine planes;high-rate quasicyclic code;low-density parity-check code;minimum Hamming distance;parity-check matrix;AWGN;Hamming codes;cyclic codes;parity check codes;},
  Timestamp                = {2011.10.11}
}

@Article{springerlink:10.1007_s10623-005-6685-6,
  Title                    = {Quasi-Cyclic Codes from a Finite Affine Plane},
  Author                   = {Kamiya, Norifumi and Fossorier, Marc},
  Journal                  = {Designs, Codes and Cryptography},
  Year                     = {2006},
  Note                     = {10.1007/s10623-005-6685-6},
  Pages                    = {311-329},
  Volume                   = {38},

  Abstract                 = {Finite geometry codes are defined as the null spaces of the incidence matrices of points and flats in finite geometries. In this paper, we investigate the incidence matrix of points other than the origin and lines not passing through the origin in the affine plane AG (2,2 s ), and we present two classes of quasi-cyclic codes derived from submatrices of the point-line incidence matrix. We also investigate the 2-ranks of those submatrices.},
  Affiliation              = {NEC Corporation Internet Systems Research Laboratories 1753 Shimonumabe Nakahara, Kawasaki Kanagawa 211-8666 Japan 1753 Shimonumabe Nakahara, Kawasaki Kanagawa 211-8666 Japan},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink10.1007_s10623-005-6685-6.pdf:PDF},
  ISSN                     = {0925-1022},
  Issue                    = {3},
  Keyword                  = {Mathematics and Statistics},
  Publisher                = {Springer Netherlands},
  Timestamp                = {2011.10.13},
  Url                      = {http://dx.doi.org/10.1007/s10623-005-6685-6}
}

@InProceedings{523390,
  Title                    = {A Fast Galois-Field Transform Algorithm Using Normal Bases},
  Author                   = {Rom-Shen Kao and Taylor, F.J.},
  Booktitle                = {Signals, Systems and Computers, 1990 Conference Record Twenty-Fourth Asilomar Conference on},
  Year                     = {1990},
  Month                    = {nov},
  Pages                    = {511},
  Volume                   = {1},

  Abstract                 = {Not available},
  Doi                      = {10.1109/ACSSC.1990.523390},
  File                     = {:PDF\\00523390.pdf:PDF},
  ISSN                     = {1058-6393},
  Keywords                 = {Computational complexity;Computer architecture;Discrete Fourier transforms;Equations;Fourier transforms;Galois fields;Hardware;Polynomials;Power engineering computing;Very large scale integration;},
  Timestamp                = {2013.01.21}
}

@Article{6259854,
  Title                    = {Efficient Algorithm for Finding Dominant Trapping Sets of LDPC Codes},
  Author                   = {Karimi, M. and Banihashemi, A.H.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2012},
  Number                   = {11},
  Pages                    = {6942-6958},
  Volume                   = {58},

  Abstract                 = {This paper presents an efficient algorithm for finding the dominant trapping sets of a low-density parity-check (LDPC) code. The algorithm can be used to estimate the error floor of LDPC codes or as a tool to design LDPC codes with low error floors. For regular codes, the algorithm is initiated with a set of short cycles as the input. For irregular codes, in addition to short cycles, variable nodes with low degree and cycles with low approximate cycle extrinsic message degree (ACE) are also used as the initial inputs. The initial inputs are then expanded recursively to dominant trapping sets of increasing size. At the core of the algorithm lies the analysis of the graphical structure of dominant trapping sets and the relationship of such structures to short cycles, low-degree variable nodes, and cycles with low ACE. The algorithm is universal in the sense that it can be used for an arbitrary graph and that it can be tailored to find a variety of graphical objects, such as absorbing sets and Zyablov-Pinsker trapping sets, known to dominate the performance of LDPC codes in the error floor region over different channels and for different iterative decoding algorithms. Simulation results on several LDPC codes demonstrate the accuracy and efficiency of the proposed algorithm. In particular, the algorithm is significantly faster than the existing search algorithms for dominant trapping sets.},
  Doi                      = {10.1109/TIT.2012.2205663},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06259854.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {graph theory;iterative decoding;parity check codes;LDPC code;Zyablov-Pinsker trapping sets;arbitrary graph;different iterative decoding algorithms;dominant trapping sets;irregular codes;low ACE;low error floors;low-degree variable nodes;low-density parity-check codes;regular codes;search algorithms;Accuracy;Algorithm design and analysis;Charge carrier processes;Complexity theory;Decoding;Iterative decoding;Absorbing sets;approximate cycle extrinsic message degree (ACE);dominant trapping sets;elementary trapping sets;error floor;error floor estimation;low-density parity-check (LDPC) codes;short cycles;trapping sets},
  Timestamp                = {2013.12.17}
}

@InProceedings{1286526,
  Title                    = {Semi-parallel reconfigurable architectures for real-time LDPC decoding},
  Author                   = {Karkooti, M. and Cavallaro, J.R.},
  Booktitle                = {Information Technology: Coding and Computing, 2004. Proceedings. ITCC 2004. International Conference on},
  Year                     = {2004},
  Month                    = {April},
  Pages                    = {579-585 Vol.1},
  Volume                   = {1},

  Abstract                 = {This paper presents a semi-parallel architecture for decoding low density parity check (LDPC) codes. A modified version of min-sum algorithm has been used which the advantage of simpler computations has compared to sum-product algorithm without any loss in performance. Special structure of the parity check matrix of the proposed code leads to an efficient semi-parallel implementation of the decoder for a family of (3, 6) LDPC codes. A prototype architecture has been implemented in VHDL on programmable hardware. The design is easily scalable and reconfigurable for larger block sizes. Simulation results show that our proposed decoder for a block length of 1536 bits can achieve data rates up to 127 Mbps.},
  Doi                      = {10.1109/ITCC.2004.1286526},
  File                     = {:PDF\\01286526.pdf:PDF},
  Keywords                 = {field programmable gate arrays;hardware description languages;parallel architectures;parity check codes;reconfigurable architectures;FPGA implementation;VHDL;area-time tradeoffs;channel coding;low density parity check codes;min-sum algorithm;minsum algorithm;parallel architecture;parity check matrix;programmable hardware;prototype architecture;real-time LDPC decoding;semiparallel reconfigurable architectures;sum-product algorithm;Computer architecture;Error correction codes;Field programmable gate arrays;Hardware;Iterative decoding;Parallel architectures;Parity check codes;Reconfigurable architectures;Throughput;Turbo codes},
  Timestamp                = {2015.07.17}
}

@Article{1054640,
  Title                    = {On majority-logic decoding for duals of primitive polynomial codes},
  Author                   = { Kasami, T. and Shu Lin},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1971},

  Month                    = may,
  Number                   = {3},
  Pages                    = { 322 - 331},
  Volume                   = {17},

  Abstract                 = { The class of polynomial codes introduced by Kasami et al. has considerable inherent algebraic and geometric structure. It has been shown that this class of codes and their dual codes contain many important classes of cyclic codes as subclasses, such as BCH codes, Reed-Solomon codes, generalized Reed-Muller codes, projective geometry codes, and Euclidean geometry codes. The purpose of this paper is to investigate further properties of polynomial codes and their duals. First, majority-logic decoding for the duals of certain primitive polynomial codes is considered. Two methods of forming nonorthogonal parity-check sums are presented. Second, the maximality of Euclidean geometry codes is proved. The roots of the generator polynomial of an Euclidean geometry code are specified.},
  Doi                      = {10.1109/TIT.1971.1054640},
  File                     = {:PDF\\On_Majority-Logic_Decoding_for_Duals_of_Primitive_polynomial_Codes.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Dual codes; Geometry codes; Majority logic decoding; Polynomial codes;}
}

@Article{1054127,
  Title                    = {New generalizations of the Reed-Muller codes--I: Primitive codes},
  Author                   = { Kasami, T. and Shu Lin and Peterson, W.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1968},

  Month                    = mar,
  Number                   = {2},
  Pages                    = { 189 - 199},
  Volume                   = {14},

  Abstract                 = { First it is shown that all binary Reed-Muller codes with one digit dropped can be made cyclic by rearranging the digits. Then a natural generalization to the nonbinary case is presented, which also includes the Reed-Muller codes and Reed-Solomon codes as special cases. The generator polynomial is characterized and the minimum weight is established. Finally, some results on weight distribution are given.},
  Doi                      = {10.1109/TIT.1968.1054127},
  File                     = {:C\:\\Users\\Public\\Documents\\My eBooks\\refereces\\PDF\\01054127.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Cyclic codes; Reed-Muller codes;}
}

@Article{1054226,
  Title                    = {Polynomial codes},
  Author                   = { Kasami, T. and Shu Lin and Peterson, W.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1968},

  Month                    = nov,
  Number                   = {6},
  Pages                    = { 807 - 814},
  Volume                   = {14},

  Abstract                 = { A class of cyclic codes is introduced by a polynomial approach that is an extension of the Mattson-Solomon method and of the Muller method. This class of codes contains several important classes of codes as subclasses, namely, BCH codes, Reed-Solomon codes, generalized primitive Reed-Muller codes, and finite geometry codes. Certain fundamental properties of this class of codes are derived. Some subclasses are shown to be majority-logic decodable.},
  Doi                      = {10.1109/TIT.1968.1054226},
  File                     = {:PDF\\Polynomial_Codes.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { BCH codes; Geometry codes; Polynomial codes; Reed-Muller codes; Reed-Solomon codes;}
}

@Article{PhysRev.76.1232,
  Title                    = {Crystal Statistics. II. Partition Function Evaluated by Spinor Analysis},
  Author                   = {Kaufman, Bruria},
  Journal                  = {Phys. Rev.},
  Year                     = {1949},

  Month                    = {Oct},
  Pages                    = {1232--1243},
  Volume                   = {76},

  Doi                      = {10.1103/PhysRev.76.1232},
  File                     = {:PDF\\PhysRev.76.1232.pdf:PDF},
  Issue                    = {8},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.07.25},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRev.76.1232}
}

@Article{PhysRev.76.1244,
  Title                    = {Crystal Statistics. III. Short-Range Order in a Binary Ising Lattice},
  Author                   = {Kaufman, Bruria and Onsager, Lars},
  Journal                  = {Phys. Rev.},
  Year                     = {1949},

  Month                    = {Oct},
  Pages                    = {1244--1252},
  Volume                   = {76},

  Doi                      = {10.1103/PhysRev.76.1244},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\PhysRev.76.1244.pdf:PDF},
  Issue                    = {8},
  Publisher                = {American Physical Society},
  Timestamp                = {2013.06.25},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRev.76.1244}
}

@Article{1207365,
  Title                    = {Binary intersymbol interference channels: Gallager codes, density evolution, and code performance bounds},
  Author                   = {Kavcic, A. and Xiao Ma and Mitzenmacher, M.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {july},
  Number                   = {7},
  Pages                    = { 1636 - 1652},
  Volume                   = {49},

  Abstract                 = {We study the limits of performance of Gallager codes (low-density parity-check (LDPC) codes) over binary linear intersymbol interference (ISI) channels with additive white Gaussian noise (AWGN). Using the graph representations of the channel, the code, and the sum-product message-passing detector/decoder, we prove two error concentration theorems. Our proofs expand on previous work by handling complications introduced by the channel memory. We circumvent these problems by considering not just linear Gallager codes but also their cosets and by distinguishing between different types of message flow neighborhoods depending on the actual transmitted symbols. We compute the noise tolerance threshold using a suitably developed density evolution algorithm and verify, by simulation, that the thresholds represent accurate predictions of the performance of the iterative sum-product algorithm for finite (but large) block lengths. We also demonstrate that for high rates, the thresholds are very close to the theoretical limit of performance for Gallager codes over ISI channels. If C denotes the capacity of a binary ISI channel and if Ci.i.d. denotes the maximal achievable mutual information rate when the channel inputs are independent and identically distributed (i.i.d.) binary random variables (Ci.i.d. le;C), we prove that the maximum information rate achievable by the sum-product decoder of a Gallager (coset) code is upper-bounded by Ci.i.d.. The last topic investigated is the performance limit of the decoder if the trellis portion of the sum-product algorithm is executed only once; this demonstrates the potential for trading off the computational requirements and the performance of the decoder.},
  Doi                      = {10.1109/TIT.2003.813563},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01207365.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { AWGN; Gallager codes; LDPQ codes; additive white Gaussian noise; binary ISI channel; binary intersymbol interference channels; binary linear intersymbol interference channels; channel inputs; channel memory; code performance bounds; cosets; density evolution; error concentration theorems; graph representations; i.i.d. binary random variables; independent and identically distributed binary random variables; iterative sum-product algorithm; low-density parity-check codes; maximal achievable mutual information rate; maximum information rate achievable; message flow neighborhoods; noise tolerance threshold; sum-product decoder; sum-product message-passing detector/decoder; AWGN channels; binary codes; channel capacity; channel coding; decoding; graph theory; intersymbol interference; linear codes; message passing;},
  Timestamp                = {2011.11.24}
}

@InProceedings{5335615,
  Title                    = {Spin-transfer torque RAM: A road to universal memory},
  Author                   = {Kawahara, T.},
  Booktitle                = {Modern Problems of Nanoelectronics, Micro- and Nanosystem Technologies, 2009. INTERNANO 2009. International School and Seminar on},
  Year                     = {2009},
  Month                    = oct,
  Pages                    = {160 -185},

  Abstract                 = {A collection of slides from the author's conference presentation is given. The title is "Spin-transfer torque RAM: a road to universal memory".},
  Doi                      = {10.1109/INTERNANO.2009.5335615},
  File                     = {:PDF\\Spin-Transfer_Torque_RAM_A_road_to_universal_memory.pdf:PDF},
  Keywords                 = {RAM;random-access storage;universal memory;random-access storage;}
}

@InProceedings{5707027,
  Title                    = {Multiple-write WOM-codes},
  Author                   = {Kayser, S. and Yaakobi, E. and Siegel, P.H. and Vardy, A. and Wolf, J.K.},
  Booktitle                = {Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on},
  Year                     = {2010},
  Month                    = {Sept},
  Pages                    = {1062-1068},

  Abstract                 = {A Write Once Memory (WOM) is a storage device that consists of cells that can take on q possible linearly-ordered values, with the added constraint that rewrites can only increase a cell's value. In the binary case, each cell can change from the level zero to the level one only once. Examples of WOMs include punch cards, optical disks, and more recently flash memories. A length-n, t-write WOM-code is a coding scheme that allows t messages to be stored in n cells. If in the i-th write we write one of Mi messages, then the rate of the i-th write is the ratio of the number of bits written to the WOM to the total number of cells used, i.e., log2(Mi)/n. The rate of the WOM-code is the sum of all individual rates in all writes. In this paper, we review a recent construction of binary two-write WOM-codes. The construction is generalized for two-write WOM-codes with q levels per cell. Then, we show how to use such a code with ternary cells in order to construct three and four-write WOM-codes. This construction is used recursively in order to generate a family of t-write WOM-codes for all t. Another generalized construction is given which provides us with more ways to construct families of WOM-codes. Finally, we give a comparison between our codes and the best known WOM-codes in order to show that the WOM-codes constructed here outperform all previously known WOM-codes for 3 ≤ t ≤ 10.},
  Doi                      = {10.1109/ALLERTON.2010.5707027},
  File                     = {:PDF\\05707027.pdf:PDF},
  Keywords                 = {write-once storage;flash memories;multiple-write WOM-codes;optical disks;punch cards;q possible linearly-ordered values;t-write WOM-codes;write once memory;Ash;Decoding;Linear code;Parity check codes;USA Councils;Vectors},
  Timestamp                = {2015.06.11}
}

@Article{PhysRev.81.988,
  Title                    = {A Theory of Cooperative Phenomena},
  Author                   = {Kikuchi, Ryoichi},
  Journal                  = {Phys. Rev.},
  Year                     = {1951},

  Month                    = {Mar},
  Pages                    = {988--1003},
  Volume                   = {81},

  Doi                      = {10.1103/PhysRev.81.988},
  File                     = {:PDF\\kikuchiPhysRev81-988.pdf:PDF},
  Issue                    = {6},
  Numpages                 = {0},
  Publisher                = {American Physical Society},
  Timestamp                = {2015.02.06},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRev.81.988}
}

@InProceedings{6026357,
  Title                    = {A high-speed layered min-sum LDPC decoder for error correction of NAND Flash memories},
  Author                   = {Jonghong Kim and Junhee Cho and Wonyong Sung},
  Booktitle                = {Circuits and Systems (MWSCAS), 2011 IEEE 54th International Midwest Symposium on},
  Year                     = {2011},
  Month                    = {aug},
  Pages                    = {1 -4},

  Abstract                 = {NAND Flash memory controllers need to equip strong and high speed error correction blocks as the cell size scales down and multi-level cell technology is employed. We have developed an LDPC (low-density parity-check) decoder for NAND Flash memory error correction, and implemented it using a layered min-sum decoding architecture. A shortened (69615, 66897) regular EG-LDPC code that has the code rate of 96% is used, which has a good minimum distance and quasi-cyclic structure. In order to increase the decoding throughput and reduce the chip area, the word-length reduction of variable-to-check messages, compression of the check-to-variable information, and pipelined parallel architecture are employed. Furthermore, fixed-point arithmetic optimization of node update processing units is also conducted to mitigate the quantization error, thereby enhances the error performance of the decoder. The synthesis and simulation results show that the SRAM area storing check-to-variable messages is much reduced, which leads to 38% saving in hardware area compared to the non-optimized serial architecture, and the design also exhibits a good error performance that is close to that of the floating-point implementation. The decoder can achieve the maximum decoding throughput of 6.24Gb/s and occupies the chip area of 48m m2 with 0.13um CMOS process.},
  Doi                      = {10.1109/MWSCAS.2011.6026357},
  File                     = {:PDF\\06026357.pdf:PDF},
  ISSN                     = {1548-3746},
  Keywords                 = {CMOS process;NAND Flash memory controllers;SRAM area;check-to-variable information compression;check-to-variable messages;fixed-point arithmetic optimization;floating-point implementation;high-speed error correction blocks;high-speed layered min-sum LDPC decoder;low density parity check decoder;multilevel cell technology;node update processing units;pipelined parallel architecture;quantization error mitigation;regular EG-LDPC code;size 0.13 mum;variable-to-check messages;word-length reduction;CMOS logic circuits;CMOS memory circuits;NAND circuits;SRAM chips;error correction codes;flash memories;parallel architectures;parity check codes;},
  Timestamp                = {2012.10.22}
}

@InProceedings{6364973,
  Title                    = {Performance of rate 0.96 (68254, 65536) EG-LDPC code for NAND Flash memory error correction},
  Author                   = {Jonghong Kim and Dong-hwan Lee and Wonyong Sung},
  Booktitle                = {Communications (ICC), 2012 IEEE International Conference on},
  Year                     = {June},
  Pages                    = {7029-7033},

  Abstract                 = {As the process technology scales down and the number of bits per cell increases, NAND Flash memory is more prone to bit errors. In this paper, we employ a rate-0.96 (68254, 65536) Euclidean geometry (EG) low-density parity-check (LDPC) code for NAND Flash memory error correction, and evaluate the performance under binary input (BI) additive white Gaussian noise (AWGN) and NAND Flash memory channels. The performance effect of output signal quantization is also studied. We show the strategies for determining the optimum quantization boundaries and computing the quantized log-likelihood ratio (LLR) for the NAND Flash channel model that is approximated as a mixture of Gaussian distributions. Simulation results show that the error performance with the NAND Flash memory channel is much different from that with the BI-AWGN channel. Since the distribution of NAND Flash memory output signal is not stationary, it is important to accurately assess the stochastic distribution of the signal for optimum sensing.},
  Doi                      = {10.1109/ICC.2012.6364973},
  File                     = {:PDF\\06364973.pdf:PDF},
  ISSN                     = {1550-3607},
  Keywords                 = {Gaussian distribution;NAND circuits;error correction codes;flash memories;geometry;parity check codes;signal processing;BI AWGN channel;EG;Euclidean geometry low-density parity-check code;Gaussian distributions;LLR;NAND Flash memory error correction;binary input additive white Gaussian noise;optimum quantization boundaries;optimum sensing;output signal quantization;quantized log-likelihood ratio;rate 0.96 (68254, 65536) EG-LDPC code;stochastic distribution;Bit error rate;Error correction codes;Flash memory;Parity check codes;Quantization;Sensors;Threshold voltage;LDPC codes;NAND Flash memory;quantization},
  Timestamp                = {2013.03.11}
}

@Article{1687-6180-2012-195,
  Title                    = {Low-energy error correction of NAND Flash memory through soft-decision decoding},
  Author                   = {Jonghong Kim and Wonyong Sung},
  Journal                  = {EURASIP Journal on Advances in Signal Processing},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {1-12},
  Volume                   = {2012},

  File                     = {:PDF\\1687-6180-2012-195.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.07}
}

@InProceedings{6292015,
  Title                    = {Rank modulation hardware for flash memories},
  Author                   = {Mina Kim and Jong Kap Park and Twigg, C.M.},
  Booktitle                = {Circuits and Systems (MWSCAS), 2012 IEEE 55th International Midwest Symposium on},
  Year                     = {2012},
  Month                    = {Aug},
  Pages                    = {294-297},

  Abstract                 = {Flash has been widely used as nonvolatile memories, such as secondary or long-term persistent storage, but it has recently been adopted for solid-state drives in many mobile applications. Advances in flash technology, such as multi-level cells and error correction, have improved storage density and reliability, but these characteristics continue to be a challenge as the technology scales to smaller dimensions. Most notably, these techniques have been unable to directly deal with the failures caused by block erasures, except by spreading out the erasures across the entire memory through wear leveling. Rank modulation provides a new approach to multi-level flash memory cells; it uses the relative ranking of cell levels, instead of their absolute values. A flash memory with rank modulation based upon winner-take-all circuits is proposed, simulated, and fabricated. Using rank modulation, memory reliability and capacity is further improved.},
  Doi                      = {10.1109/MWSCAS.2012.6292015},
  File                     = {:PDF\\06292015.pdf:PDF},
  ISSN                     = {1548-3746},
  Keywords                 = {flash memories;reliability;block erasures;error correction;flash memories;long term persistent storage;memory reliability;multilevel flash memory cells;nonvolatile memories;rank modulation hardware;solid state drives;storage density;wear leveling;winner-take-all circuits;Arrays;Ash;Encoding;Memory management;Modulation;Programming;Reliability},
  Timestamp                = {2015.03.31}
}

@Article{4276926,
  Title                    = {Quasi-Cyclic Low-Density Parity-Check Codes With Girth Larger Than 12},
  Author                   = {Sunghwan Kim and Jong-Seon No and Habong Chung and Dong-Joon Shin},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {aug},
  Number                   = {8},
  Pages                    = {2885 -2891},
  Volume                   = {53},

  Abstract                 = {A quasi-cyclic (QC) low-density parity-check (LDPC) code can be viewed as the protograph code with circulant permutation matrices (or circulants). In this correspondence, we find all the subgraph patterns of protographs of QC LDPC codes having inevitable cycles of length 2i, i = 6, 7, 8, 9,10, i.e., the cycles that always exist regardless of the shift values of circulants. It is also derived that if the girth of the protograph is 2g, g gt; 2, its protograph code cannot have the inevitable cycles of length smaller than 6g. Based on these subgraph patterns, we propose new combinatorial construction methods of the protographs, whose protograph codes can have girth larger than or equal to 14 or 18. We also propose a couple of shift value assigning rules for circulants of a QC LDPC code guaranteeing the girth 14.},
  Doi                      = {10.1109/TIT.2007.901193},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04276926.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {circulant permutation matrices;combinatorial construction method;protograph code;quasi-cyclic low-density parity-check codes;subgraph patterns;graph theory;matrix algebra;parity check codes;},
  Timestamp                = {2011.12.06}
}

@InProceedings{1010932,
  Title                    = {Parallel VLSI architectures for a class of LDPC codes},
  Author                   = {Sungwook Kim and Sobelman, G.E. and Jaekyun Moon},
  Booktitle                = {Circuits and Systems, 2002. ISCAS 2002. IEEE International Symposium on},
  Year                     = {2002},
  Pages                    = { II-93 - II-96 vol.2},
  Volume                   = {2},

  Abstract                 = {This paper presents high-performance encoder and decoder architectures for a class of low density parity check (LDPC) codes. The codes considered here are based on the parallelly concatenated parity check encoder structure. A major advantage of these codes is that the generator matrix and the parity check matrix are both sparse, which leads to efficient VLSI implementations for the encoder and the decoder. Our designs use 6-bit quantization with a code rate of 8/9 and a block size of 576 bits. An evaluation of the speed and hardware complexity is given, and simulation results for the bit error rate are obtained},
  Doi                      = {10.1109/ISCAS.2002.1010932},
  File                     = {:PDF\\01010932.pdf:PDF},
  Keywords                 = {LDPC codes; VLSI architectures; bit error rate; decoder architecture; generator matrix; low density parity check codes; parallel architectures; parallelly concatenated parity check encoder; parity check matrix; quantization; VLSI; concatenated codes; decoding; error statistics; integrated circuit design; parallel architectures; quantisation (signal); sparse matrices;},
  Timestamp                = {2011.04.22}
}

@InProceedings{5984614,
  Author                   = {Kim, Wanki and Park, Sung Il and Zhang, Zhiping and Yang-Liauw, Young and Sekar, Deepak and Wong, H.-S. Philip and Wong, S. Simon},
  Booktitle                = {VLSI Technology (VLSIT), 2011 Symposium on},
  Month                    = {june},
  Pages                    = {22 -23},

  Abstract                 = {Nitrogen-doped AlOX Resistive RAM has been integrated on CMOS. The memory cell requires no forming, and sub- #x03BC;A programming currents. The cell is capable of multi-bit storage, reliable for over 105 switching cycles and 10 years retention at 125 #x00B0;C.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984614.pdf:PDF},
  ISSN                     = {0743-1562},
  Timestamp                = {2011.08.28}
}

@InProceedings{5984636,
  Author                   = {Kim, Y. and Oh, S. C. and Lim, W. C. and Kim, J. H. and Kim, W. J. and Jeong, J. H. and Shin, H. J. and Kim, K. W. and Kim, K. S. and Park, J. H. and Park, S. H. and Kwon, H. and Ah, K.H. and Lee, J. E. and Park, S. O. and Choi, S. and Kang, H. K. and Chung, C.},
  Booktitle                = {VLSI Technology (VLSIT), 2011 Symposium on},
  Month                    = {june},
  Pages                    = {210 -211},

  Abstract                 = {28nm MTJ for 8 #x223C;16Gb MRAM device has been successfully integrated with special patterning amp; etch technique. Resistance (R) separation between high and low R states was 15.2 #x03C3;, comparable to that for 80nm MTJ cells. Thermal stability factor ( #x0394;) followed prediction fairly well, and MTJ with free layer (FL) of 25 #x00C5; and aspect ratio (AR) of 3 showed #x0394; of 56. In order to realize sub-30nm MRAM device, a novel FL with substantially low critical current density (Jc) or revolutionary MTJ scheme needs to be developed.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984636.pdf:PDF},
  ISSN                     = {0743-1562},
  Timestamp                = {2011.08.28}
}

@InProceedings{5984628,
  Title                    = {Bi-layered RRAM with unlimited endurance and extremely uniform switching},
  Author                   = {Kim, Young-Bae and Lee, Seung Ryul and Lee, Dongsoo and Lee, Chang Bum and Chang, Man and Hur, Ji Hyun and Lee, Myoung-Jae and Park, Gyeong-Su and Kim, Chang Jung and Chung, U-In and Yoo, In-Kyeong and Kim, Kinam},
  Booktitle                = {VLSI Technology (VLSIT), 2011 Symposium on},
  Year                     = {2011},
  Month                    = {june},
  Pages                    = {52 -53},

  Abstract                 = {We demonstrate resistive random access memory (RRAM) architecture with bi-layered switching element for reliable resistive switching memory. Based on the modulated Schottky barrier modeling, several key functions to achieve a realiable bipolar switching property are extracted. Our device shows an excellent memory performance such as enduracne of 1011 cycles at 30ns, data retention of #x003E;104s at 200 #x00B0;C, and calculated bit error rate below 10 #x2212;11.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05984628.pdf:PDF},
  ISSN                     = {0743-1562},
  Timestamp                = {2011.08.28}
}

@Article{Kinzel1985,
  Title                    = {Learning and pattern recognition in spin glass models},
  Author                   = {Kinzel, W.},
  Journal                  = {Zeitschrift fur Physik B Condensed Matter},
  Year                     = {1985},
  Number                   = {2-4},
  Pages                    = {205-213},
  Volume                   = {60},

  Doi                      = {10.1007/BF01304440},
  File                     = {:PDF\\10.1007BF01304440.pdf:PDF},
  ISSN                     = {0722-3277},
  Language                 = {English},
  Publisher                = {Springer-Verlag},
  Timestamp                = {2015.07.21},
  Url                      = {http://dx.doi.org/10.1007/BF01304440}
}

@Article{Kirchner2010,
  Title                    = {Spin Path Integrals, Berry Phase, and the Quantum Phase Transition in the Sub-Ohmic Spin-Boson Model},
  Author                   = {Kirchner, Stefan},
  Journal                  = {Journal of Low Temperature Physics},
  Year                     = {2010},
  Number                   = {1-2},
  Pages                    = {282-298},
  Volume                   = {161},

  Doi                      = {10.1007/s10909-010-0193-4},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\raey.pdf:PDF},
  ISSN                     = {0022-2291},
  Keywords                 = {Quantum criticality; Quantum phase transition; Quantum-to-classical mapping; Spin-boson model; Spin coherent states; Spin path integrals; Berry phase},
  Language                 = {English},
  Publisher                = {Springer US},
  Timestamp                = {2013.10.10},
  Url                      = {http://dx.doi.org/10.1007/s10909-010-0193-4}
}

@Article{shiftregistersynthesis,
  Title                    = {Register synthesis for algebraic feedback shift registers based on non-primes},
  Author                   = {Andrew Klapper and Jinzhong Xu},
  Journal                  = {Designs, Codes and Cryptography},
  Year                     = {2004},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {227-250},
  Volume                   = {31},

  Publisher                = {Springer Netherlands}
}

@InProceedings{4813497,
  Title                    = {Web Performance Enhancement of E-business System Using the SSD},
  Author                   = {Dae-Sik Ko and Seung-Kook Cheong},
  Booktitle                = {Future Generation Communication and Networking Symposia, 2008. FGCNS '08. Second International Conference on},
  Year                     = {2008},
  Month                    = {dec.},
  Pages                    = {81 -84},
  Volume                   = {1},

  Abstract                 = {In this paper, we proposed web performance enhancement of the e-business system using the SSD (Solid State Drive). In the e-business system, transactions must be strictly ordered, should occur at a fixed point time, and not be lost. We approached to solve e-business problem by using software and hardware techniques such as SSD. X-internet has advantage of C/S and Web applications. Since X-internet technique enables to reduce the loading time of web pages, X-internet is useful to enhance performance of the e-business system. SSD is a storage device that uses DRAM or NAND Flash as primary storage media. Since the SSD stores and accesses data directly to memory chips, which results in storage speeds far greater than conventional magnetic storage devices (HDD). Therefore x-internet techniques can be used to solve confused browsing and poor UI, and SSD can be used to solve I/O bottleneck of the e-business system.},
  Doi                      = {10.1109/FGCNS.2008.100},
  File                     = {:PDF\\05370894.pdf:PDF},
  Keywords                 = {DRAM;NAND Flash;SSD;Web performance enhancement;X- internet technique;e-business system;memory chips;primary storage media;solid state drive;DRAM chips;Internet;electronic commerce;flash memories;},
  Timestamp                = {2011.05.19}
}

@InProceedings{6979914,
  Title                    = {Position modulation code for non-binary Write-Once Memories},
  Author                   = {Kobayashi, T. and Morita, H. and Manada, A.},
  Booktitle                = {Information Theory and its Applications (ISITA), 2014 International Symposium on},
  Year                     = {2014},
  Month                    = {Oct},
  Pages                    = {600-604},

  Abstract                 = {Wu and Jiang proposed a code for rewriting Write-Once Memories (WOM's), and they called it position modulation code. In this paper, we propose an analogy of their code for non-binary WOM's. We show that the proposed code asymptotically achieves the sum rate at least half of that achievable by the best possible code.},
  File                     = {:PDF\\06979914.pdf:PDF},
  Keywords                 = {modulation coding;write-once storage;WOM;nonbinary write-once memories;position modulation code;Australia;Decoding;Educational institutions;Encoding;Modulation;Polynomials;Vectors},
  Timestamp                = {2015.06.08}
}

@Article{681314,
  Title                    = {A fast parallel implementation of a Berlekamp-Massey algorithm for algebraic-geometric codes},
  Author                   = {Kotter, R.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {Jul},
  Number                   = {4},
  Pages                    = {1353-1368},
  Volume                   = {44},

  Abstract                 = {We obtain a parallel Berlekamp-Massey-type algorithm for determining error locating functions for the class of one point algebraic-geometric codes. The proposed algorithm has a regular and simple structure and is suitable for VLSI implementation. We give an outline for an implementation, which uses as main blocks γ copies of a modified one-dimensional Berlekamp-Massey algorithm, where γ is the order of the first nongap in the function space associated with the code. Such a parallel implementation determines the error locator for an algebraic-geometric code using the same time requirements as the underlying one-dimensional Berlekamp-Massey algorithm applied to the decoding of Reed-Solomon codes},
  Doi                      = {10.1109/18.681314},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00681314.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {algebraic geometric codes;decoding;parallel algorithms;Berlekamp-Massey algorithm;VLSI implementation;algebraic-geometric codes;decoding;error locating functions;fast parallel implementation;one-dimensional Berlekamp-Massey algorithm;Algorithm design and analysis;Computational complexity;Concatenated codes;Decoding;Hamming distance;Hardware;Helium;Logic;Maintenance engineering;Very large scale integration},
  Timestamp                = {2014.05.16}
}

@Article{959255,
  Title                    = {Low-density parity-check codes based on finite geometries: a rediscovery and new results},
  Author                   = {Kou, Y. and Lin, S. and Fossorier, M.P.C.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = nov,
  Number                   = {7},
  Pages                    = {2711 -2736},
  Volume                   = {47},

  Abstract                 = {This paper presents a geometric approach to the construction of low-density parity-check (LDPC) codes. Four classes of LDPC codes are constructed based on the lines and points of Euclidean and projective geometries over finite fields. Codes of these four classes have good minimum distances and their Tanner (1981) graphs have girth 6. Finite-geometry LDPC codes can be decoded in various ways, ranging from low to high decoding complexity and from reasonably good to very good performance. They perform very well with iterative decoding. Furthermore, they can be put in either cyclic or quasi-cyclic form. Consequently, their encoding can be achieved in linear time and implemented with simple feedback shift registers. This advantage is not shared by other LDPC codes in general and is important in practice. Finite-geometry LDPC codes can be extended and shortened in various ways to obtain other good LDPC codes. Several techniques of extension and shortening are presented. Long extended finite-geometry LDPC codes have been constructed and they achieve a performance only a few tenths of a decibel away from the Shannon theoretical limit with iterative decoding },
  Doi                      = {10.1109/18.959255},
  File                     = {:PDF\\Low-Density_Parity-Check_Codes_Based_on_Finite_Geometries_A_Rediscovery_and_New_Results.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Euclidean geometry;Shannon theoretical limit;Tanner graphs;concatenated codes;cyclic codes;decoding complexity;error control codes;feedback shift registers;finite geometries;finite-geometry LDPC codes;iterative decoding;linear time encoding;long extended finite-geometry LDPC codes;low-density parity-check codes;minimum distance;projective geometry;quasi-cyclic codes;shortened codes;turbo codes;computational complexity;concatenated codes;cyclic codes;error correction codes;error detection codes;iterative decoding;turbo codes;}
}

@InProceedings{5513685,
  Title                    = {The predictable leading monomial property for polynomial vectors over a ring},
  Author                   = {Kuijper, M. and Schindelar, K.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium on},
  Year                     = {2010},
  Month                    = {june},
  Pages                    = {1133 -1137},

  Abstract                 = {The "predictable degree property", a terminology introduced by Forney in 1970, is a property of polynomial matrices over a field F that has proven itself to be fundamentally useful for a range of applications. In this paper we strengthen this property into the "predictable leading monomial" property, and show that this PLM property is shared by minimal Gr\"{o}bner bases for any positional term order (here: TOP and POT) in $\mathbb{F}_{q}[t]$. The property is useful particularly for minimal interpolation-type problems. Because of the presence of zero divisors, minimal Gro\"{o}bner bases over a finite ring of the type $\mathbb{Z}_{p^{r}}$ (where p is a prime integer and r is an integer > 1) do not have the PLM property. We show how to construct, from an ordered minimal Gr\"{o}bner basis, a so-called minimal Gr\"{o}bner p-basis that does have a PLM property. The parametrization of all shortest linear recurrence relations of a finite sequence over $\mathbb{Z}_{p^{r}}$ is a type of problem for which this is useful and we include an illustrative example.},
  Doi                      = {10.1109/ISIT.2010.5513685},
  File                     = {:PDF\\05513685.pdf:PDF},
  Keywords                 = {minimal Gr\"{o}bner p-basis;polynomial vectors;predictable degree property;predictable leading monomial property;polynomial matrices;vectors;},
  Timestamp                = {2012.05.22}
}

@InProceedings{5399804,
  Title                    = {Gr\"{o}bner bases and behaviors over finite rings},
  Author                   = {Kuijper, M. and Schindelar, K.},
  Booktitle                = {Decision and Control, 2009 held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009. Proceedings of the 48th IEEE Conference on},
  Year                     = {2009},
  Month                    = {dec.},
  Pages                    = {8101 -8106},

  Abstract                 = {For several decades Grobner bases have proved useful tools for different areas in system theory, particularly multidimensional system theory. These areas range from controller design to minimal realizations of linear systems over fields. In this paper we focus on the univariate case and identify the so-called "predictable leading monomial property" as a property of a minimal Gr\"{o}bner basis that is crucial in many of these areas. The property is stronger than "row reducedness". We revisit the recently developed theory of in which row reducedness is extended to polynomial matrices over the finite ring \mathbb{Z}_{p^{r}} (with p a prime integer and r a positive integer), which find applications in error control coding over $\mathbb{Z}_{p^{r}}$. We recast the ideas of in the more general setting of Grobner bases and derive new results on how to use minimal Gr\"{o}bner bases to achieve the predictable leading monomial property over \mathbb{Z}_{p^{r}}. A major advantage of the Gr\"{o}bner approach is that computational packages are available to compute a minimal Grobner basis over $\mathbb{Z}_{p^{r}}$, such as the SINGULAR computer algebra system. Another advantage of the Grobner approach is its generality with respect to the choice of ordering of polynomial vectors.},
  Doi                      = {10.1109/CDC.2009.5399804},
  File                     = {:PDF\\05399804.pdf:PDF},
  ISSN                     = {0191-2216},
  Keywords                 = {Gr\"{o}bner bases;SINGULAR computer algebra system;computational packages;controller design;error control coding;finite rings;linear systems;minimal Grobner basis;multidimensional system theory;polynomial matrices;predictable leading monomial property;row reducedness;mathematics computing;polynomial matrices;system theory;},
  Timestamp                = {2012.05.22}
}

@InProceedings{5513611,
  Title                    = {Exhaustive search for small fully absorbing sets and the corresponding low error-floor decoder},
  Author                   = {Gyu Bum Kyung and Chih-Chun Wang},
  Booktitle                = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium on},
  Year                     = {2010},
  Pages                    = {739-743},

  Abstract                 = {This work provides an exhaustive search algorithm for finding small fully absorbing sets (FASs) of arbitrary low-density parity-check (LDPC) codes. In particular, given any LDPC code, the problem of finding all FASs of size less than t is formulated as an integer programming problem, for which a new branch-&-bound algorithm is devised. New node selection and the tree-trimming mechanisms are designed to further enhance the efficiency of the algorithm. The proposed algorithm is capable of finding all FASs of size ≤ 11 with no larger than 2 induced odd-degree check nodes for LDPC codes of length ≤ 1000. The resulting exhaustive list of small FASs is then used to devise a new post-processing decoder. Numerical results show that by taking advantage of the exhaustive list of small FASs, the proposed decoder significantly lowers the error floor for codes of practical lengths and outperforms the state-of-the-art low-error-floor decoders.},
  Doi                      = {10.1109/ISIT.2010.5513611},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05513611.pdf:PDF},
  Keywords                 = {decoding;integer programming;parity check codes;set theory;tree searching;LDPC codes;arbitrary low-density parity-check;branch-bound algorithms;exhaustive search;fully absorbing sets;integer programming problem;low error-floor decoder;low-error-floor decoders;post-processing decoder;trimming mechanisms;Algorithm design and analysis;Belief propagation;Computer errors;Electronic mail;Hamming distance;Iterative algorithms;Iterative decoding;Linear programming;Maximum likelihood decoding;Parity check codes},
  Timestamp                = {2013.12.19}
}

@Article{4100904,
  Title                    = {LDPC Codes Based on Latin Squares: Cycle Structure, Stopping Set, and Trapping Set Analysis},
  Author                   = {Laendner, S. and Milenkovic, O.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {303-312},
  Volume                   = {55},

  Abstract                 = {It is well known that certain combinatorial structures in the Tanner graph of a low-density parity-check (LDPC) code exhibit a strong influence on its performance under iterative decoding. These structures include cycles, stopping/trapping sets, and parameters such as the diameter of the code. In general, it is very hard to find a complete characterization of such configurations in an arbitrary code, and even harder to understand the intricate relationships that exist between these entities. It is, therefore, of interest to identify a simple setting in which all the described combinatorial structures can be enumerated and studied within a joint framework. One such setting is developed in this paper, for the purpose of analyzing the distribution of short cycles and the structure of stopping and trapping sets in Tanner graphs of LDPC codes based on idempotent and symmetric Latin squares. The parity-check matrices of LDPC codes based on Latin squares have a special form that allows for connecting combinatorial parameters of the codes with the number of certain subrectangles in the Latin squares. Subrectangles of interest can be easily identified, and in certain instances, completely enumerated. This study can be extended in several different directions, one of which is concerned with modifying the code design process in order to eliminate or reduce the number of configurations bearing a negative influence on the performance of the code. Another application of the results includes determining to which extent a configuration governs the behavior of the bit-error rate curve in the waterfall and error-floor regions},
  Doi                      = {10.1109/TCOMM.2006.888633},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04100904.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {graph theory;iterative decoding;matrix algebra;parity check codes;set theory;LDPC codes;Latin squares;Tanner graph;cycle structure;iterative decoding;low-density parity-check code;parity-check matrices;stopping set;trapping set analysis;AWGN;Bit error rate;Data communication;Iterative decoding;Joining processes;Memoryless systems;Parity check codes;Performance analysis;Process design;Symmetric matrices;Cayley Latin squares;design theory;low-density parity-check (LDPC) codes;stopping sets;trapping sets},
  Timestamp                = {2014.03.04}
}

@Article{Lally2001157,
  Title                    = {Algebraic structure of quasicyclic codes},
  Author                   = {Kristine Lally and Patrick Fitzpatrick},
  Journal                  = {Discrete Applied Mathematics},
  Year                     = {2001},
  Number                   = {1-2},
  Pages                    = {157 - 175},
  Volume                   = {111},

  Abstract                 = {We use Groebner bases of modules as a tool in the construction and classification of quasicyclic codes. Whereas previous studies have been mainly concerned with the 1-generator case, our results elucidate the structure of arbitrary quasicyclic codes and their duals. As an application we provide a complete characterisation of self-dual quasicyclic codes of index 2.},
  Doi                      = {DOI: 10.1016/S0166-218X(00)00350-4},
  File                     = {:PDF\\lally-fitzpatrick-quasi-cyclic-code.pdf:PDF;:PDF\\10.1.1.5.3522.pdf:PDF},
  ISSN                     = {0166-218X},
  Timestamp                = {2011.04.15},
  Url                      = {http://www.sciencedirect.com/science/article/B6TYW-43CHKRC-C/2/24015ad5fa81e63f1066bd6bd7186551}
}

@InProceedings{1549481,
  Title                    = {Algorithmic and combinatorial analysis of trapping sets in structured LDPC codes},
  Author                   = {Landner, S. and Milenkovic, O.},
  Booktitle                = {Wireless Networks, Communications and Mobile Computing, 2005 International Conference on},
  Year                     = {2005},
  Pages                    = {630-635 vol.1},
  Volume                   = {1},

  Abstract                 = {Several combinatorial properties of low-density parity-check (LDPC) codes, such as minimum distance, diameter, stopping number, girth and cycle-length distribution of the corresponding Tanner graph, are known to influence their performance under iterative decoding. Recently, a new class of combinatorial configurations, termed trapping sets, was shown to be of significant importance in determining the properties of LDPC codes in the error-floor region. Very little is known both about the existence/parameters of trapping sets in structured LDPC codes and about possible techniques for reducing their negative influence on the code's performance. In this paper, we address both these problems from an algorithmic and combinatorial perspective. We first provide a numerical study of the trapping phenomena for the Margulis code, which exhibits a fairly high error-floor. Based on this analysis, conducted for two different implementations of iterative belief propagation, we propose a novel decoding process, termed averaged decoding. Averaged decoding provides for a significant reduction in the number of incorrectly decoded frames in the error-floor region of the Margulis code. Furthermore, based on the results of the algorithmic approach, we suggest a novel combinatorial characterizations of trapping sets in the class of LDPC codes based on finite geometries. Projective geometry LDPC codes are suspected to have extremely low error-floors, which is a property that we may attribute to the non-existence of certain small trapping sets in the code graph.},
  Doi                      = {10.1109/WIRLES.2005.1549481},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01549481.pdf:PDF},
  Keywords                 = {combinatorial mathematics;iterative decoding;parity check codes;Margulis code;Tanner graph;averaged decoding;code graph;combinatorial analysis;decoding process;error-floor region;finite geometries;iterative belief propagation;iterative decoding;low-density parity-check codes;structured LDPC codes;trapping sets;Algorithm design and analysis;Belief propagation;Bit error rate;Code standards;Computer errors;Geometry;Iterative algorithms;Iterative decoding;Parity check codes;Signal to noise ratio},
  Timestamp                = {2013.12.17}
}

@Article{1210770,
  Title                    = {Statistical simulation of leakage currents in MOS and flash memory devices with a new multiphonon trap-assisted tunneling model},
  Author                   = {Larcher, L.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {2003},

  Month                    = may,
  Number                   = {5},
  Pages                    = { 1246 - 1253},
  Volume                   = {50},

  Abstract                 = {A new physics-based model of leakage current suitable for MOS and Flash memory gate oxide is presented in this paper. This model, which assumes the multiphonon trap-assisted tunneling as conduction mechanism, calculates the total leakage current summing the contributions of the percolation paths formed by one or more aligned traps. Spatial positions and energetic levels of traps have been randomly generated within the oxide by a random number generator which has been integrated into the model. Using this model, statistical simulations of leakage currents measured from both MOS and Flash EEPROM memory tunnel oxides have been carried out. In this way, experimental leakage current distributions can be directly reproduced, thus opening a wide range of useful applications in MOS and Flash EEPROM memory reliability prediction.},
  Doi                      = {10.1109/TED.2003.813236},
  ISSN                     = {0018-9383},
  Keywords                 = { EEPROM; MOS memory devices; flash memory devices; leakage currents; multiphonon trap-assisted tunneling model; percolation paths; physics-based model; random number generator; reliability prediction; statistical simulation; tunnel oxides; MOS memory circuits; circuit simulation; flash memories; integrated circuit modelling; integrated circuit reliability; leakage currents; statistical analysis; tunnelling;}
}

@Article{1337175,
  Title                    = {Statistical simulations for flash memory reliability analysis and prediction},
  Author                   = {Larcher, L. and Pavan, P.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {2004},

  Month                    = oct,
  Number                   = {10},
  Pages                    = { 1636 - 1643},
  Volume                   = {51},

  Abstract                 = {In this paper, through the use of a recently proposed statistical model of stress-induced leakage current, we will investigate the reliability of actual flash memory technologies and predict future trends. We investigate either program disturbs (namely gate and drain disturbs) and data retention of state-of-the-art flash memory cells and use this model to correlate the induced threshold voltage shift to the typical outputs coming from oxide characterization, that are density, cross section, and energy level of defects. Physical mechanisms inducing the largest threshold voltage (VT) degradation will be identified and explained. Furthermore, we predict the effects of tunnel oxide scaling on flash memory data retention, giving a rule of thumb to scale the tunnel oxide while maintaining the same retention requirements.},
  Doi                      = {10.1109/TED.2004.835023},
  ISSN                     = {0018-9383},
  Keywords                 = { device simulations; drain disturbs; flash memories; flash memory cells; flash memory data retention; flash memory reliability analysis; flash memory reliability prediction; gate disturbs; oxide characterization; semiconductor device reliability; semiconductor memories; statistical model; statistical simulations; stress-induced leakage current; threshold voltage shift; tunnel oxide scaling; failure analysis; flash memories; integrated circuit modelling; integrated circuit reliability; integrated memory circuits; semiconductor storage; statistical analysis;}
}

@Article{981221,
  Title                    = {A new compact DC model of floating gate memory cells without capacitive coupling coefficients},
  Author                   = {Larcher, L. and Pavan, P. and Pietri, S. and Albani, L. and Marmiroli, A.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {2002},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {301 -307},
  Volume                   = {49},

  Abstract                 = {This paper presents for the first time a new compact SPICE model of floating gate nonvolatile memory cells capable to reproduce effectively the complete DC electrical behavior in every bias conditions. This model features many advantages compared to previous ones: it is simple and easy to implement since it uses SPICE circuit elements, is scalable, and its computational time is not excessive. It is based on a new procedure that calculates the floating gate voltage without using fixed capacitive coupling coefficients, thus improving the floating gate voltage estimate that is fundamental for the correct modeling of cell operations. Moreover, this model requires only the usual parameters adopted for SPICE-like models of MOS transistors plus the floating gate-control gate capacitance, making it very attractive to industry as the same parameter extraction procedure used for MOS transistors can be directly applied. The model we propose has been validated on E2PROM and flash memory cells manufactured in existing technology (0.35 mu;m and 0.25 mu;m) by STMicroelectronics },
  Doi                      = {10.1109/16.981221},
  File                     = {:PDF\\A_New_Compact_DC_Model_of_Floating_Gate_Memory_Cells_Without_Capacitive_Coupling_Coefficients.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {0.25 micron;0.35 micron;DC electrical behavior;E2PROM memory cells;EEPROM;MOS transistors;SPICE circuit elements;SPICE-like models;capacitive coupling coefficients;cell operation modeling;compact DC model;compact SPICE model;computational time;flash memory cells;floating gate memory cells;floating gate nonvolatile memory cells;floating gate voltage;floating gate voltage estimate;floating gate-control gate capacitance;parameter extraction procedure;scalable model;semiconductor device modeling;EPROM;SPICE;circuit simulation;flash memories;integrated circuit modelling;parameter estimation;}
}

@InProceedings{5370894,
  Title                    = {Popularity Based Cache Management Scheme for RAID Which Uses an SSD as a Cache},
  Author                   = {Dongkyu Lee and Kern Koh},
  Booktitle                = {Computer Sciences and Convergence Information Technology, 2009. ICCIT '09. Fourth International Conference on},
  Year                     = {2009},
  Month                    = {nov.},
  Pages                    = {913 -915},

  Abstract                 = {In this paper, we propose a static cache management policy for RAID architecture which uses an SSD as a disk cache. Because SSD has several drawbacks, dynamic cache management scheme such as LRU can cause serious problem i.e., too much write operations can be occurred in order to contain most recently used data. Frequent write operations on SSD not only slow down the performance but also shorten the lifetime. For this reason, we propose a static cache management policy which takes popularity and size into account. Popular data of some period is copied into SSD and too large files are excluded since too large files can lower the hit ratio. We used real trace to evaluate our scheme and the results showed that write counts are reduced to 1/37 of LRU policy as we desired. Unfortunately, the average hit ratio was lower than LRU policy but when popular data is similar between two periods, our scheme showed better hit ratio than LRU. This implies that our scheme can be used on web server which follows zipf's distribution and has smooth changing of popular data.},
  Doi                      = {10.1109/ICCIT.2009.310},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05370894.pdf:PDF},
  Keywords                 = {LRU policy;RAID architecture;SSD;Web server;disk cache;dynamic cache management scheme;popularity based cache management scheme;static cache management policy;zipf's distribution;RAID;cache storage;memory architecture;},
  Timestamp                = {2011.05.20}
}

@Article{PhysRevLett.71.211,
  Title                    = {New Monte Carlo algorithm: Entropic sampling},
  Author                   = {Lee, Jooyoung},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {1993},

  Month                    = {Jul},
  Pages                    = {211--214},
  Volume                   = {71},

  Doi                      = {10.1103/PhysRevLett.71.211},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.71.211.pdf:PDF},
  Issue                    = {2},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.03.09},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevLett.71.211}
}

@InProceedings{1197798,
  Title                    = {Degradation of tunnel oxide by FN current stress and its effects on data retention characteristics of 90 nm NAND flash memory cells},
  Author                   = {Jae-Duk Lee and Jeong-Hyuk Choi and Donggun Park and Kinam Kim},
  Booktitle                = {Reliability Physics Symposium Proceedings, 2003. 41st Annual. 2003 IEEE International},
  Year                     = {2003},
  Month                    = {march-4 april},
  Pages                    = { 497 - 501},

  Doi                      = {10.1109/RELPHY.2003.1197798},
  ISSN                     = { },
  Keywords                 = { 2 Gbit; 90 nm; 90-nm cell transistors; FN current stress; NAND flash memory cells; cell transistor width; data retention characteristics; electron trap sites; failure mechanism; fast traps; hole trap site; interface trap generation; interface trap relaxation; slow traps; tunnel oxide; tunnel oxide degradation;Id-Vg hysteresis curve; MOSFET; NAND circuits; electron traps; failure analysis; flash memories; hole traps; integrated circuit reliability; interface states; tunnelling;}
}

@Article{6520839,
  Title                    = {Improving Performance and Capacity of Flash Storage Devices by Exploiting Heterogeneity of MLC Flash Memory},
  Author                   = {Sungjin Lee and Jihong Kim},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {2445-2458},
  Volume                   = {63},

  Abstract                 = {The multi-level cell (MLC) NAND flash memory technology enables multiple bits of information to be stored in a memory cell, thus making it possible to increase the density of flash memory without increasing the die size. In MLC NAND flash memory, each memory cell can be programmed as a single-level cell or a multi-level cell at runtime because of its performance/capacity asymmetric programming property, which is called flexible programming in this paper. Therefore, MLC flash memory has a potential to achieve the high performance of SLC flash memory while preserving its maximum capacity. In this paper, we present a flexible flash file system, called FlexFS, which takes advantage of flexible programming. FlexFS divides a flash memory medium into SLC and MLC regions, and then dynamically changes two different types of regions to provide an optimal storage solution to end-users in terms of performance and capacity. FlexFS also provides a reasonable storage lifetime by managing the wearing rate of NAND flash memory, which is accelerated by the use of flexible programming. Our implementation of FlexFS in the Linux 2.6 kernel shows that it achieves the I/O performance comparable to SLC flash memory while guaranteeing the capacity of MLC flash memory in various real-world workloads.},
  Doi                      = {10.1109/TC.2013.120},
  File                     = {:PDF\\06520839.pdf:PDF},
  ISSN                     = {0018-9340},
  Keywords                 = {NAND circuits;flash memories;storage management;FlexFS;Linux 2.6 kernel;MLC flash memory;NAND flash memory technology;flash storage device;flexible flash file system;flexible programming;multilevel cell;performance-capacity asymmetric programming;storage lifetime;wearing rate;Ash;Memory management;Microprocessors;Performance evaluation;Programming;Writing;NAND flash memory;file system;operating system;storage system},
  Timestamp                = {2015.06.10}
}

@Article{1475332,
  Title                    = {Fowler-Nordheim tunneling into thermally grown SiO2},
  Author                   = {Lenzlinger, M. and Snow, E.H.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1968},

  Month                    = sep,
  Number                   = {9},
  Pages                    = { 686},
  Volume                   = {15},

  Doi                      = {10.1109/T-ED.1968.16430},
  ISSN                     = {0018-9383}
}

@Article{1576951,
  Title                    = {Efficient encoding of quasi-cyclic low-density parity-check codes},
  Author                   = {Zongwang Li and Lei Chen and Lingqi Zeng and Shu Lin and Fong, W.H.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {Jan.},
  Number                   = {1},
  Pages                    = {71-81},
  Volume                   = {54},

  Abstract                 = {Quasi-cyclic (QC) low-density parity-check (LDPC) codes form an important subclass of LDPC codes. These codes have encoding advantage over other types of LDPC codes. This paper addresses the issue of efficient encoding of QC-LDPC codes. Two methods are presented to find the generator matrices of QC-LDPC codes in systematic-circulant (SC) form from their parity-check matrices, given in circulant form. Based on the SC form of the generator matrix of a QC-LDPC code, various types of encoding circuits using simple shift registers are devised. It is shown that the encoding complexity of a QC-LDPC code is linearly proportional to the number of parity bits of the code for serial encoding, and to the length of the code for high-speed parallel encoding.},
  Doi                      = {10.1109/TCOMM.2005.861667},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01576951.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {matrix algebra;parity check codes;shift registers;LDPC codes;encoding circuits;encoding complexity;generator matrices;high-speed parallel encoding;parity bits;parity-check matrices;quasicyclic low-density parity-check codes;serial encoding;shift registers;systematic-circulant form;Application software;Belief propagation;Circuits;Computer errors;Encoding;Iterative decoding;NASA;Parity check codes;Shift registers;Sparse matrices;Array of circulants;quasi-cyclic (QC) low-density parity-check (LDPC) codes;systematic-circulant (SC) form},
  Timestamp                = {2013.03.14}
}

@InProceedings{1312997,
  Title                    = {Low-density parity-check code constructions for hardware implementation},
  Author                   = {Liao, E. and Engling Yeo and Nikolic, B.},
  Booktitle                = {Communications, 2004 IEEE International Conference on},
  Year                     = {2004},
  Month                    = {june},
  Pages                    = { 2573 - 2577 Vol.5},
  Volume                   = {5},

  Abstract                 = {We present several hardware architectures to implement low-density parity-check (LDPC) decoders for codes constructed with a hierarchical structure. The proposed hierarchical formulation of the LDPC code allows a structured hardware realization of the decoder. For a fully-parallel implementation, there is a reduced routing congestion that allows implementations for blocks sizes up to 1024 bits in 0.13 mu;m technology. Partially and fully serial implementations benefits greatly from the structure of the code as well, leading to several flexible, efficient architectures. In a general purpose 0.13 mu;m technology, the approximate area required by a 1024-bit fully-parallel LDPC decoder is found to be 12.5 mm2 while a serial decoder can be implemented in an area of 0.15 mm2.},
  Doi                      = {10.1109/ICC.2004.1312997},
  File                     = {:PDF\\01312997.pdf:PDF},
  ISSN                     = { },
  Keywords                 = { 0.13 mum; 1024 bit; BER performance; bit error rate; fully-parallel decoder; hardware implementation; low-density parity-check code constructions; routing congestion; serial decoder; sum-product algorithm; decoding; error statistics; parity check codes; telecommunication congestion control; telecommunication network routing;},
  Timestamp                = {2011.04.22}
}

@Article{springerlink:10.1023/A:1018679025763,
  Title                    = {A Comparison Between Broad Histogram and Multicanonical Methods},
  Author                   = {Lima, A. R. and de Oliveira, P. M. C. and Penna, T. J. P.},
  Journal                  = {Journal of Statistical Physics},
  Year                     = {2000},
  Note                     = {10.1023/A:1018679025763},
  Pages                    = {691-705},
  Volume                   = {99},

  Abstract                 = {We discuss the conceptual differences between the broad histogram (BHM) and reweighting methods in general, and particularly the so-called multicanonical (MUCA) approaches. The main difference is that BHM is based on microcanonical, fixed-energy averages which depend only on the good statistics taken inside each energy level. The detailed distribution of visits among different energy levels, determined by the particular dynamic rule one adopts, is irrelevant. Contrary to MUCA, where the results are extracted from the dynamic rule itself, within BHM any microcanonical dynamics could be adopted. As a numerical test, we have used both BHM and MUCA in order to obtain the spectral energy degeneracy of the Ising model in 4×4×4 and 32×32 lattices, for which exact results are known. We discuss why BHM gives more accurate results than MUCA, even using the same Markovian sequence of states. In addition, such an advantage increases for larger systems.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink_10_1023_A_1018679025763.pdf:PDF},
  ISSN                     = {0022-4715},
  Issue                    = {3},
  Keyword                  = {Physics and Astronomy},
  Publisher                = {Springer Netherlands},
  Timestamp                = {2012.03.12},
  Url                      = {http://dx.doi.org/10.1023/A:1018679025763}
}

@Article{s11704-014-3377-2,
  Title                    = {Leach: an automatic learning cache for inline primary deduplication system},
  Author                   = {Lin, Bin and Li, Shanshan and Liao, Xiangke and Zhang, Jing and Liu, Xiaodong},
  Journal                  = {Frontiers of Computer Science},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {175-183},
  Volume                   = {8},

  Doi                      = {10.1007/s11704-014-3377-2},
  File                     = {:PDF\\10.1007s11704-014-3377-2.pdf:PDF},
  ISSN                     = {2095-2228},
  Keywords                 = {deduplication; duplicate detection; splay tree; cache},
  Language                 = {English},
  Publisher                = {Higher Education Press},
  Timestamp                = {2015.07.22},
  Url                      = {http://dx.doi.org/10.1007/s11704-014-3377-2}
}

@TechReport{NASA_NAG5-1278,
  Title                    = {Quasi-Cyclic LDPC Codes},
  Author                   = {Shu Lin},
  Institution              = {Department of Electrical and Computer Engineering, University of California, Davis, CA},

  File                     = {:PDF\\Quasi-Cyclic_LDPC_Codes_NASA_NAG5-1278.pdf:PDF},
  Timestamp                = {2013.04.26}
}

@Article{1055019,
  Title                    = {Multifold Euclidean geometry codes},
  Author                   = { Shu Lin},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1973},

  Month                    = jul,
  Number                   = {4},
  Pages                    = { 537 - 548},
  Volume                   = {19},

  Abstract                 = { This paper presents a class of majority-logic decodable codes whose structure is based on the structural properties of Euclidean geometries (EG) and codes that are invariant under the affine group of permutations. This new class of codes contains the ordinary EG codes and some generalized EG codes as subclasses. One subclass of new codes is particularly interesting: they are the most efficient majority-logic decodable codes that have been constructed.},
  Doi                      = {10.1109/TIT.1973.1055019},
  File                     = {:C\:\\Users\\Public\\Documents\\My eBooks\\refereces\\PDF\\01055019.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Geometry codes; Majority logic decoding;}
}

@Article{1054900,
  Title                    = {On the Number of information symbols in polynomial codes},
  Author                   = { Shu Lin},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1972},

  Month                    = nov,
  Number                   = {6},
  Pages                    = { 785 - 794},
  Volume                   = {18},

  Abstract                 = { Polynomial codes and their dual codes as introduced by Kasami, Lin, and Peterson have considerable algebraic and geometric structure. It has been shown that these codes contain many well-known classes of cyclic codes as subclasses, such as BCH codes, projective geometry codes (PG codes), Euclidean geometry codes (EG codes), and generalized Reed-Muller codes (GRM codes). In this paper, combinatorial expressions for the number of information symbols and parity-check symbols in polynomial codes are derived. The results are applied to two important subclasses of codes, the PG codes and EG codes.},
  Doi                      = {10.1109/TIT.1972.1054900},
  File                     = {:PDF\\01054900.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Polynomial codes;}
}

@Book{Lin:2004:ECC:983680,
  Title                    = {Error Control Coding, Second Edition},
  Author                   = {Lin, Shu and Costello, Daniel J.},
  Publisher                = {Prentice-Hall, Inc.},
  Year                     = {2004},

  Address                  = {Upper Saddle River, NJ, USA},

  ISBN                     = {0130426725}
}

@Book{lincost,
  Title                    = {Error Control Coding: Fundamentals and Applications},
  Author                   = {Shu Lin and Costello, Jr., Daniel J.},
  Publisher                = {Prentice-Hall},
  Year                     = {1983},
  Note                     = {Englewood Clifts, NJ},

  Timestamp                = {2011.04.11}
}

@Article{1459069,
  Title                    = {On the algebraic structure of quasi-cyclic codes III: generator theory},
  Author                   = {Ling, S. and Sole, P.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2005},

  Month                    = july,
  Number                   = {7},
  Pages                    = {2692 -2700},
  Volume                   = {51},

  Abstract                 = {Following Parts I and II, quasi-cyclic codes of given index are studied as codes over a finite polynomial ring. These latter codes are decomposed by the Chinese Remainder Theorem (CRT), or equivalently the Mattson-Solomon transform, into products of shorter codes over larger alphabets. We characterize and enumerate self-dual one-generator quasi-cyclic codes in that context. We give an algorithm to remove some equivalent codes from that enumeration. A generalization to multigenerator codes is sketched.},
  Doi                      = {10.1109/TIT.2005.850142},
  File                     = {:PDF\\01459069.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {CRT;Chinese remainder theorem;DFT;Mattson-Solomon transform;automorphism group;discrete Fourier transform;equivalent code;finite polynomial ring;multigenerator code;quasi-cyclic code;self-dual one-generator;cyclic codes;discrete Fourier transforms;dual codes;transform coding;}
}

@Article{959257,
  Title                    = {On the algebraic structure of quasi-cyclic codes .I. Finite fields },
  Author                   = {San Ling and Sole, P.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = nov,
  Number                   = {7},
  Pages                    = {2751 -2760},
  Volume                   = {47},

  Abstract                 = {A new algebraic approach to quasi-cyclic codes is introduced. The key idea is to regard a quasi-cyclic code over a field as a linear code over an auxiliary ring. By the use of the Chinese remainder theorem (CRT), or of the discrete Fourier transform (DFT), that ring can be decomposed into a direct product of fields. That ring decomposition in turn yields a code construction from codes of lower lengths which turns out to be in some cases the celebrated squaring and cubing constructions and in other cases the (u+ upsi;|u- upsi;) and Vandermonde constructions. All binary extended quadratic residue codes of length a multiple of three are shown to be attainable by the cubing construction. Quinting and septing constructions are introduced. Other results made possible by the ring decomposition are a characterization of self-dual quasi-cyclic codes, and a trace representation that generalizes that of cyclic codes},
  Doi                      = {10.1109/18.959257},
  File                     = {:PDF\\00959257.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Chinese remainder theorem;DFT;Golay codes;Vandermonde construction;algebraic structure;auxiliary ring;binary extended quadratic residue codes;code construction;code length;cubing construction;cyclic codes;discrete Fourier transform;finite fields;linear code;polynomial ring;quasi-cyclic codes;quinting construction;ring decomposition;self-dual quasi-cyclic codes;septing construction;squaring construction;trace representation;Golay codes;binary codes;cyclic codes;discrete Fourier transforms;dual codes;linear codes;residue codes;}
}

@Article{1057320,
  Title                    = {Generalized Reed - Solomon codes from algebraic geometry},
  Author                   = { van Lint, J. and Springer, T.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1987},

  Month                    = may,
  Number                   = {3},
  Pages                    = { 305 - 309},
  Volume                   = {33},

  Abstract                 = {A few years ago Tsfasman {em et al.,} using results from algebraic geometry, showed that there is a sequence of codes which are generalizations of Goppa codes and which exceed the Gilbert-Varshamov bound. We show that a similar sequence of codes (in fact, the duals of the previous codes) can be found by generalizing the construction of Reed-Solomon codes. Our approach has the advantage that it uses less complicated concepts from algebraic geometry.},
  Doi                      = {10.1109/TIT.1987.1057320},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01057320.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Dual coding; Geometry; Goppa coding; Reed-Solomon coding;}
}

@Article{992777,
  Title                    = {On ensembles of low-density parity-check codes: asymptotic distance distributions},
  Author                   = {Litsyn, S. and Shevelev, V.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {887 -908},
  Volume                   = {48},

  Abstract                 = {We derive expressions for the average distance distributions in several ensembles of regular low-density parity-check codes (LDPC). Among these ensembles are the standard one defined by matrices having given column and row sums, ensembles defined by matrices with given column sums or given row sums, and an ensemble defined by bipartite graphs},
  Doi                      = {10.1109/18.992777},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00992777.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {LDPC;asymptotic distance distributions;average distance distributions;binary parity-check matrices;bipartite graphs;column sums;low-density parity-check codes;row sums;error detection codes;graph theory;matrix algebra;},
  Timestamp                = {2011.11.17}
}

@Article{Little1997293,
  Title                    = {On the structure of Hermitian codes},
  Author                   = {John Little and Keith Saints and Chris Heegard},
  Journal                  = {Journal of Pure and Applied Algebra},
  Year                     = {1997},
  Number                   = {3},
  Pages                    = {293 - 314},
  Volume                   = {121},

  Abstract                 = {Let Xm denote the Hermitian curve xm + 1 = ym + y over the field Fm2. Let Q be the single point at infinity, and let D be the sum of the other m3 points of Xm rational over Fm2, each with multiplicity 1. Xm has a cyclic group of automorphisms of order m2 竏・1, which induces automorphisms of each of the the one-point algebraic geometric Goppa codes CL(D, aQ) and their duals. As a result, these codes have the structure of modules over the ring Fq[t], and this structure can be used to good effect in both encoding and decoding. In this paper we examine the algebraic structure of these modules by means of the theory of Groebner bases. We introduce a root diagram for each of these codes (analogous to the set of roots for a cyclic code of length q 竏・1 over Fq), and show how the root diagram may be determined combinatorially from a. We also give a specialized algorithm for computing Groebner bases, adapted to these particular modules. This algorithm has a much lower complexity than general Groebner basis algorithms, and has been successfully implemented in the Maple computer algebra system. This permits the computation of Groebner bases and the construction of compact systematic encoders for some quite large codes (e.g. codes such as CL(D, 4010Q) on the curve X16, with parameters n = 4096, k = 3891).},
  Doi                      = {10.1016/S0022-4049(96)00067-9},
  File                     = {:PDF\\Little1997293.pdf:PDF},
  ISSN                     = {0022-4049},
  Timestamp                = {2012.06.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0022404996000679}
}

@Article{4524251,
  Title                    = {Generalized Combining Method for Design of Quasi-Cyclic LDPC Codes},
  Author                   = {Yuanhua Liu and Xinmei Wang and Ruwei Chen and Yucheng He},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2008},

  Month                    = {may },
  Number                   = {5},
  Pages                    = {392 -394},
  Volume                   = {12},

  Abstract                 = {A generalization of the Chinese Remainder Theorem (CRT) combining method is proposed to design much more and better quasi-cyclic (QC) LDPC codes when the parity check matrices of the component codes are given. It can design a much larger class of QC-LDPC codes with similar performance by loosening the condition for determining the intermediate parameters. By permuting the block rows of the parity check matrices of the component codes, a lot of QC-LDPC codes with much less 6-cycles and better performance can be designed. At a BER of 10-6 some QC-LDPC codes designed by the generalized combining method outperform those designed by the CRT combining method by 0.5 dB.},
  Doi                      = {10.1109/LCOMM.2008.080132},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04524251.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = {BER;QC-low density parity check codes;bit error rate;quasi-cyclic LDPC codes;error statistics;parity check codes;},
  Timestamp                = {2011.12.06}
}

@InProceedings{5076201,
  Title                    = {Design of Quasi-Cyclic LDPC Codes Based on Euclidean Geometries},
  Author                   = {Yuan-Hua Liu and Xin-Mei Wang and Jian-Hua Ma},
  Booktitle                = {Advanced Information Networking and Applications, 2009. AINA '09. International Conference on},
  Year                     = {2009},
  Month                    = may,
  Pages                    = {207 -211},

  Abstract                 = {This paper presents an algebraic method for constructing quasi-cyclic (QC) low-density parity-check (LDPC) codes based on the structural properties of Euclidean geometries. The construction method results in a class of QC-LDPC codes with girth of at least 6. Codes in this class perform very close to the Shannon limit with iterative decoding. Simulations show that the designed QC-LDPC codes have almost the same performance with the existing QC Euclidean geometry LDPC codes.},
  Doi                      = {10.1109/AINA.2009.35},
  File                     = {:\\\\yrlshare.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05076201.pdf:PDF},
  ISSN                     = {1550-445X},
  Keywords                 = {Shannon limit;algebraic method;euclidean geometries;iterative decoding;low density parity check codes;quasi cyclic LDPC codes;cyclic codes;geometric codes;iterative decoding;parity check codes;}
}

@Article{1580676,
  Title                    = {Structured LDPC codes for high-density recording: large girth and low error floor},
  Author                   = {Lu, J. and Moura, J.M.F.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {feb.},
  Number                   = {2},
  Pages                    = { 208 - 213},
  Volume                   = {42},

  Abstract                 = { High-rate low-density parity-check (LDPC) codes are the focus of intense research in magnetic recording because, when decoded by the iterative sum-product algorithm, they show decoding performance close to the Shannon capacity. However, cycles, especially short cycles, are harmful to LDPC codes. The paper describes the partition-and-shift LDPC (PS-LDPC) codes, a new class of regular, structured LDPC codes that can be designed with large girth and arbitrary large minimum distance. Large girth leads to more efficient iterative decoding and codes with better error-floor properties than random LDPC codes. PS-LDPC codes can be designed for any desired column weight and with flexible code rates. The paper details the girth and distance properties of the codes and their systematic construction and presents analytical and simulation performance results that show that, in the high signal-to-noise ratio region, PS-LDPC codes outperform random codes, alleviating the error floor phenomenon.},
  Doi                      = {10.1109/TMAG.2005.861748},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01580676.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = { Shannon capacity; column weight; decoding performance; distance property; error floor phenomenon; error-floor properties; flexible code rates; girth property; high-density recording; iterative codes; iterative decoding; iterative sum-product algorithm; magnetic recording; partition-and-shift LDPC codes; simulation performance; systematic construction; error statistics; iterative decoding; magnetic recording; parity check codes;},
  Timestamp                = {2011.12.06}
}

@Article{PhysRevLett.52.1156,
  Title                    = {Nature of the Spin-Glass Phase},
  Author                   = {M\'ezard, M. and Parisi, G. and Sourlas, N. and Toulouse, G. and Virasoro, M.},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {1984},

  Month                    = {Mar},
  Pages                    = {1156--1159},
  Volume                   = {52},

  Doi                      = {10.1103/PhysRevLett.52.1156},
  File                     = {:PDF\\PhysRevLett.52.1156.pdf:PDF},
  Issue                    = {13},
  Numpages                 = {0},
  Publisher                = {American Physical Society},
  Timestamp                = {2015.02.23},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevLett.52.1156}
}

@Article{748992,
  Title                    = {Good error-correcting codes based on very sparse matrices},
  Author                   = {MacKay, D.J.C.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {399 -431},
  Volume                   = {45},

  Doi                      = {10.1109/18.748992},
  File                     = {:PDF\\Good_Error-Correcting_Codes_Based_on_Very_Sparse_Matrices.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Gallager codes;Gaussian channels;MacKay-Neal codes;Shannon limit;binary-symmetric channel;channel coding;code sequences;concatenated codes;convolutional codes;decoding;error-correcting codes;information rates;optimally decoded codes;performance;sum-product algorithm;symmetric stationary ergodic noise;turbo codes;very sparse matrices;Gaussian channels;channel coding;decoding;error correction codes;noise;sparse matrices;}
}

@Article{585036,
  Title                    = {Near Shannon limit performance of low density parity check codes },
  Author                   = {MacKay, D.J.C. and Neal, R.M.},
  Journal                  = {Electronics Letters},
  Year                     = {1997},

  Month                    = mar,
  Number                   = {6},
  Pages                    = {457 -458},
  Volume                   = {33},

  Abstract                 = {The authors report the empirical performance of Gallager's low density parity check codes on Gaussian channels. They show that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of turbo codes},
  Doi                      = {10.1049/el:19970362},
  File                     = {:PDF\\Near_Shannon_Limit_Performance_of_Low_Density_Parity_Check_Codes.pdf:PDF},
  ISSN                     = {0013-5194},
  Keywords                 = { Gaussian channels; empirical performance; low density parity check codes; near Shannon limit performance; error correction codes;}
}

@TechReport{MERL_TR2001-18,
  Title                    = {A conversation about the Bethe free energy and sum-product (discussion document)},
  Author                   = {David J.C. MacKay and Jonathan S. Yedidia and William T. Freeman and Yair Weiss},
  Institution              = {MERL - Mitsubishi Electric Research Laboratories},
  Year                     = {2001},

  Address                  = {Cambridge, MA 02139},
  Month                    = may,
  Number                   = {TR2001-18},

  File                     = {:PDF\\TR2001-18.pdf:PDF},
  Timestamp                = {2015.01.29},
  Url                      = {http://www.merl.com/publications/TR2001-18/}
}

@InProceedings{Macay-Davey-FFT-QSPA,
  Title                    = {Evaluation of Gallager codes of short block length and high rate applications},
  Author                   = {D. J. C. MacKay and M. C. Davey},
  Booktitle                = {Mathematics its Applications: Codes, Systems Graphical Models},
  Year                     = {2000},

  File                     = {:PDF\\10.1007-978-1-4613-0165-3_6.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.05.12}
}

@Book{2487773,
  Title                    = {The theory of error correcting codes},
  Author                   = {Florence Jessie MacWilliams and Neil James Alexander Sloane},
  Publisher                = {North-Holland},
  Year                     = {1977},

  File                     = {:PDF\\MacWilliams_Sloane_Theory_of_error_correcting_codes.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.05.12}
}

@InProceedings{5372237,
  Title                    = {Error Control Coding for Multilevel Cell Flash Memories Using Nonbinary Low-Density Parity-Check Codes},
  Author                   = {Maeda, Y. and Kaneko, H.},
  Booktitle                = {Defect and Fault Tolerance in VLSI Systems, 2009. DFT '09. 24th IEEE International Symposium on},
  Year                     = {2009},
  Month                    = {oct},
  Pages                    = {367 -375},

  Abstract                 = {Conventional flash memories generally utilize simple error control codes, such as Hamming code and BCH code. In future high-density multilevel cell (MLC) flash memories, however, it is estimated that raw bit error rate (BER) will soar with increasing number of charge levels, and hence the conventional error control coding will not be sufficient for these memories. Low-density parity-check (LDPC) code is a class of strong error control codes which are adopted in practical wired/wireless communication systems, and hence the LDPC code is an important candidate for error control code in future MLC memories. Application of the LDPC code to MLC memory is not so straightforward as conventional error control codes because the LDPC code usually employs soft-input decoding to achieve low decoded BER, and hence analysis of the error probability is crucial, especially when nonbinary codes are applied. Therefore, this paper analyzes error characteristics of MLC flash memory from error control coding viewpoint, and then proposes an error control coding using nonbinary LDPC codes. Evaluation shows that the decoded BER of the nonbinary LDPC code is lower than that of conventional binary irregular LDPC code, and also demonstrates that nonbinary LDPC code defined by a parity-check matrix having average column weight w = 2.5 has lower decoded BER than nonbinary LDPC codes with w = 2 and 3.},
  Doi                      = {10.1109/DFT.2009.25},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05372237.pdf:PDF},
  ISSN                     = {1550-5774},
  Keywords                 = {BCH code;Bose-Chaudhuri-Hocquenghem codes;Hamming code;LDPC code;bit error rate;error control coding;error probability;flash memory;multilevel cell flash memories;nonbinary low-density parity-check codes;parity-check matrix;soft-input decoding;wired/wireless communication systems;BCH codes;Hamming codes;error correction codes;flash memories;parity check codes;},
  Timestamp                = {2012.03.29}
}

@Article{Maharaj2005261,
  Title                    = {Riemann-Roch spaces of the Hermitian function field with applications to algebraic geometry codes and low-discrepancy sequences},
  Author                   = {Hiren Maharaj and Gretchen L. Matthews and Gottlieb Pirsic},
  Journal                  = {Journal of Pure and Applied Algebra},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {261 - 280},
  Volume                   = {195},

  Abstract                 = {This paper is concerned with two applications of bases of Riemann-Roch spaces. In the first application, we define the floor of a divisor and obtain improved bounds on the parameters of algebraic geometry codes. These bounds apply to a larger class of codes than that of Homma and Kim (J. Pure Appl. Algebra 162 (2001) 273). Then we determine explicit bases for large classes of Riemann-Roch spaces of the Hermitian function field. These bases give better estimates on the parameters of a large class of m-point Hermitian codes. In the second application, these bases are used for fast implementation of Xing and Niederreiter's method (Acta. Arith. 72 (1995) 281) for the construction of low-discrepancy sequences.},
  Doi                      = {DOI: 10.1016/j.jpaa.2004.06.010},
  File                     = {:\\\\yrlshare.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Maharaj2005261.pdf:PDF},
  ISSN                     = {0022-4049},
  Url                      = {http://www.sciencedirect.com/science/article/B6V0K-4D0Y5RT-1/2/c7af6d8d01a2c3def3da19a087b2acd4}
}

@Article{762874,
  Title                    = {Detailed observation of small leak current in flash memories with thin tunnel oxides},
  Author                   = {Manabe, Y. and Okuyama, K. and Kubota, K. and Nozoe, A. and Karashima, T. and Ujiie, K. and Kanno, H. and Nakashima, M. and Ajika, N.},
  Journal                  = {Semiconductor Manufacturing, IEEE Transactions on},
  Year                     = {1999},

  Month                    = may,
  Number                   = {2},
  Pages                    = {170 -174},
  Volume                   = {12},

  Abstract                 = {This paper describes a method for measuring the small current through the oxides on the order of 10-20 A or less using a floating gate MOSFET and the application results on flash memories with thin tunnel oxides. The method is based on an accurate measurement of the threshold voltage of a floating gate MOSFET with no charge in the floating gate. We applied this method to flash memories to investigate the leak current behavior through thin tunnel oxides with very small areas ( lt;0.16 mu;m2), and found some anomalous phenomena which cannot be observed from SILC measurements if we use large capacitors. We also discuss possible mechanisms to explain the phenomena },
  Doi                      = {10.1109/66.762874},
  File                     = {:PDF\\Detailed_Observation_of_Small_Leak_Current_in_Flash_Memories_with_Thin_Tunnel_Oxides.pdf:PDF},
  ISSN                     = {0894-6507},
  Keywords                 = {UV irradiation;charge-up;flash memory;floating gate MOSFET;leak current measurement;threshold voltage;tunnel oxide;MOSFET;electric current measurement;flash memories;integrated circuit measurement;leakage currents;tunnelling;}
}

@Article{1255474,
  Title                    = {High-throughput LDPC decoders},
  Author                   = {Mansour, M.M. and Shanbhag, N.R.},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {976-996},
  Volume                   = {11},

  Abstract                 = {A high-throughput memory-efficient decoder architecture for low-density parity-check (LDPC) codes is proposed based on a novel turbo decoding algorithm. The architecture benefits from various optimizations performed at three levels of abstraction in system design-namely LDPC code design, decoding algorithm, and decoder architecture. First, the interconnect complexity problem of current decoder implementations is mitigated by designing architecture-aware LDPC codes having embedded structural regularity features that result in a regular and scalable message-transport network with reduced control overhead. Second, the memory overhead problem in current day decoders is reduced by more than 75% by employing a new turbo decoding algorithm for LDPC codes that removes the multiple checkto-bit message update bottleneck of the current algorithm. A new merged-schedule merge-passing algorithm is also proposed that reduces the memory overhead of the current algorithm for low to moderate-throughput decoders. Moreover, a parallel soft-input-soft-output (SISO) message update mechanism is proposed that implements the recursions of the Balh-Cocke-Jelinek-Raviv (BCJR) algorithm in terms of simple "max-quartet" operations that do not require lookup-tables and incur negligible loss in performance compared to the ideal case. Finally, an efficient programmable architecture coupled with a scalable and dynamic transport network for storing and routing messages is proposed, and a full-decoder architecture is presented. Simulations demonstrate that the proposed architecture attains a throughput of 1.92 Gb/s for a frame length of 2304 bits, and achieves savings of 89.13% and 69.83% in power consumption and silicon area over state-of-the-art, with a reduction of 60.5% in interconnect length.},
  Doi                      = {10.1109/TVLSI.2003.817545},
  File                     = {:PDF\\01255474.pdf:PDF},
  ISSN                     = {1063-8210},
  Keywords                 = {VLSI;algorithm theory;decoding;elemental semiconductors;integrated circuit interconnections;network routing;parity check codes;power consumption;silicon;turbo codes;288 byte;Balh-Cocke-Jelinek-Raviv algorithm;dynamic transport network;interconnect complexity;interconnect length;low-density parity-check decoders;memory overhead;memory-efficient decoder;merged-schedule merge-passing algorithm;multiple check to-bit message;optimizations;power consumption;routing messages;scalable message-transport network;silicon;soft-input soft-output message update mechanism;storing;turbo decoding algorithm;Algorithm design and analysis;Decoding;Design optimization;Energy consumption;Memory architecture;Parity check codes;Performance loss;Routing;Silicon;Throughput},
  Timestamp                = {2015.07.17}
}

@InProceedings{1029622,
  Title                    = {Low-power VLSI decoder architectures for LDPC codes},
  Author                   = {Mansour, M.M. and Shanbhag, N.R.},
  Booktitle                = {Low Power Electronics and Design, 2002. ISLPED '02. Proceedings of the 2002 International Symposium on},
  Year                     = {2002},
  Pages                    = { 284 - 289},

  Doi                      = {10.1109/LPE.2002.146756},
  File                     = {:PDF\\01029622.pdf:PDF},
  Keywords                 = { BCJR algorithm; LDPC codes; algorithmic performance; interconnect-driven code design; iterative decoding; low-density parity check codes; low-power VLSI; message switching activity; message-passing algorithm; power savings; random codes; reduced-complexity parallel decoder architecture; reliability metrics; structural regularity; VLSI; error detection codes; iterative decoding; low-power electronics; message passing; parity check codes; random codes; reliability;},
  Timestamp                = {2011.04.22}
}

@InProceedings{5454106,
  Title                    = {Density evolution-based analysis and design of LDPC codes with a priori information},
  Author                   = {Martalo, M. and Ferrari, G. and Abrardo, A. and Franceschini, M. and Raheli, R.},
  Booktitle                = {Information Theory and Applications Workshop (ITA), 2010},
  Year                     = {2010},
  Month                    = {31 2010-feb. 5},
  Pages                    = {1 -9},

  Abstract                 = {In this paper, we consider multiple access schemes with correlated sources, where a priori information, in terms of source correlation, is available at the access point (AP). In particular, we assume that each source uses a proper low-density parity-check (LDPC) code to transmit, through an additive white Gaussian noise (AWGN) channel, its information sequence to the AP. At the AP, the information sequences are recovered by an iterative decoder, with component decoders associated with the sources, which exploit the available a priori information. In order to analyze the behaviour of the considered multiple access coded system, we propose a density evolution-based approach, which allows to determine a signal-to-noise ratio (SNR) transfer chart and compute the system multi-dimensional SNR feasible region. The proposed technique, besides characterizing the performance of LDPC-coded multiple access scheme, is expedient to design optimized LDPC codes for this application.},
  Doi                      = {10.1109/ITA.2010.5454106},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05454106.pdf:PDF},
  Keywords                 = {LDPC-coded multiple access scheme;access point;additive white Gaussian noise channel;correlated sources;density evolution-based analysis;iterative decoder;low density parity check code;multidimensional SNR feasible region;multiple access coded system;multiple access schemes;optimized LDPC codes;signal-to-noise ratio transfer chart;AWGN channels;decoding;multi-access systems;parity check codes;},
  Timestamp                = {2012.02.29}
}

@Conference{a4ea8d1a1a0a9e726c3ba9737db6ca22,
  Title                    = {Analyst Perspective: Deep Dive into Solid State Storage - Technologies and Architecture},
  Author                   = {Martin, D.},
  Year                     = {2011},
  Month                    = {April},
  Number                   = {Spring},
  Organization             = {Demartek},
  Publisher                = {COMPUTERWORLD},
  Series                   = {Storage Networking World},
  Volume                   = {2011},

  Comment                  = {[読む価値あり]HDDとSSDの性能比較の概要。読んでおいた方がよい。},
  File                     = {:PDF\\a4ea8d1a1a0a9e726c3ba9737db6ca22_Martin_Monday_0845_SNWS11.pdf:PDF},
  Keywords                 = {Solid State Storage Track},
  Memo                     = {Solid State Storage Track},
  Url                      = {https://www.eiseverywhere.com/ehome/SNWS2011/31916/}
}

@Article{ieice_e92_a7_1677-1689,
  Title                    = {A Probabilistic Algorithm for Computing the Weight Distribution of LDPC Codes},
  Author                   = {Masanori HIROTOMO,Masami MOHRI,Masakatu MORII},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2009},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1677-1689},
  Volume                   = {E92-A},

  Abstract                 = {Low-density parity-check (LDPC) codes are linear block codes defined by sparse parity-check matrices. The codes exhibit excellent performance under iterative decoding, and the weight distribution is used to analyze lower error probability of their decoding performance. In this paper, we propose a probabilistic method for computing the weight distribution of LDPC codes. The proposed method efficiently finds low-weight codewords in a given LDPC code by using Stern's algorithm, and stochastically computes the low part of the weight distribution from the frequency of the found codewords. It is based on a relation between the number of codewords with a given weight and the rate of generating the codewords in Stern's algorithm. In the numerical results for LDPC codes of length 504, 1008 and 4896, we could compute the weight distribution by the proposed method with greater accuracy than by conventional methods.},
  File                     = {:PDF\\e92-a_7_1677.pdf:PDF},
  Timestamp                = {2013.02.05}
}

@Article{Massey,
  Title                    = {Shift-Register Synthesis and BCH Decoding},
  Author                   = {J. Massey},
  Journal                  = {IEEE Transactions on Information Theory},
  Year                     = {1969},
  Pages                    = {122-127},
  Volume                   = {15},

  File                     = {:PDF\\Shift-Regiser_Synthesis_and_BCH_Decoding.pdf:PDF},
  Issue                    = {1}
}

@InProceedings{4036370,
  Title                    = {Inverse-Free Implementation of Berlekamp-Massey-Sakata Algorithm for Decoding Codes on Algebraic Curves},
  Author                   = {Matsui, H. and Mita, S.},
  Booktitle                = {Information Theory, 2006 IEEE International Symposium on},
  Year                     = {2006},
  Month                    = {July},
  Pages                    = {2250-2254},

  Abstract                 = {The authors have already proposed small-scale decoder for codes on algebraic curves, which updates decoding data serially and has the same number of calculators for the finite-field computations as the decoders for Reed-Solomon codes except for one calculator for inverse. In this research, we eliminate divisions of the finite field from error-location algorithm, and propose an inverse-free architecture for the error-location of codes on algebraic curves which reduces Kotter's architecture},
  Doi                      = {10.1109/ISIT.2006.261967},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04036370.pdf:PDF},
  Keywords                 = {Reed-Solomon codes;algebraic codes;decoding;Berlekamp-Massey-Sakata algorithm;Reed-Solomon codes;algebraic curves;decoding codes;error-location;error-location algorithm;inverse-free architecture;small-scale decoder;Calculators;Circuits;Clocks;Computer architecture;Costs;Decoding;Equations;Estimation error;Galois fields;Information science},
  Timestamp                = {2014.03.25}
}

@Article{1522645,
  Title                    = {Systolic array architecture implementing Berlekamp-Massey-Sakata algorithm for decoding codes on a class of algebraic curves},
  Author                   = {Matsui, H. and Sakata, S. and Kurihara, M. and Mita, S.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {3856-3871},
  Volume                   = {51},

  Doi                      = {10.1109/TIT.2005.856950},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01522645.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {algebraic codes;decoding;error correction codes;parallel algorithms;systolic arrays;BMS;Berlekamp-Massey-Sakata algorithm;Grobner basis;algebraic curve code;circuit scale;decoding;error-locator polynomial;parallel algorithm;systolic array architecture implementation;Circuits;Equations;Galois fields;Iterative algorithms;Iterative decoding;Logic;Materials science and technology;Polynomials;Systolic arrays;Two dimensional displays;Berlekamp–Massey–Sakata (BMS) algorithm;GrÖbner basis;codes on algebraic curves;parallel decoding;systolic array},
  Timestamp                = {2014.03.25}
}

@Article{6317187,
  Title                    = {Constructions of Rank Modulation Codes},
  Author                   = {Mazumdar, A. and Barg, A. and Zemor, G.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {1018-1029},
  Volume                   = {59},

  Abstract                 = {Rank modulation is a way of encoding information to correct errors in flash memory devices as well as impulse noise in transmission lines. Modeling rank modulation involves construction of packings of the space of permutations equipped with the Kendall tau distance. As our main set of results, we present several general constructions of codes in permutations that cover a broad range of code parameters. In particular, we show a number of ways in which conventional error-correcting codes can be modified to correct errors in the Kendall space. Our constructions are nonasymptotic and afford simple encoding and decoding algorithms of essentially the same complexity as required to correct errors in the Hamming metric. As an example, from binary Bose-Chaudhuri-Hocquenghem codes, we obtain codes correcting t Kendall errors in n memory cells that support the order of n!/(log2n!)t messages, for any constant t=1,2,.... We give many examples of rank modulation codes with specific parameters. Turning to asymptotic analysis, we construct families of rank modulation codes that correct a number of errors that grows with n at varying rates, from Θ(n) to Θ(n2). One of our constructions gives rise to a family of rank modulation codes for which the tradeoff between the number of messages and the number of correctable Kendall errors approaches the optimal scaling rate.},
  Doi                      = {10.1109/TIT.2012.2221121},
  File                     = {:PDF\\06317187.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Hamming codes;decoding;error correction codes;modulation coding;Bose-Chaudhuri-Hocquenghem code;Hamming metric;Kendall tau distance;asymptotic analysis;decoding algorithm;encoding information;error-correcting code;flash memory devices;impulse noise;optimal scaling rate;parameter coding;rank modulation code;transmission line;Decoding;Encoding;Measurement;Modulation;Polynomials;Tin;Vectors;Codes in permutations;Gray map;Kendall tau distance;flash memory;rank modulation;transpositions},
  Timestamp                = {2015.03.31}
}

@Article{McEliece:1981:SSR:358746.358762,
  Title                    = {On Sharing Secrets and Reed-Solomon Codes},
  Author                   = {McEliece, R. J. and Sarwate, D. V.},
  Journal                  = {Commun. ACM},
  Year                     = {1981},

  Month                    = sep,
  Number                   = {9},
  Pages                    = {583--584},
  Volume                   = {24},

  Acmid                    = {358762},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/358746.358762},
  File                     = {:PDF\\p583-mceliece.pdf:PDF},
  ISSN                     = {0001-0782},
  Issue_date               = {Sept. 1981},
  Keywords                 = {Reed-Solomon codes, cryptography, interpolation, key management, privacy, secret sharing systems},
  Numpages                 = {2},
  Publisher                = {ACM},
  Timestamp                = {2015.06.11},
  Url                      = {http://doi.acm.org/10.1145/358746.358762}
}

@Article{5437429,
  Title                    = {On the Hardness of Approximating Stopping and Trapping Sets},
  Author                   = {McGregor, A. and Milenkovic, O.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {1640-1650},
  Volume                   = {56},

  Abstract                 = {We prove that approximating the size of stopping and trapping sets in Tanner graphs of linear block codes, and more restrictively, the class of low-density parity-check (LDPC) codes, is NP-hard. The ramifications of our findings are that methods used for estimating the height of the error-floor of moderate- and long-length LDPC codes, based on stopping and trapping set enumeration, cannot provide accurate worst-case performance predictions for most codes.},
  Doi                      = {10.1109/TIT.2010.2040941},
  ISSN                     = {0018-9448},
  Keywords                 = {block codes;computational complexity;graph theory;linear codes;parity check codes;LDPC codes;NP-hard;Tanner graphs;linear block codes;stopping sets;trapping sets;AWGN;Bit error rate;Block codes;Iterative algorithms;Iterative decoding;Maximum likelihood decoding;Maximum likelihood estimation;Message passing;Parity check codes;Performance loss;Low-density parity-check (LDPC) codes;NP hardness;stopping sets;trapping sets},
  Timestamp                = {2014.10.19}
}

@Article{MERKX1984,
  Title                    = {Womcodes constructed with projective geometries},
  Author                   = {MERKX, Frans},
  Journal                  = {Traitement Du Signal},
  Year                     = {1984},
  Number                   = {2-2},
  Pages                    = {227–231},
  Volume                   = {1},

  Abstract                 = {We consider storage media which consist of a number of write-once bit positions (wits) . A wit initially contains
 a "0", that may be irreversibly overwritten with a "1".
 It was shown by Rivest and Shamir [5] that, by coding techniques one can reuse such a write-once memor y
 (worn) up to a very high rate . We present two new cyclic womcodes, based on PG (2,2) and PG (3,2) respectively ,
 which attain the RS-bound. These codes can be decoded with a decoding algorithm for Hamming codes . Some
 other high-rate womcodes, derived from those above, are discussed .},
  File                     = {:PDF\\Womcodes_constructed_with_projective_geometries.pdf:PDF},
  Keywords                 = {Error-correcting codes; finite geometry; numerical storage media},
  Owner                    = {jason},
  Timestamp                = {2015.06.08},
  Url                      = {http://hdl.handle.net/2042/1590}
}

@Article{10.1051:jphys:01984004505084300,
  Title                    = {Replica symmetry breaking and the nature of the spin glass phase},
  Author                   = {{Mezard, M.} and {Parisi, G.} and {Sourlas, N.} and {Toulouse, G.} and {Virasoro, M.}},
  Journal                  = {J. Phys. France},
  Year                     = {1984},
  Number                   = {5},
  Pages                    = {843-854},
  Volume                   = {45},

  Doi                      = {10.1051/jphys:01984004505084300},
  File                     = {:PDF\\ajp-jphys_1984_45_5_843_0.pdf:PDF},
  Timestamp                = {2015.02.23},
  Url                      = {http://dx.doi.org/10.1051/jphys:01984004505084300}
}

@Book{ECCforNVRAM,
  Title                    = {Error Correction Codes for Non-Volatile Memories},
  Author                   = {R. Micheloni and A. Marelli and R. Ravasio},
  Publisher                = {Springer},
  Year                     = {2008},
  Month                    = {June}
}

@InProceedings{4558857,
  Title                    = {Bit error rate in NAND Flash memories},
  Author                   = {Mielke, N. and Marquart, T. and Ning Wu and Kessenich, J. and Belgal, H. and Schares, E. and Trivedi, F. and Goodness, E. and Nevill, L.R.},
  Booktitle                = {Reliability Physics Symposium, 2008. IRPS 2008. IEEE International},
  Year                     = {2008},
  Month                    = {27 2008-may 1},
  Pages                    = {9 -19},

  Abstract                 = {NAND flash memories have bit errors that are corrected by error-correction codes (ECC). We present raw error data from multi-level-cell devices from four manufacturers, identify the root-cause mechanisms, and estimate the resulting uncorrectable bit error rates (UBER). Write, retention, and read-disturb errors all contribute. Accurately estimating the UBER requires care in characterization to include all write errors, which are highly erratic, and guardbanding for variation in raw bit error rate. NAND UBER values can be much better than 10-15, but UBER is a strong function of program/erase cycling and subsequent retention time, so UBER specifications must be coupled with maximum specifications for these quantities.},
  Doi                      = {10.1109/RELPHY.2008.4558857},
  File                     = {:PDF\\04558857.pdf:PDF},
  Keywords                 = {NAND flash memories;error-correction codes;multilevel-cell devices;read-disturb errors;root-cause mechanisms;uncorrectable bit error rates;write errors;NAND circuits;error correction codes;error statistics;flash memories;},
  Timestamp                = {2011.04.22}
}

@Article{959254,
  Title                    = {Bounds on the maximum-likelihood decoding error probability of low-density parity-check codes},
  Author                   = {Miller, G. and Burshtein, D.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {nov},
  Number                   = {7},
  Pages                    = {2696 -2710},
  Volume                   = {47},

  Abstract                 = {We derive both upper and lower bounds on the decoding error probability of maximum-likelihood (ML) decoded low-density parity-check (LDPC) codes. The results hold for any binary-input symmetric-output channel. Our results indicate that for various appropriately chosen ensembles of LDPC codes, reliable communication is possible up to channel capacity. However, the ensemble averaged decoding error probability decreases polynomially, and not exponentially. The lower and upper bounds coincide asymptotically, thus showing the tightness of the bounds. However, for ensembles with suitably chosen parameters, the error probability of almost all codes is exponentially decreasing, with an error exponent that can be set arbitrarily close to the standard random coding exponent},
  Doi                      = {10.1109/18.959254},
  File                     = {:PDF\\00959254.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Gallager's bound;LDPC codes;average spectrum;binary-input symmetric-output channel;channel capacity;discrete memoryless channel;ensemble averaged decoding error probability;error exponent;linear codes;low-density parity-check codes;lower bounds;maximum-likelihood decoding error probability;random coding exponent;reliable communication;upper bounds;channel capacity;error detection codes;error statistics;linear codes;maximum likelihood decoding;memoryless systems;},
  Timestamp                = {2011.11.15}
}

@Article{5467625,
  Title                    = {A Study of Write Margin of Spin Torque Transfer Magnetic Random Access Memory Technology},
  Author                   = {Tai Min and Qiang Chen and Beach, R. and Jan, G. and Cheng Horng and Kula, W. and Torng, T. and Tong, R. and Zhong, T. and Tang, D. and Pokang Wang and Mao-min Chen and Sun, J.Z. and Debrosse, J.K. and Worledge, D.C. and Maffitt, T.M. and Gallagher, W.J.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {june },
  Number                   = {6},
  Pages                    = {2322 -2327},
  Volume                   = {46},

  Abstract                 = {Key design parameters of 64 Mb STT-MRAM at 90-nm technology node are discussed. A design point was developed with adequate TMR for fast read operation, enough energy barrier for data retention and against read disturbs, a write voltage satisfying the long term reliability against dielectric breakdown and a write bit error rate below 10-9. A direct experimental method was developed to determine the data retention lifetime that avoids the discrepancy in the energy barrier values obtained with spin current- and field-driven switching measurements. Other parameters detrimental to write margins such as backhopping and the existence of a low breakdown population are discussed. At low bit-error regime, new phenomenon emerges, suggestive of a bifurcation of switching modes. The dependence of the bifurcated switching threshold on write pulse width, operating temperature, junction dimensions and external field were studied. These show bifurcated switching to be strongly influenced by thermal fluctuation related to the spatially inhomogeneous free layer magnetization. An external field along easy axis direction assisting switching was shown to be effective for significantly reducing the percentage of MTJs showing bifurcated switching.},
  Doi                      = {10.1109/TMAG.2010.2043069},
  File                     = {:PDF\\05467625.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = {MTJ;STT-MRAM;TMR;bifurcated switching threshold;bit error regime;dielectric breakdown;magnetic random access memory technology;magnetic tunneling junction;magnetization;nanotechnology node;reliability;size 90 nm;spin torque transfer;storage capacity 64 Mbit;thermal fluctuation;tunneling magnetoresistance;write margin;write pulse width;MRAM devices;electric breakdown;nanotechnology;reliability;tunnelling magnetoresistance;},
  Timestamp                = {2011.06.03}
}

@Article{817571,
  Title                    = {Soft breakdown conduction in ultrathin (3-5 nm) gate dielectrics },
  Author                   = {Miranda, E. and Sune, J. and Rodriguez, R. and Nafria, M. and Aymerich, X. and Fonseca, L. and Campabadal, F.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {2000},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {82 -89},
  Volume                   = {47},

  Abstract                 = {Prior to any attempt to model a charge transport mechanism, a precise knowledge of the parameters on which the current depends is essential. In this work, the soft breakdown (SBD) failure mode of ultrathin (3-5 nm) SiO2 layers in polysilicon-oxide-semiconductor structures is investigated. This conduction regime is characterized by a large leakage current and by multilevel current fluctuations, both at low applied voltages. In order to obtain a general picture of SBD, room-temperature current-voltage (I-V) measurements have been performed on samples with different gate areas, oxide thicknesses and substrate types. An astounding matching between some of these I-V characteristics has been found. The obtained results and the comparison with the final breakdown regime suggest that the current flow through a SBD spot is largely influenced by its atomic-scale dimensions as occurs in a point contact configuration. Experimental data are also presented which demonstrate that specific current fluctuations can be ascribed to a blocking behavior of unstable SBD conduction channels},
  Doi                      = {10.1109/16.817571},
  File                     = {:PDF\\Soft_Breakdown_Conduction_in_Ultrathin(3-5nm)_Gate_Dielectrics.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {3 to 5 nm;I-V characteristics;SiO2;atomic-scale dimensions;breakdown regime;charge transport mechanism;current fluctuations;failure mode;gate areas;leakage current;multilevel current fluctuations;oxide thicknesses;point contact configuration;polysilicon-oxide-semiconductor structures;room-temperature current-voltage measurements;soft breakdown conduction;ultrathin gate dielectrics;MIS devices;dielectric thin films;failure analysis;leakage currents;semiconductor device breakdown;semiconductor device reliability;silicon compounds;}
}

@Article{1519184,
  Title                    = {An effective error correction using a combination of algebraic geometric codes and Parity codes for HDD},
  Author                   = {Mita, S. and Matsui, H.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {oct},
  Number                   = {10},
  Pages                    = { 2992 - 2994},
  Volume                   = {41},

  Abstract                 = {This paper describes the performance of an efficient error-correcting system for hard disk drives. The performance of the codes on algebraic curves, such as Hermitian codes over GF(28), elliptic codes over GF(29), and Fermat codes over GF(210) is compared with that of conventional Reed-Solomon (RS) codes. In particular, an adoption of Hermitian codes can reduce the redundant part by approximately 800 bits more than the RS codes when an error-correcting capability of 240 bytes is adopted for a long sector size. Moreover, we propose an error-correcting system based on a combination of algebraic geometric codes and parity codes. This combination system can cover a bit-error rate of approximately 10-2 under a condition of EEPR4 channel and additive Gaussian noise.},
  Doi                      = {10.1109/TMAG.2005.854451},
  File                     = {:PDF\\01519184.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = { Bahl-Cocke-Jelinek-Raviv algorithm; EEPR4 channel; Fermat codes; Hermitian codes; Reed-Solomon codes; additive Gaussian noise; algebraic curves; algebraic geometric codes; belief propagation algorithm; bit-error rate; elliptic codes; error correction; hard disk drives; magnetic recording channels; parity codes; Reed-Solomon codes; algebraic geometric codes; error correction codes; error statistics; hard discs; parity check codes;},
  Timestamp                = {2011.06.14}
}

@InProceedings{4339706,
  Title                    = {A novel SPRAM (SPin-transfer torque RAM) with a synthetic ferrimagnetic free layer for higher immunity to read disturbance and reducing write-current dispersion},
  Author                   = {Miura, K. and Kawahara, T. and Takemura, R. and Hayakawa, J. and Ikeda, S. and Sasaki, R. and Takahashi, H. and Matsuoka, H. and Ohno, H.},
  Booktitle                = {VLSI Technology, 2007 IEEE Symposium on},
  Year                     = {2007},
  Month                    = {june},
  Pages                    = {234 -235},

  Abstract                 = {A novel SPRAM (spin-transfer torque RAM) consisting of MgO-barrier-based magnetic tunnel junctions (MTJs) with a synthetic ferrimagnetic (SyF) structure in a free layer was demonstrated for both higher immunity to read disturbance and a sufficient margin between the read and write currents. Since magnetization of the free layer becomes stable against thermal fluctuation with increasing thermal-stability factor E/kBT, the SyF free layer of the MTJs realized a magnetic information retention of over 10 years due to its high E/kBT of 67. Furthermore, it was found that the SyF free layer has an advantage of reducing dispersion of write-current density Jc, which is necessary for securing an adequate margin between the read and write currents.},
  Doi                      = {10.1109/VLSIT.2007.4339706},
  File                     = {:PDF\\04339706.pdf:PDF},
  Keywords                 = {MgO;SPRAM;magnetic information retention;magnetic tunnel junctions;read disturbance;spin-transfer torque RAM;synthetic ferrimagnetic free layer;thermal fluctuation;thermal-stability;write-current density;write-current dispersion;ferrimagnetic materials;magnetic tunnelling;magnetisation;random-access storage;},
  Timestamp                = {2011.06.03}
}

@InProceedings{1309947,
  Title                    = {Advanced flash memory reliability},
  Author                   = {Modelli, A. and Visconti, A. and Bez, R.},
  Booktitle                = {Integrated Circuit Design and Technology, 2004. ICICDT '04. International Conference on},
  Year                     = {2004},
  Pages                    = { 211 - 218},

  Abstract                 = { With reference to the mainstream technology, the most relevant failure mechanisms that affect reliability of Flash memory are reviewed, showing the primary role played by tunnel oxide defects. The degradation of device. performance induced by program/erase cycling is discussed, specifically for what concern the leakage that affect a very small fraction of memory cells after cycling. The dependence of the leakage on tunnel oxide thickness, number of cycles, and temperature is analyzed. The leakage current is explained by trap-assisted tunneling involving one, two or more traps, with decreasing occurrence probability. Finally, data are presented showing the robustness of scaled Flash memory to alpha particles and electromagnetic radiation.},
  Doi                      = {10.1109/ICICDT.2004.1309947},
  File                     = {:PDF\\Advanced_Flash_Memory_Reliability.pdf:PDF},
  ISSN                     = { },
  Keywords                 = { Fowler-Nordheim tunneling; advanced flash memory reliability; alpha particles robustness; channel hot-electron injection; data retention; device. performance degradation; electromagnetic radiation robustness; electron-hole pairs; failure mechanisms; leakage current; memory endurance; number of cycles; program-erase cycling; single floating-gate transistor; temperature dependence; trap-assisted tunneling; tunnel oxide defects; tunnel oxide thickness; flash memories; hot carriers; integrated circuit reliability; leakage currents; radiation hardening (electronics); tunnelling;}
}

@InProceedings{Monagan:2006:RSM:1145768.1145809,
  Title                    = {Rational simplification modulo a polynomial ideal},
  Author                   = {Monagan, Michael and Pearce, Roman},
  Booktitle                = {Proceedings of the 2006 international symposium on Symbolic and algebraic computation},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {239--245},
  Publisher                = {ACM},
  Series                   = {ISSAC '06},

  Acmid                    = {1145809},
  Doi                      = {10.1145/1145768.1145809},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\p239-monagan.pdf:PDF},
  ISBN                     = {1-59593-276-3},
  Keywords                 = {Groebner Bases, field of fractions, quotient ring, rational expression, side relations, simplification},
  Location                 = {Genoa, Italy},
  Numpages                 = {7},
  Timestamp                = {2012.05.16},
  Url                      = {http://doi.acm.org/10.1145/1145768.1145809}
}

@Book{1088886,
  Title                    = {Error Correction Coding: Mathematical Methods and Algorithms},
  Author                   = {Moon, Todd K.},
  Publisher                = {Wiley-Interscience},
  Year                     = {2005},

  ISBN                     = {0471648000}
}

@InProceedings{6167471,
  Title                    = {Robust decoder architecture for multi-level flash memory storage channels},
  Author                   = {Motwani, R. and Chong Ong},
  Booktitle                = {Computing, Networking and Communications (ICNC), 2012 International Conference on},
  Year                     = {2012},
  Month                    = {30 2012-feb. 2},
  Pages                    = {492 -496},

  Abstract                 = {Multi-level-cell (MLC) flash memory comprises of cells which can be programmed to multiple levels. Recent MLC flash memory systems support 3 bits per cell to 4 bits per cell which means that the individual cells are programmed to 8 or 16 distinct levels respectively. MLC flash has higher raw bit error rate (rber) than the single bit per cell flash. This calls for the use of sophisticated error control coding (ECC) schemes like LDPC codes. The flash memory channel is usually modeled with level distributions having Gaussian probability density function. However, the Gaussian distribution is not a good fit for practical NAND channels. Even for the beginning of life of the flash memory device, this model has to be replaced by a more realistic model. With erases and re-writes, the channel towards the end of life of the device is far from Gaussian and floating-gate to floating-gate coupling and charge loss not only increases the raw bit error rate but causes the level distributions to become asymmetric. Given such channel impairments, if the LDPC decoder assumes Gaussian level distributions, its performance can degrade considerably. Hence, modifications are required in the decoding algorithm to cope up with the channel distortions. In order to keep the hardware costs within limits, simple schemes with minimal hardware increase are desirable to keep up the performance. In this paper, the flash channel degradations are first enlisted and then simple solutions are proposed which keep the performance in check as the flash memory transits from a channel with moderate impairments to the end of life condition, where the level distributions for the different levels are highly asymmetric.},
  Doi                      = {10.1109/ICCNC.2012.6167471},
  File                     = {:PDF\\06167471.pdf:PDF},
  Keywords                 = {Gaussian level distributions;Gaussian probability density function;LDPC decoder;MLC flash memory systems;NAND channels;bit error rate;channel distortions;charge loss;error control coding schemes;flash channel degradations;flash memory channel transition;floating-gate coupling device;individual cells;multilevel cell flash memory storage channels;robust decoder architecture;Gaussian distribution;channel coding;error statistics;flash memories;parity check codes;},
  Timestamp                = {2012.10.22}
}

@Article{PhysRevE.62.1577,
  Title                    = {Statistical physics of regular low-density parity-check error-correcting codes},
  Author                   = {Murayama, Tatsuto and Kabashima, Yoshiyuki and Saad, David and Vicente, Renato},
  Journal                  = {Phys. Rev. E},
  Year                     = {2000},

  Month                    = {Aug},
  Pages                    = {1577--1591},
  Volume                   = {62},

  Doi                      = {10.1103/PhysRevE.62.1577},
  File                     = {:PDF\\0003121.pdf:PDF},
  Issue                    = {2},
  Numpages                 = {0},
  Publisher                = {American Physical Society},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevE.62.1577}
}

@Article{1506715,
  Title                    = {A combining method of quasi-cyclic LDPC codes by the Chinese remainder theorem},
  Author                   = {Seho Myung and Kyeongcheol Yang},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2005},

  Month                    = {sep},
  Number                   = {9},
  Pages                    = { 823 - 825},
  Volume                   = {9},

  Abstract                 = { In this paper we propose a method of constructing quasi-cyclic low-density parity-check (QC-LDPC) codes of large length by combining QC-LDPC codes of small length as their component codes, via the Chinese remainder theorem. The girth of the QC-LDPC codes obtained by the proposed method is always larger than or equal to that of each component code. By applying the method to array codes, we present a family of high-rate regular QC-LDPC codes with no 4-cycles. Simulation results show that they have almost the same performance as random regular LDPC codes.},
  Doi                      = {10.1109/LCOMM.2005.1506715},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01506715.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = { Chinese remainder theorem; QC-LDPC; array codes; component code; quasi-cyclic low-density parity-check code; random code; cyclic codes; parity check codes; random codes;},
  Timestamp                = {2011.12.06}
}

@Article{1468309,
  Title                    = {Quasi-cyclic LDPC codes for fast encoding},
  Author                   = {Myung, S. and Yang, K. and Kim, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2005},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {2894 -2901},
  Volume                   = {51},

  Abstract                 = {In this correspondence we present a special class of quasi-cyclic low-density parity-check (QC-LDPC) codes, called block-type LDPC (B-LDPC) codes, which have an efficient encoding algorithm due to the simple structure of their parity-check matrices. Since the parity-check matrix of a QC-LDPC code consists of circulant permutation matrices or the zero matrix, the required memory for storing it can be significantly reduced, as compared with randomly constructed LDPC codes. We show that the girth of a QC-LDPC code is upper-bounded by a certain number which is determined by the positions of circulant permutation matrices. The B-LDPC codes are constructed as irregular QC-LDPC codes with parity-check matrices of an almost lower triangular form so that they have an efficient encoding algorithm, good noise threshold, and low error floor. Their encoding complexity is linearly scaled regardless of the size of circulant permutation matrices.},
  Doi                      = {10.1109/TIT.2005.851753},
  File                     = {:PDF\\Quasi-Cyclic_LDPC_Codes_for_Fast_Encoding.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {QC-LDPC;block-type LDPC code;circulant permutation matrix;encoding algorithm;quasicyclic low-density parity-check code;AWGN channels;block codes;channel coding;cyclic codes;matrix algebra;parity check codes;}
}

@Article{5257335,
  Title                    = {A Study of LDPC Coding and Iterative Decoding System in Magnetic Recording System Using Bit-Patterned Medium With Write Error},
  Author                   = {Nakamura, Y. and Okamoto, Y. and Osawa, H. and Aoi, H. and Muraoka, H.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2009},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {3753 -3756},
  Volume                   = {45},

  Abstract                 = {In this paper, low-density parity-check (LDPC) coding and iterative decoding system is studied in the magnetic recording system using a bit-patterned medium (BPM). The magnetic recording system using BPM has a serious problem of requiring precise write synchronization to correctly write the recording sequence on each intended island. In this paper, an error locator using the parity-check matrix of LDPC code is utilized to curb the influence of write error on the iterative decoding process. The performance of the proposed system with the error locator is evaluated by computer simulation for magnetic recording system using BPM with synchronization write error at a recording density of 2 Tb/in2, and it is compared with a conventional Reed-Solomon (RS) encoding and decoding system. The results show that the proposed system provides better performance.},
  Doi                      = {10.1109/TMAG.2009.2022331},
  File                     = {:PDF\\A_Study_of_LDPC_Coding_and_Iterative_Decoding_System_in_Magnetic_Recording_System_Using_Bit-Patterned_Medium_with_Write_Error.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = {LDPC coding;bit-patterned medium;iterative decoding system;low-density parity-check coding;magnetic recording system;recording sequence;write error;write synchronization;error statistics;iterative decoding;magnetic recording;parity check codes;synchronisation;}
}

@InProceedings{5474157,
  Title                    = {A Hybrid Flash Memory SSD Scheme for Enterprise Database Applications},
  Author                   = {Byung-Woo Nam and Gap-Joo Na and Sang-Won Lee},
  Booktitle                = {Web Conference (APWEB), 2010 12th International Asia-Pacific},
  Year                     = {2010},
  Month                    = {april},
  Pages                    = {39 -44},

  Abstract                 = {Flash memory has many advantages such as high performance, low electronic power, non-volatile storage and physical stability, over hard-disks. For this reason, flash memory has been deployed as data storage for mobile devices, including PDAs, MP3 players, laptop-computers and database systems. According to the cell type, flash memory can be divided into SLC(Single Level Chip) and MLC(Multi Level Chip). In general, SLC is known to have high performance and longer lifetime (i.e. more than 100 K wear-leveling) while MLC is to offer larger capacity and with low price but have wear leveling of not longer than 10 K. In this paper, we show that it is possible to design a fast and cost-efficient storage by combining two types of flash memories in a hybrid fashion. Specifically, we propose a hybrid flash memory solid state disk(SSD) scheme using FAST FTL for enterprise applications, where SLC chip is used as the log space for FAST while MLC chips store the normal data blocks. SLC chips allow fast and durable performance for write while MLC chips provide the large capacity. And, this is mainly due to the FAST FTL algorithm's characteristics: it tends to direct the random writes to SLC chips and direct the other most random read to MLC chips. By taking the advantages of both chip types, we can find an economically desirable flash SSD design option. Experimental results show that our hybrid flash SSD scheme outperforms MLC-only flash scheme by far both in terms of performance and price.},
  Doi                      = {10.1109/APWeb.2010.70},
  File                     = {:PDF\\05474157.pdf:PDF},
  Keywords                 = {data storage;enterprise database applications;flash memory solid state disk;flash translation layer;hybrid flash memory SSD scheme;multilevel chip;nonvolatile storage;physical stability;single level chip;wear leveling;business data processing;database management systems;flash memories;},
  Timestamp                = {2011.05.20}
}

@Book{neubauer2007,
  Title                    = {Coding Theory: Algorithms, Architectures and Applications},
  Author                   = {Andre Neubauer and Jurgen Freudenberger and Volker Kuhn},
  Publisher                = {Wiley \& Sons, Ltd.},
  Year                     = {2007},

  File                     = {:PDF\\Coding Theory - Algorithms, Architectures, and Applications [Andre Neubauer et al.] 2007.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.01.08}
}

@Article{springerlink:10.1023/A:1004614130865,
  Title                    = {Error Estimation in the Histogram Monte Carlo Method},
  Author                   = {Newman, M.E.J. and Palmer, R.G.},
  Journal                  = {Journal of Statistical Physics},
  Year                     = {1999},
  Note                     = {10.1023/A:1004614130865},
  Pages                    = {1011-1026},
  Volume                   = {97},

  Abstract                 = {We examine the sources of error in the histogram reweighting method for Monte Carlo data analysis. We demonstrate that, in addition to the standard statistical error which has been studied elsewhere, there are two other sources of error, one arising through correlations in the reweighted samples, and one arising from the finite range of energies sampled by a simulation of finite length. We demonstrate that while the former correction is usually negligible by comparison with statistical fluctuations, the latter may not be, and give criteria for judging the range of validity of histogram extrapolations based on the size of this latter correction.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink_10_1023_A_100461413865.pdf:PDF},
  ISSN                     = {0022-4715},
  Issue                    = {5},
  Keyword                  = {Physics and Astronomy},
  Publisher                = {Springer Netherlands},
  Timestamp                = {2012.03.12},
  Url                      = {http://dx.doi.org/10.1023/A:1004614130865}
}

@InProceedings{1264306,
  Title                    = {Tightening union bound by applying Verdu theorem for LDPC},
  Author                   = {Vu-Duc Ngo and Sin-Chong Park},
  Booktitle                = {Personal, Indoor and Mobile Radio Communications, 2003. PIMRC 2003. 14th IEEE Proceedings on},
  Year                     = {2003},
  Month                    = {Sept},
  Pages                    = {420-423 Vol.1},
  Volume                   = {1},

  Abstract                 = {A tight bound on the bit error probability for low density parity check codes is derived by eliminating the useless codewords out of the union bound. The elimination of these codewords is done by applying Verdu's theorem and calculating tin Hamming weight of the codewords that belong to the irreducible set. This approach is simpler than other bound techniques in the sense of the complexity of computation and it also extends the reliable region of Eb/N0 in which the bound yields meaningful results.},
  Doi                      = {10.1109/PIMRC.2003.1264306},
  File                     = {:PDF\\01264306.pdf:PDF},
  Keywords                 = {Hamming codes;computational complexity;error statistics;parity check codes;LDPC;Verdu theorem;bit error probability;codewords Hamming weight;computational complexity;distance spectrum;irreducible set;low density parity check codes;union bound;Bit error rate;Block codes;Cities and towns;Error probability;Hamming weight;Information rates;Parity check codes;Sparse matrices;Turbo codes;Upper bound},
  Timestamp                = {2014.11.10}
}

@Article{6169192,
  Title                    = {On the Construction of Structured LDPC Codes Free of Small Trapping Sets},
  Author                   = {Nguyen, D.V. and Chilappagari, S.K. and Marcellin, M.W. and Vasic, B.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {2280-2302},
  Volume                   = {58},

  Abstract                 = {We present a method to construct low-density parity-check (LDPC) codes with low error floors on the binary symmetric channel. Codes are constructed so that their Tanner graphs are free of certain small trapping sets. These trapping sets are selected from the trapping set ontology for the Gallager A/B decoder. They are selected based on their relative harmfulness for a given decoding algorithm. We evaluate the relative harmfulness of different trapping sets for the sum-product algorithm by using the topological relations among them and by analyzing the decoding failures on one trapping set in the presence or absence of other trapping sets. We apply this method to construct structured LDPC codes. To facilitate the discussion, we give a new description of structured LDPC codes whose parity-check matrices are arrays of permutation matrices. This description uses Latin squares to define a set of permutation matrices that have disjoint support and to derive a simple necessary and sufficient condition for the Tanner graph of a code to be free of four cycles.},
  Doi                      = {10.1109/TIT.2011.2173733},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06169192.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {binary codes;decoding;matrix algebra;parity check codes;Gallager A-B decoder;Latin squares;Tanner graphs;binary symmetric channel;decoding failures;low error floors;low-density parity-check codes;parity-check matrices;permutation matrices;small trapping sets;structured LDPC code;sum-product algorithm;trapping set ontology;Algorithm design and analysis;Charge carrier processes;Databases;Decoding;Iterative decoding;Merging;Error floor;Latin squares;structured low-density parity-check codes;trapping sets},
  Timestamp                = {2013.11.08}
}

@Book{nishimori_spinglass,
  Title                    = {Statistical Physics of Spin Glasses and Information Processing: An Introduction},
  Author                   = {Hidetoshi Nishimori},
  Publisher                = {Oxford Univ Pr on Demand},
  Year                     = {2001},
  Month                    = {September},

  Abstract                 = {Spin glasses are magnetic materials. Statistical mechanics, a subfield of physics, has been a powerful tool to theoretically analyse various unique properties of spin glasses. A number of new analytical techniques have been developed to establish a theory of spin glasses. Surprisingly, these techniques have turned out to offer new tools and viewpoints for the understanding of information processing problems, including neural networks, error-correcting codes, image restoration, and optimization problems. This book is one of the first publications of the past ten years that provide a broad overview of this interdisciplinary field. Most of the book is written in a self-contained manner, assuming only a general knowledge of statistical mechanics and basic probability theory. It provides the reader with a sound introduction to the field and to the analytical techniques necessary to follow its most recent developments.},
  File                     = {:PDF\\Statistical physics of spin glasses and information processing an introduction - Nishimori H..pdf:PDF},
  ISBN                     = {978-0198509417},
  Timestamp                = {2012.03.16},
  Totalpages               = {243}
}

@Article{doi:10.1143/JPSJ.62.2973,
  Title                    = {Optimum Decoding Temperature for Error-Correcting Codes},
  Author                   = {Nishimori, Hidetoshi},
  Journal                  = {Journal of the Physical Society of Japan},
  Year                     = {1993},
  Number                   = {9},
  Pages                    = {2973-2975},
  Volume                   = {62},

  Doi                      = {10.1143/JPSJ.62.2973},
  Eprint                   = { 
 http://dx.doi.org/10.1143/JPSJ.62.2973
 
},
  File                     = {:PDF\\jpsjE62E2973.pdf:PDF},
  Timestamp                = {2015.02.03},
  Url                      = { 
 http://dx.doi.org/10.1143/JPSJ.62.2973
 
}
}

@Article{1580807,
  Title                    = {Algebraic construction of sparse matrices with large girth},
  Author                   = {O'Sullivan, M.E.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {feb. },
  Number                   = {2},
  Pages                    = {718 -727},
  Volume                   = {52},

  Abstract                 = {In this correspondence, we present a method for constructing sparse matrices that have a compact description and whose associated bipartite graphs have large girth. Based on an arbitrary seed matrix of nonnegative integers a new matrix is constructed which replaces each entry of the seed matrix with a sum of permutation matrices. Algebraic conditions that lead to short cycles in the associated bipartite graph are analyzed and methods to achieve large girth in two special cases are presented. In one, all the permutation matrices are circulants; in the other they are all affine permutation matrices. When used to define a low-density parity-check (LDPC) code the compact description should lead to efficient implementation and the large girth to good error correction performance. The method is adaptable to a variety of rates, and a variety of row and column degrees},
  Doi                      = {10.1109/TIT.2005.862120},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01580807.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {LDPC;algebraic construction;arbitrary seed matrix;associated bipartite graph;circulant permutation matrix;error correction performance;large girth;low-density parity-check code;nonnegative integer;sparse matrix;error correction codes;graph theory;parity check codes;sparse matrices;},
  Timestamp                = {2011.12.06}
}

@Article{4860408,
  Title                    = {Neutral electron trap generation in SiO2 by hot holes},
  Author                   = {Ogawa, Shigeo and Shiono, Noboru and Shimaya, Masakazu},
  Journal                  = {Applied Physics Letters},
  Year                     = {1990},

  Month                    = apr,
  Number                   = {14},
  Pages                    = {1329 -1331},
  Volume                   = {56},

  Abstract                 = {Experimental evidence for neutral electron trap generation in SiO2 caused by injected holes is presented. The neutral electron traps are detected by Fowler #x2013;Nordheim (FN) tunneling electron injection after avalanche hole injection. The density of generated neutral traps increases with the number of injected holes, but does not saturate with that of the trapped holes. The centroid of generated neutral traps is found to be in the middle of the oxide. These results suggest that neutral traps are generated by the holes and not only by the recombination of electrons with trapped holes. The origin of neutral traps is considered to be associated with dipolar defects formed by SiO bond breaking under hole transport in the oxide.},
  Doi                      = {10.1063/1.103200},
  File                     = {:PDF\\APL_Vol56_P1329.pdf:PDF},
  ISSN                     = {0003-6951}
}

@Article{Ogita05accuratesum,
  Title                    = {Accurate Sum and Dot Product},
  Author                   = {Takeshi Ogita and Siegfried M. Rump and Shin'ichi Oishi},
  Journal                  = {SIAM J. Sci. Comput},
  Year                     = {2005},
  Pages                    = {2005},
  Volume                   = {26},

  Abstract                 = {Algorithms for summation and dot product of floating point numbers are presented which are fast in terms of measured computing time. We show that the computed results are as accurate as if computed in twice or K-fold working precision, K 3. For twice the working precision our algorithms for summation and dot product are some 40 % faster than the corresponding XBLAS routines while sharing similar error estimates. Our algorithms are widely applicable because they require only addition, subtraction and multiplication of floating point numbers in the same working precision as the given data. Higher precision is unnecessary, algorithms are straight loops without branch, and no access to mantissa or exponent is necessary.},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Ogita05accuratesum.pdf:PDF},
  Timestamp                = {2011.08.27}
}

@Article{4806138,
  Title                    = {Low-Complexity Switch Network for Reconfigurable LDPC Decoders},
  Author                   = {Daesun Oh and Parhi, K.K.},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {85-94},
  Volume                   = {18},

  Abstract                 = {In this paper, we propose an efficient low-complexity switch network design for reconfigurable low-density parity-check (LDPC) decoders. The proposed architecture leads to significant reductions in hardware complexity. Since the structured quasi-cyclic (QC) LDPC codes for most modern wireless communication systems include multiple code rates, various block lengths, and different sizes of submatrices, a reconfigurable LDPC decoder is desirable and the barrel shifter needs to be programmable. The Benes network cannot be optimized as the barrel shifter for a reconfigurable LDPC decoder when the input size of barrel shifter is not a power of 2. Also, it is not trivial to generate all the control signals on-the-fly for numerous 2 ?? 2 switches in the switch network. In this paper, a novel low-complexity switch network design is proposed, which can be used efficiently when the input size of barrel shifters is not a power of 2. Furthermore, we propose a novel algorithm to generate all the control signals, which can be implemented with a small size of lookup table (LUT) or a simple combination logic on-the-fly, using the properties that both the full-size switch network can be broken into two half-size switch networks and the barrel shifters for the structured QC LDPC decoders require only cyclic shifts. Compared with conventional Benes networks using a dedicated LUT or a complicated signal generating algorithm, the proposed architectures achieve significant hardware reductions in implementing the barrel shifters for reconfigurable LDPC decoders. In synthesis result using the TSMC 0.18-??m standard cell CMOS technology, the proposed switch network for a reconfigurable LDPC decoder of IEEE 802.16e and IEEE 802.11n can be implemented with an area of 0.772 mm2, which leads to a significant area reduction.},
  Doi                      = {10.1109/TVLSI.2008.2007736},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04806138.pdf:PDF},
  ISSN                     = {1063-8210},
  Keywords                 = {communication complexity;cyclic codes;parity check codes;table lookup;telecommunication switching;Benes network;IEEE 802.11n;IEEE 802.16e;TSMC 0.18-??m standard cell CMOS technology;barrel shifter;combination logic on-the-fly;complicated signal generating algorithm;hardware complexity;lookup table;low-complexity switch network;modern wireless communication systems;reconfigurable low-density parity-check decoders;size 0.18 mum;structured quasi-cyclic low-density parity-check decoders codes;Barrel shifter;Benes network;VLSI;low-density parity-check (LDPC) decoder;reconfigurable;switch network},
  Timestamp                = {2013.05.28}
}

@InProceedings{4253023,
  Title                    = {Efficient Highly-Parallel Decoder Architecture for Quasi-Cyclic Low-Density Parity-Check Codes},
  Author                   = {Daesun Oh and Parhi, K.K.},
  Booktitle                = {Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on},
  Year                     = {2007},
  Month                    = may,
  Pages                    = {1855 -1858},

  Abstract                 = {In this paper, the authors propose an efficient highly-parallel decoder architecture using partially overlapped decoding scheme for quasi-cyclic (QC) low-density parity-check (LDPC) codes, which leads to reduction in hardware complexity and power consumption. Generally, due to the regularly structured parity-check matrix H of QC LDPC codes, the message updating computations in the check node unit (CNU) and the variable node unit (VNU) can be efficiently overlapped, which increases the decoding throughput by maximizing the hardware utilization efficiency (HUE). However, the partially overlapped decoding scheme cannot be used to design a highly-parallel decoding architecture for high-throughput applications. For (3, 5)-regular QC LDPC codes, our proposed method could reduce the hardware complexity by approximately 33% for the CNU and 20% for the VNU in the highly-parallel decoder architecture without any performance degradation. In addition, the power consumption can be minimized by reducing the total number of memory accesses for updated messages.},
  Doi                      = {10.1109/ISCAS.2007.378276},
  File                     = {:PDF\\Efficient_Highly-Parallel_Decoder_Architecture_for_Quasi-Cyclic_Low-Density_Parity_Check_Codes.pdf:PDF},
  Keywords                 = {CNU;HUE;LDPC;VNU;check node unit;hardware utilization efficiency;highly-parallel decoder;low-density parity-check codes;quasi-cyclic codes;variable node unit;cyclic codes;decoding;parity check codes;}
}

@Article{PhysRev.65.117,
  Title                    = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition},
  Author                   = {Onsager, Lars},
  Journal                  = {Phys. Rev.},
  Year                     = {1944},

  Month                    = {Feb},
  Pages                    = {117--149},
  Volume                   = {65},

  Doi                      = {10.1103/PhysRev.65.117},
  File                     = {:PDF\\PhysRev.65.117.pdf:PDF},
  Issue                    = {3-4},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.08.21},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRev.65.117}
}

@InProceedings{5746286,
  Title                    = {A 4Mb conductive-bridge resistive memory with 2.3GB/s read-throughput and 216MB/s program-throughput},
  Author                   = {Otsuka, W. and Miyata, K. and Kitagawa, M. and Tsutsui, K. and Tsushima, T. and Yoshihara, H. and Namise, T. and Terao, Y. and Ogata, K.},
  Booktitle                = {Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 2011 IEEE International},
  Year                     = {2011},
  Month                    = {feb.},
  Pages                    = {210 -211},

  Abstract                 = {The growing demand for higher performance in the storage and access of data in various consumer electronic and computing devices has driven the development of nonvolatile memory (NVM) technologies. The promising candidates for future NVM such as FeRAM and PCM have demonstrated shorter access time, faster programming and wide read/write bandwidth in the chip and the memory macro. Resistive memory (ReRAM) is also one of alternative NVMs, because of its low operating voltage, high speed and good scalability. Several types of ReRAM characteristics have been investigated on memory array. However, most are limited in terms of memory array performance because of not having suitable read/write circuit for ReRAM. In this work, we present a 4Mb conductive bridge ReRAM test macro realizing 2.3GB/S read-throughput, 216MB/S program-throughput and robust reliability results by using read/write fully functional device technology with direct sense in programming (DSIP) method.},
  Doi                      = {10.1109/ISSCC.2011.5746286},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05746286.pdf:PDF},
  ISSN                     = {0193-6530},
  Keywords                 = {computing device;conductive bridge ReRAM test macro;conductive-bridge resistive memory;consumer electronic;memory array performance;memory macro;nonvolatile memory technology;program-throughput;read-throughput;read/write circuit;read/write fully functional device technology;robust reliability;storage capacity 4 Mbit;memory architecture;random-access storage;},
  Timestamp                = {2011.08.24}
}

@Article{796391,
  Title                    = {Constructing codes from algebraic curves},
  Author                   = {Ozbudak, F. and Stichtenoth, H.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1999},

  Month                    = nov,
  Number                   = {7},
  Pages                    = {2502 -2505},
  Volume                   = {45},

  Abstract                 = {We discuss some previous constructions of codes from algebraic curves due to Xing, Niederreiter and Lam (see ibid., vol.45, no.7, p.2498-2501, 1999), and we investigate their relations with Goppa's (1981) algebraic-geometric codes},
  Doi                      = {10.1109/18.796391},
  File                     = {:PDF\\00796391.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Goppa algebraic-geometric codes;algebraic curves;algebraic function fields;code construction;linear codes;Goppa codes;algebraic geometric codes;linear codes;}
}

@Article{springerlink:10.1007/s00200-007-0048-7,
  Title                    = {Gr\"{o}bner basis approach to list decoding of algebraic geometry codes},
  Author                   = {O’Keeffe, Henry and Fitzpatrick, Patrick},
  Journal                  = {Applicable Algebra in Engineering, Communication and Computing},
  Year                     = {2007},
  Note                     = {10.1007/s00200-007-0048-7},
  Pages                    = {445-466},
  Volume                   = {18},

  Abstract                 = {We show how our Gröbner basis algorithm, which was previously applied to list decoding of Reed Solomon codes, can be used in the hard and soft decision list decoding of Algebraic Geometry codes. In addition, we present a linear functional version of our Gröbner basis algorithm in order to facilitate comparisons with methods based on duality.},
  Affiliation              = {National University of Ireland Boole Centre for Research in Informatics Cork Ireland},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\springerlink_10.1007_s00200-007-0048-7.pdf:PDF},
  ISSN                     = {0938-1279},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Berlin / Heidelberg},
  Timestamp                = {2012.05.15},
  Url                      = {http://dx.doi.org/10.1007/s00200-007-0048-7}
}

@InProceedings{613165,
  Title                    = {Optimized arithmetic for Reed-Solomon encoders},
  Author                   = {Paar, C.},
  Year                     = {1997},
  Month                    = {jun.},
  Pages                    = {250},

  Doi                      = {10.1109/ISIT.1997.613165},
  File                     = {:PDF\\Optimized_arithmetic_for_Reed-Solomon_encoders.pdf:PDF},
  Journal                  = {Information Theory. 1997. Proceedings., 1997 IEEE International Symposium on},
  Keywords                 = {Galois fields;Reed-Solomon encoders;arithmetic operation;low complexity constant multipliers;multiplication;optimization algorithms;optimized arithmetic;Galois fields;Reed-Solomon codes;arithmetic codes;computational complexity;matrix multiplication;optimisation;}
}

@InProceedings{6125398,
  Title                    = {An FPGA implementation architecture for decoding of polar codes},
  Author                   = {Pamuk, A},
  Booktitle                = {Wireless Communication Systems (ISWCS), 2011 8th International Symposium on},
  Year                     = {2011},
  Month                    = {Nov},
  Pages                    = {437-441},

  Abstract                 = {Polar codes are a class of codes versatile enough to achieve the Shannon bound in a large array of source and channel coding problems. For that reason it is important to have efficient implementation architectures for polar codes in hardware. Motivated by this fact we propose a belief propagation (BP) decoder architecture for an increasingly popular hardware platform; Field Programmable Gate Array (FPGA). The proposed architecture supports any code rate and is quite flexible in terms of hardware complexity and throughput. The architecture can also be extended to support multiple block lengths without increasing the hardware complexity a lot. Moreover various schedulers can be adapted into the proposed architecture so that list decoding techniques can be used with a single block. Finally the proposed architecture is compared with a convolutional turbo code (CTC) decoder for WiMAX taken from a Xilinx Product Specification and seen that polar codes are superior to CTC codes both in hardware complexity and throughput.},
  Doi                      = {10.1109/ISWCS.2011.6125398},
  File                     = {:PDF\\06125398.pdf:PDF},
  ISSN                     = {2154-0217},
  Keywords                 = {block codes;channel coding;decoding;field programmable gate arrays;information theory;scheduling;source coding;BP decoder architecture;CTC decoder;FPGA implementation architecture;Shannon bound;WiMAX;Xilinx product specification;belief propagation decoder architecture;channel coding problem;convolutional turbo code decoder;decoding;field programmable gate array;hardware complexity;multiple block length;polar code architecture;popular hardware platform;source array;Channel coding;Complexity theory;Decoding;Field programmable gate arrays;Hardware;Throughput;FPGA;Polar codes;belief propagation decoding;bp decoder;hardware implementation},
  Timestamp                = {2014.08.26}
}

@InProceedings{5873231,
  Title                    = {Drift-Tolerant Multilevel Phase-Change Memory},
  Author                   = {Papandreou, N. and Pozidis, H. and Mittelholzer, T. and Close, G.F. and Breitwisch, M. and Lam, C. and Eleftheriou, E.},
  Booktitle                = {Memory Workshop (IMW), 2011 3rd IEEE International},
  Year                     = {2011},
  Month                    = {may},
  Pages                    = {1 -4},

  Abstract                 = {Multilevel-cell (MLC) storage is a typical way for achieving increased capacity and thus lower cost-per-bit in memory technologies. In phase-change memory (PCM), however, MLC storage is seriously hampered by the phenomenon of resistance drift. Reference cells may be used to offer some relief, however their effectiveness is limited due to the stochastic nature of drift. In this paper, an alternative way to cope with drift in PCM is introduced, based on modulation coding. The new drift tolerant coding technique encodes information in the relative order of resistance levels in a codeword. Experimental results from a 90-nm PCM prototype chip demonstrate the effectiveness of the proposed method in offering high resilience to drift. Most notably, 4 levels/cell storage with raw bit-error-rates in the order of 10-5 is achieved in a 200 kcell array and maintained for over 30 days after programming at room temperature.},
  Doi                      = {10.1109/IMW.2011.5873231},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05873231.pdf:PDF},
  Keywords                 = {MLC storage;bit-error-rates;drift tolerant coding technique;drift-tolerant multilevel phase-change memory;modulation coding;multilevel-cell storage;reference cells;resistance drift;room temperature;size 90 nm;temperature 293 K to 298 K;error statistics;modulation coding;phase change memories;},
  Timestamp                = {2011.07.09}
}

@Article{0305-4470-13-3-042,
  Title                    = {The order parameter for spin glasses: a function on the interval 0-1},
  Author                   = {G Parisi},
  Journal                  = {Journal of Physics A: Mathematical and General},
  Year                     = {1980},
  Number                   = {3},
  Pages                    = {1101},
  Volume                   = {13},

  Abstract                 = {The breaking of the replica symmetry in spin glasses is studied. It is found that the order parameter is a function on the interval 0-1. This approach is used to study the Sherrington-Kirkpatrick model. Exact results are obtained near the critical temperature. Approximated results at all the temperatures are in excellent agreement with the computer simulations at zero external magnetic field.},
  Timestamp                = {2015.03.04},
  Url                      = {http://stacks.iop.org/0305-4470/13/i=3/a=042}
}

@Article{jphyslet:019800041015036100,
  Title                    = {A Simple hypothesis for the spin glass phase of the Infinite-ranged SK model},
  Author                   = {{Parisi, G.} and {Toulouse, G.}},
  Journal                  = {J. Physique Lett.},
  Year                     = {1980},
  Number                   = {15},
  Pages                    = {361-364},
  Volume                   = {41},

  Doi                      = {10.1051/jphyslet:019800041015036100},
  File                     = {:PDF\\ajp-jphyslet_1980_41_15_361_0.pdf:PDF},
  Timestamp                = {2015.02.23},
  Url                      = {http://dx.doi.org/10.1051/jphyslet:019800041015036100}
}

@InProceedings{5341269,
  Title                    = {Reliability and performance enhancement technique for SSD array storage system using RAID mechanism},
  Author                   = {Kwanghee Park and Dong-Hwan Lee and Youngjoo Woo and Geunhyung Lee and Ju-Hong Lee and Deok-Hwan Kim},
  Booktitle                = {Communications and Information Technology, 2009. ISCIT 2009. 9th International Symposium on},
  Year                     = {2009},
  Month                    = {sept.},
  Pages                    = {140 -145},

  Abstract                 = {Recently solid state drive (SSD) based on NAND flash memory chips becomes popular in the consumer electronics market because it is tough on shock and its I/O performance is better than that of conventional hard disk drive. However, as the density of the semiconductor grows higher, the distance between its wires narrows down, their interferences are frequently occurred, and the bit error rate of semiconductor increases. Such frequent error occurrence and short life cycle in NAND flash memory reduce the reliability of SSD. In this paper, we present reliability and performance enhancement technique on new RAID system based on SSD. First, we analyze the existing RAID mechanism in the environment of SSD array and then develop a new RAID methodology adaptable to SSD array storage system. Via trace-driven simulation, we evaluated the performance of our new optimized SSD array storage using RAID mechanism. The proposed method enhances the reliability of SSD array 2% higher than that of existing RAID system and improves the I/O performance of SSD array 28% higher than that of existing RAID system.},
  Doi                      = {10.1109/ISCIT.2009.5341269},
  File                     = {:PDF\\05341269.pdf:PDF},
  Keywords                 = {I/O performance;NAND flash memory chip;RAID mechanism;SSD array storage system reliability;bit error rate;consumer electronics market;solid state drive;trace-driven simulation;RAID;disc drives;error statistics;flash memories;storage management chips;},
  Timestamp                = {2011.05.19}
}

@InProceedings{4773759,
  Title                    = {Shuffled BP decoding for punctured LDPC codes},
  Author                   = {Sangjoon Park and Sunyoung Lee and Keumchan Whang},
  Booktitle                = {Communications, 2008. APCC 2008. 14th Asia-Pacific Conference on},
  Year                     = {2008},
  Month                    = {Oct},
  Pages                    = {1-5},

  Abstract                 = {In this paper, we propose a scheduling method of the shuffled BP decoding algorithm for punctured LDPC codes. The number of iteration required to recover whole punctured variable nodes in the standard BP decoding algorithm is increased with the number of punctured variable nodes. However, only one number of iteration is required in the proposed shuffled BP scheduling regardless of the puncturing algorithm and the number of punctured variable nodes. Furthermore, the proposed shuffled BP scheduling can improve the reliability of the unpunctured variable nodes at the 1st iteration, while impossible in the layered BP scheduling if there are no check nodes that only unpunctured variable nodes participate. Simulation results verify that the proposed shuffled BP scheduling has comparable performance to the layered BP scheduling in spite of its smaller number of scheduling group at rate below 0.9.},
  File                     = {:PDF\\04773759.pdf:PDF},
  Keywords                 = {iterative decoding;parity check codes;belief propagation;low density parity check codes;punctured LDPC codes;punctured variable nodes;self-recovery;shuffled BP decoding;AWGN;Binary phase shift keying;Code standards;Convergence;Iterative algorithms;Iterative decoding;Parity check codes;Scheduling algorithm;Turbo codes;Wireless communication;LDPC(Low Density Parity Check) Codes;Puncturing;SR(Step-Recovery);Shuffled BP(Belief Propagation) Decoding},
  Timestamp                = {2015.05.22}
}

@InProceedings{5564941,
  Title                    = {Hash join in commercial database with flash memory SSD},
  Author                   = {Sang-Shin Park and Sang-Won Lee},
  Booktitle                = {Computer Science and Information Technology (ICCSIT), 2010 3rd IEEE International Conference on},
  Year                     = {2010},
  Month                    = {july},
  Pages                    = {265 -268},
  Volume                   = {4},

  Abstract                 = {Hash join is one of important operations in database system, and its performance may slow down because of disk I/O in hash table overflow phenomenon. In that phenomenon, the more overflow of hash table occurs, the more disk I/O arise, so join performance go from bad to worse. Dominant disk I/O patterns of hash join are sequential writes and random reads, then flash memory SSD is in a more advantageous position than magnetic disk on those I/O patterns. Therefore, using a flash memory SSD instead of magnetic disk as a temporary storage of database will prevent existing performance degradation. In this paper, we show calculated costs by query optimizer in some test cases first. And then, we also show empirically hash join performance in these test cases on temporary table space created on magnetic disk and flash memory SSD. Consequently, the average response time with flash memory SSD was about nine times, minimum and twenty times, max faster than that with magnetic disk. And you might encounter problems that estimated cost by query optimizer and real performance do not correspond on magnetic disk. But, that problem was solved on flash memory SSD.},
  Doi                      = {10.1109/ICCSIT.2010.5564941},
  File                     = {:PDF\\05564941.pdf:PDF},
  Keywords                 = {commercial database system;disk I/O pattern;flash memory SSD;hash join;hash table overflow phenomenon;magnetic disk;query optimizer;random read;sequential write;flash memories;magnetic disc storage;},
  Timestamp                = {2011.05.19}
}

@Article{678579,
  Title                    = {Degradation of thin tunnel gate oxide under constant Fowler-Nordheim current stress for a flash EEPROM},
  Author                   = {Young-Bog Park and Schroder, D.K.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {1361 -1368},
  Volume                   = {45},

  Abstract                 = {The degradation of thin tunnel gate oxide under constant Fowler-Nordheim (FN) current stress was studied using flash EEPROM structures. The degradation is a strong function of the amount of injected charge density (Qinj), oxide thickness, and the direction of stress. Positive charge trapping is usually dominant at low Qinj followed by negative charge trapping at high Qinj , causing a turnaround of gate voltage and threshold voltage. Interface trap generation continues to increase with increasing stress, as evidenced by subthreshold slope and transconductance. Gate injection stress creates more positive charge traps and interface traps than does substrate injection stress. Oxide degradation gets more severe for thicker oxide, due to more oxide charge trapping and interface trap generation by impact ionization. A simple model of oxide degradation and breakdown was established based on the experimental results. It indicates that the damage in the oxide is more serious near the anode interface by impact ionization and oxide breakdown is also closely related to surface roughness at the cathode interface. When all the damage sites in the oxide connect and a conductive path between cathode and anode is formed, oxide breakdown occurs. The damage is more serious for thicker oxide because a thicker oxide is more susceptible to impact ionization},
  Doi                      = {10.1109/16.678579},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00678579.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {constant Fowler-Nordheim current stress;damage sites;flash EEPROM;gate injection stress;impact ionization;injected charge density;negative charge trapping;oxide degradation;oxide thickness;positive charge trapping;substrate injection stress;subthreshold slope;surface roughness;transconductance;tunnel gate oxide;EPROM;MOS memory circuits;electric breakdown;electron traps;hole traps;impact ionisation;insulating thin films;integrated circuit reliability;internal stresses;leakage currents;surface topography;}
}

@Book{pearl2014probabilistic,
  Title                    = {Probabilistic reasoning in intelligent systems: networks of plausible inference},
  Author                   = {Pearl, Judea},
  Publisher                = {Morgan Kaufmann},
  Year                     = {2014},

  Timestamp                = {2015.07.30}
}

@Article{45279,
  Title                    = {On a decoding algorithm for codes on maximal curves},
  Author                   = {Pellikaan, R.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1989},

  Month                    = {nov},
  Number                   = {6},
  Pages                    = {1228 -1232},
  Volume                   = {35},

  Abstract                 = {A decoding algorithm for algebraic geometric codes that was given by A.N. Skorobogatov and S.G. Vladut (preprint, Inst. Problems of Information Transmission, 1988) is considered. The author gives a modified algorithm, with improved performance, which he obtains by applying the above algorithm a number of times in parallel. He proves the existence of the decoding algorithm on maximal curves by showing the existence of certain divisors. However, he has so far been unable to give an efficient procedure of finding these divisors},
  Doi                      = {10.1109/18.45279},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00045279.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {algebraic geometric codes;decoding algorithm;divisors;maximal curves;codes;decoding;},
  Timestamp                = {2011.11.10}
}

@Book{book_peterson_weldon,
  Title                    = {ERROR-CORRECTING CODES Second Edition},
  Author                   = {W. W. Petersen and E. J. Weldon, Jr.},
  Publisher                = {THE MIT PRESS},
  Year                     = {1972},

  Timestamp                = {2011.11.11}
}

@Article{petersonanderson1987,
  Title                    = {A Mean Field Theory Learning Algorithm for Neural Networks},
  Author                   = {Carsten Peterson and James R. Anderson},
  Journal                  = {Complex Systems},
  Year                     = {1987},
  Number                   = {5},
  Pages                    = {995-1019},
  Volume                   = {1},

  File                     = {:PDF\\complex_systems01-5-6.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.30}
}

@Article{Piyas19991677,
  Title                    = {Calculation of the probability of hole injection from polysilicon gate into silicon dioxide in MOS structures under high-field stress},
  Author                   = {Piyas and Samanta},
  Journal                  = {Solid-State Electronics},
  Year                     = {1999},
  Number                   = {9},
  Pages                    = {1677 - 1687},
  Volume                   = {43},

  Abstract                 = {A theoretical study of the mechanism for hole injection from the degenerately doped poly-Si gate into the gate oxide in metal-oxide Silicon (MOS) structures under high-field stress (5-15 MV/cm) is presented. Our theoretical model for the anode hole injection mechanism (AHI) is due to the injection of holes generated by impact ionization of energetic electrons entering into the conduction band of poly-Si gate from the oxide conduction band under the applied electric field. An analytical approach is proposed on the basis of the above injection mechanism to estimate the hole injection probability per injected electron in order to utilize these data in MOS device simulation under high-field Fowler-Nordheim (FN) stress. The computed values of anode hole injection probability data are compared with experimental results. Using the anode hole injection probability from the present model, we have predicted thin tunnel gate oxide degradation under constant current and voltage FN stress.},
  Doi                      = {10.1016/S0038-1101(99)00144-6},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Piyas19991677.pdf:PDF},
  ISSN                     = {0038-1101},
  Timestamp                = {2011.09.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0038110199001446}
}

@Article{S0025-5718-1971-0301966-0,
  Title                    = {The fast Fourier transform in a finite field},
  Author                   = {J. M. Pollard},
  Journal                  = {Mathematics of Computation},
  Year                     = {1971},

  File                     = {:PDF\\S0025-5718-1971-0301966-0.pdf:PDF},
  Timestamp                = {2015.05.15},
  Url                      = {http://www.ams.org/journals/mcom/1971-25-114/S0025-5718-1971-0301966-0/}
}

@Article{4641893,
  Title                    = {Design of regular (2,d/sub c/)-LDPC codes over GF(q) using their binary images},
  Author                   = {Poulliat, C. and Fossorier, M. and Declercq, D.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {October},
  Number                   = {10},
  Pages                    = {1626-1635},
  Volume                   = {56},

  Abstract                 = {In this paper, a method to design regular (2, dc)- LDPC codes over GF(q) with both good waterfall and error floor properties is presented, based on the algebraic properties of their binary image. First, the algebraic properties of rows of the parity check matrix H associated with a code are characterized and optimized to improve the waterfall. Then the algebraic properties of cycles and stopping sets associated with the underlying Tanner graph are studied and linked to the global binary minimum distance of the code. Finally, simulations are presented to illustrate the excellent performance of the designed codes.},
  Doi                      = {10.1109/TCOMM.2008.060527},
  File                     = {:PDF\\04641893.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {binary codes;channel coding;graph theory;image coding;parity check codes;Tanner graph;binary images;parity check matrix;regular (2,dc)-LDPC codes;Belief propagation;Binary codes;Design methodology;Error correction codes;Hamming weight;Image coding;Iterative algorithms;Iterative decoding;Optimization methods;Parity check codes;Channel coding;binary image;error correction coding;iterative decoding;non-binary LDPC codes},
  Timestamp                = {2015.04.21}
}

@InProceedings{7063416,
  Title                    = {AXaaS: Case for acceleration as a service},
  Author                   = {Powers, N. and Ailing, A. and Gyampoh-Vidogah, R. and Soyata, T.},
  Booktitle                = {Globecom Workshops (GC Wkshps), 2014},
  Year                     = {2014},
  Month                    = {Dec},
  Pages                    = {117-121},

  Abstract                 = {The ubiquity and the range of utility of "smart" devices is ever increasing. Device limitations have lead developers to leverage cloud-offloading to gain performance for their applications. As users become aware of the expanding utility of their devices through these powerful applications, they tend to demand more from them. However, developers' intent on providing state-of-the-art applications will undoubtedly hit performance barriers for emerging products due to the inherently high latency of the prevailing mobile-cloud architecture. This paper proposes a new type of service architecture called AXaaS (Acceleration as a Service) that will empower developers to satisfy user demand for greater application performance and fully realize new computationally-intensive applications that would be otherwise impossible or impractical. While Telecom Service Providers (TSP) already provide data and bandwidth services, we introduce a new paradigm in which the TSP may charge subscribers for computational acceleration of complex applications by outsourcing computational tasks to larger cloud operators. We provide an exposition of the performance potential of such a service by examining its theoretical impact upon an open-source-based Face Recognition application. We also examine a sample instantiation of cloud resources via Amazon Web Services, and estimate the return on investment for a TSP implementing AXaaS. We find the TSP-side ROI to be quite favorable, which means that AXaaS is a viable new aaS alternative.},
  Doi                      = {10.1109/GLOCOMW.2014.7063416},
  File                     = {:PDF\\07063416.pdf:PDF},
  Keywords                 = {Web services;cloud computing;face recognition;mobile computing;outsourcing;public domain software;AXaaS;Amazon Web services;TSP-side ROI;acceleration as a service;bandwidth services;cloud operators;cloud resources;cloud-offloading;mobile-cloud architecture;open-source-based face recognition application;service architecture;smart devices;telecom service providers;user demand satisfaction;Acceleration;Cloud computing;Conferences;Databases;Face;Performance evaluation;Real-time systems},
  Timestamp                = {2015.07.27}
}

@InProceedings{4290561,
  Title                    = {Scaling Non-Volatile Memory Below 30nm},
  Author                   = {Prall, K.},
  Booktitle                = {Non-Volatile Semiconductor Memory Workshop, 2007 22nd IEEE},
  Year                     = {2007},
  Month                    = {aug},
  Pages                    = {5 -10},

  Abstract                 = {The future scaling challenges of non-volatile memories for 32 Gb+ using 30 nm and below feature sizes are discussed. The key challenges reviewed include structural integrity, floating gate scaling, floating gate replacement, noise and variation. Future trends are discussed.},
  Doi                      = {10.1109/NVSMW.2007.4290561},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\04290561.pdf:PDF},
  Keywords                 = {floating gate replacement;floating gate scaling;nonvolatile memory scaling;nanoelectronics;random-access storage;},
  Timestamp                = {2011.07.16}
}

@InProceedings{5351232,
  Title                    = {On upper bounds for the achievable rates of LDPC codes},
  Author                   = {Presman, N. and Litsyn, S.},
  Booktitle                = {Information Theory Workshop, 2009. ITW 2009. IEEE},
  Year                     = {2009},
  Month                    = {oct},
  Pages                    = {21 -25},

  Abstract                 = {Upper bounds on the mutual entropy of syndrome components of low-density parity-check (LDPC) codes are developed. Using these bounds, an upper bound is derived on the rates of LDPC codes for which reliable communication over a memoryless binary-input symmetric-output (MBIOS) channel is achievable. This bound improves on the earlier known bounds due to Gallager, Burshtein et al., and Weichmann-Sason.},
  Doi                      = {10.1109/ITW.2009.5351232},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05351232.pdf:PDF},
  Keywords                 = {LDPC codes;low-density parity-check;memoryless binary-input symmetric-output channel;mutual entropy;upper bounds;entropy;parity check codes;},
  Timestamp                = {2011.11.15}
}

@Book{NumericalRecipesInC,
  Title                    = {Numerical Recipes with Source Code CD-ROM 3rd Edition : The Art of Scientific Computing},
  Author                   = {William H. Press and Saul A. Teukolsky and William T. Vetterling and Brian P. Flannery},
  Publisher                = { Cambridge University Press},
  Year                     = {2007},

  Abstract                 = {This book/CD bundle of the greatly expanded third edition of Numerical Recipes now has wider coverage than ever before, many new, expanded and updated sections, and two completely new chapters. Co-authored by four leading scientists from academia and industry, Numerical Recipes starts with basic mathematics and computer science and proceeds to complete, working routines. The informal, easy-to-read style that made earlier editions so popular is kept throughout. Highlights of the new material include: a new chapter on classification and inference, Gaussian mixture models, HMMs, hierarchical clustering, and SVMs; a new chapter on computational geometry, covering KD trees, quad- and octrees, Delaunay triangulation, and algorithms for lines, polygons, triangles, and spheres; interior point methods for linear programming; MCMC; an expanded treatment of ODEs with completely new routines; and many new statistical distributions. For support or further licence information please visit www.nr.com.},
  ISBN                     = {978-0521884075}
}

@Article{Rajwar:2002:TLE:635508.605399,
  Title                    = {Transactional lock-free execution of lock-based programs},
  Author                   = {Rajwar, Ravi and Goodman, James R.},
  Journal                  = {SIGOPS Oper. Syst. Rev.},
  Year                     = {2002},

  Month                    = {oct},
  Pages                    = {5--17},
  Volume                   = {36},

  Acmid                    = {605399},
  Address                  = {New York, NY, USA},
  Doi                      = {http://doi.acm.org/10.1145/635508.605399},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\p5-rajwar.pdf:PDF},
  ISSN                     = {0163-5980},
  Issue                    = {5},
  Numpages                 = {13},
  Publisher                = {ACM},
  Timestamp                = {2011.11.25},
  Url                      = {http://doi.acm.org/10.1145/635508.605399}
}

@Article{Rajwar:2005:VTM:1080695.1070011,
  Title                    = {Virtualizing Transactional Memory},
  Author                   = {Rajwar, Ravi and Herlihy, Maurice and Lai, Konrad},
  Journal                  = {SIGARCH Comput. Archit. News},
  Year                     = {2005},

  Month                    = {May},
  Pages                    = {494--505},
  Volume                   = {33},

  Acmid                    = {1070011},
  Address                  = {New York, NY, USA},
  Doi                      = {http://doi.acm.org/10.1145/1080695.1070011},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\Rajwar_2005_VTM_1080695_1070011.pdf:PDF},
  ISSN                     = {0163-5964},
  Issue                    = {2},
  Numpages                 = {12},
  Publisher                = {ACM},
  Timestamp                = {2011.11.25},
  Url                      = {http://doi.acm.org/10.1145/1080695.1070011}
}

@InProceedings{1365107,
  Title                    = {Analysis of an algorithm for irregular LDPC code construction},
  Author                   = {Ramamoorthy, A. and Wesel, R.D.},
  Booktitle                = {Information Theory, 2004. ISIT 2004. Proceedings. International Symposium on},
  Year                     = {2004},
  Month                    = {june-2 july},
  Pages                    = { 69},

  Abstract                 = { This work presents a rigorous analysis of an algorithm proposed by Tian et al. (2003) for the construction of irregular LDPC codes with reduced stopping sets and low error floors. Computation of the expected number of stopping sets of a given size proves that the algorithm significantly outperforms a random construction. We show that the algorithm provably reduces the expected number of stopping sets up to a certain size (based on the input parameters). The expected number of cycles of a given size is computed for both constructions.},
  Doi                      = {10.1109/ISIT.2004.1365107},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01365107.pdf:PDF},
  Keywords                 = { error floor; irregular LDPC code construction; stopping set; parity check codes;},
  Timestamp                = {2012.04.10}
}

@InProceedings{6717247,
  Title                    = {FPGA Accelerated Computing Platform for MATLAB and C/C++},
  Author                   = {Rasul, R. and Mutaal, A. and Saqib, N.A. and Kaleem, M. and Shaukat, A. and Khanum, A. and Khan, M.A.},
  Booktitle                = {Frontiers of Information Technology (FIT), 2013 11th International Conference on},
  Year                     = {2013},
  Month                    = {Dec},
  Pages                    = {166-171},

  Abstract                 = {With latest advancements in architecture, reprogram ability and availability of abundant on-chip resources, FPGAs (Field Programmable Gate Array) are used as hardware accelerators to speedup computationally intensive tasks with inherent parallelism. However non-availability of standard MATLAB and C/C++ computation routines and communication interface for general purpose programming restricted researchers and developers from easily utilizing the parallel computational ability of FPGAs in MATLAB and C/C++. In this article we propose a proof of concept implementation for software-hardware co-design that can be used with MATLAB and C/C++ to share the burden of intensified computing with the FPGA. Typical applications which can be divided into multiple tasks to be executed in parallel can be easily transferred to FPGA by utilizing the proposed method. Some of the applications which can efficiently use this concept are image processing, video processing, data encryption and data compression. Results obtained by using our method and routines implemented in software and hardware provide 50% to 100% computational acceleration, as compared to routines running in software on MATLAB running on a computer. The design and concept can aid developers to use FPGAs in combination with higher level computational languages such as MATLAB and C/C++.},
  Doi                      = {10.1109/FIT.2013.38},
  Keywords                 = {C++ language;cryptography;data compression;field programmable gate arrays;hardware-software codesign;mathematics computing;parallel processing;video signal processing;C language;C++ language;FPGA accelerated computing platform;MATLAB;architecture;communication interface;computational language;computationally intensive task;data compression;data encryption;field programmable gate array;general purpose programming;hardware accelerators;image processing;inherent parallelism;on-chip resources;parallel computational ability;parallel execution;reprogram ability;software-hardware codesign;video processing;Acceleration;Computer architecture;Field programmable gate arrays;Hardware;Libraries;MATLAB;C/C++;FPGA;GPU;Hardware Acceleration;MATLAB;MEX},
  Timestamp                = {2015.07.27}
}

@Article{1055352,
  Title                    = {The use of finite fields to compute convolutions},
  Author                   = {Reed, Irving S. and Truong, T.K.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1975},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {208-213},
  Volume                   = {21},

  Abstract                 = {A transform is defined in the Galois field of q^2 elements GF(q^2) , a finite field analogous to the field of complex numbers, when q is a prime such that (--1) is not a quadratic residue. It is shown that the action of this transform over GF(q^2) is equivalent to the discrete Fourier transform of a sequence of complex integers of finite dynamic range. If q is a Mersenne prime, one can utilize the fast Fourier transform (FFT) algorithm to yield a fast convolution without the usual roundoff problem of complex numbers.},
  Doi                      = {10.1109/TIT.1975.1055352},
  File                     = {:PDF\\01055352.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Convolution;Galois fields;Number-theoretic transforms;Arithmetic;Discrete Fourier transforms;Discrete transforms;Dynamic range;Fast Fourier transforms;Filtering;Fourier transforms;Galois fields;Helium;Roundoff errors},
  Timestamp                = {2015.05.15}
}

@Article{ReedandShih,
  Title                    = {VLSI design of inverse-free Berlekamp-Massey algorithm},
  Author                   = {Reed, T. S. and Shih, M. T. and Truong, T. K.},
  Journal                  = {Computer and Digital Techniques, IEE Proceedings E},
  Year                     = {1991},

  Month                    = {September},
  Pages                    = {295-298},
  Volume                   = {138},

  File                     = {:PDF\\VLSI_design_of_inverse-freeBerlekamp-Massey_argorithm.pdf:PDF},
  Issue                    = {5}
}

@Article{701488,
  Title                    = {Modeling and simulation of stress-induced leakage current in ultrathin SiO2 films},
  Author                   = {Ricco, B. and Gozzi, G. and Lanzoni, M.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {jul},
  Number                   = {7},
  Pages                    = {1554 -1560},
  Volume                   = {45},

  Abstract                 = {This paper presents a new model fur stress-induced leakage current (SILC) in ultrathin SiO2 films, that is able to explain and accurately represent the experimental data obtained with MOS capacitors fabricated with different technologies and oxide thickness in the 3-7 nm range},
  Doi                      = {10.1109/16.701488},
  File                     = {:PDF\\00701488.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {Conductive films;Conductivity;Electrons;Leakage current;MOS capacitors;Nonvolatile memory;Stress;Substrates;Tunneling;Voltage;MOS capacitors;insulating thin films;leakage currents;silicon compounds;3 to 7 nm;MOS capacitor;SILC;SiO2;modeling;simulation;stress induced leakage current;ultrathin SiO2 film;},
  Timestamp                = {2013.02.13}
}

@InProceedings{ErrorFloorsofLDPCCodes_Richardson,
  Title                    = {Error floor of LDPC codes},
  Author                   = {T.J. Richardson},
  Booktitle                = {Proceedings of Allerton Conference of Communications, Control and Computing},
  Year                     = {2003},
  Month                    = {oct},
  Pages                    = {1426-1435},

  File                     = {:PDF\\ErrorFloorsofLDPCCodes_Richardson.pdf:PDF},
  Timestamp                = {2013.01.25}
}

@Article{910578,
  Title                    = {Design of capacity-approaching irregular low-density parity-check codes},
  Author                   = {Richardson, T.J. and Shokrollahi, M.A. and Urbanke, R.L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {feb},
  Number                   = {2},
  Pages                    = {619 -637},
  Volume                   = {47},

  Abstract                 = {We design low-density parity-check (LDPC) codes that perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on the work of Richardson and Urbanke (see ibid., vol.47, no.2, p.599-618, 2000). Assuming that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph possess a certain symmetry. Using this symmetry property we then show that, under the assumption of no cycles, the message densities always converge as the number of iterations tends to infinity. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief-propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present some simulation results for the codes found which show that the performance of the codes is very close to the asymptotic theoretical bounds.},
  Doi                      = {10.1109/18.910578},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00910578.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Shannon capacity;additive white Gaussian noise channel;asymptotic theoretical bounds;belief-propagation decoder;binary-input AWGN channel;bipartite graph;capacity-approaching irregular codes;code design;decoding;degree distribution;degree structure optimisation;irregular bipartite graphs;low-density parity-check codes;message densities;message nodes;probability densities;simulation results;stability condition;symmetric communication channel;symmetry property;upper bound;AWGN channels;channel capacity;decoding;error detection codes;optimisation;probability;stability;},
  Timestamp                = {2012.04.10}
}

@Article{910577,
  Title                    = {The capacity of low-density parity-check codes under message-passing decoding},
  Author                   = {Richardson, T.J. and Urbanke, R.L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {feb},
  Number                   = {2},
  Pages                    = {599 -618},
  Volume                   = {47},

  Abstract                 = {We present a general method for determining the capacity of low-density parity-check (LDPC) codes under message-passing decoding when used over any binary-input memoryless channel with discrete or continuous output alphabets. Transmitting at rates below this capacity, a randomly chosen element of the given ensemble will achieve an arbitrarily small target probability of error with a probability that approaches one exponentially fast in the length of the code. (By concatenating with an appropriate outer code one can achieve a probability of error that approaches zero exponentially fast in the length of the code with arbitrarily small loss in rate.) Conversely, transmitting at rates above this capacity the probability of error is bounded away from zero by a strictly positive constant which is independent of the length of the code and of the number of iterations performed. Our results are based on the observation that the concentration of the performance of the decoder around its average performance, as observed by Luby et al. in the case of a binary-symmetric channel and a binary message-passing algorithm, is a general phenomenon. For the particularly important case of belief-propagation decoders, we provide an effective algorithm to determine the corresponding capacity to any desired degree of accuracy. The ideas presented in this paper are broadly applicable and extensions of the general method to low-density parity-check codes over larger alphabets, turbo codes, and other concatenated coding schemes are outlined},
  Doi                      = {10.1109/18.910577},
  File                     = {:\\\\homepd\\pd6\\ユニット管理\\refereces\\PDF\\00910577.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {belief-propagation decoders;binary message-passing algorithm;binary-input memoryless channel;binary-symmetric channel;code capacity;code length;code rate;concatenated coding;continuous output alphabet;decoder performance;discrete output alphabet;error probability;iterative decoding;low-density parity-check codes;message-passing decoding;outer code;turbo codes;concatenated codes;error detection codes;error statistics;iterative decoding;memoryless systems;telecommunication channels;turbo codes;},
  Timestamp                = {2011.12.02}
}

@Article{910579,
  Title                    = {Efficient encoding of low-density parity-check codes},
  Author                   = {Richardson, T.J. and Urbanke, R.L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},
  Number                   = {2},
  Pages                    = {638-656},
  Volume                   = {47},

  Abstract                 = {Low-density parity-check (LDPC) codes can be considered serious competitors to turbo codes in terms of performance and complexity and they are based on a similar philosophy: constrained random code ensembles and iterative decoding algorithms. We consider the encoding problem for LDPC codes. More generally we consider the encoding problem for codes specified by sparse parity-check matrices. We show how to exploit the sparseness of the parity-check matrix to obtain efficient encoders. For the (3,6)-regular LDPC code, for example, the complexity of encoding is essentially quadratic in the block length. However, we show that the associated coefficient can be made quite small, so that encoding codes even of length n≃100000 is still quite practical. More importantly, we show that “optimized” codes actually admit linear time encoding},
  Doi                      = {10.1109/18.910579},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00910579.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {computational complexity;error detection codes;iterative decoding;optimisation;random codes;sparse matrices;binary erasure channel;block length;code length;coefficient;complexity;constrained random code ensembles;efficient encoding;iterative decoding algorithms;linear time encoding;low-density parity-check codes;optimized codes;performance;regular LDPC code;sparse parity-check matrices;turbo codes;Algorithm design and analysis;Channel capacity;Encoding;Iterative algorithms;Iterative decoding;Parity check codes;Physics;Sparse matrices;Technological innovation;Turbo codes},
  Timestamp                = {2013.10.15}
}

@InProceedings{5451825,
  Title                    = {Flash memory SSD based DBMS for data warehouses and data marts},
  Author                   = {Rizvi, S.S. and Tae-Sun Chung},
  Booktitle                = {Computer and Automation Engineering (ICCAE), 2010 The 2nd International Conference on},
  Year                     = {2010},
  Month                    = {feb.},
  Pages                    = {557 -559},
  Volume                   = {3},

  Abstract                 = {Flash memory based high capacity SSDs open the doors for large enterprise applications for better performance and high reliability. Flash memory hardware characteristics not allow disk based schemes implication directly. For employing such schemes, we need to revise them on some level to make them effective for flash media storage. In this paper, we aim to implement DBMS on flash memory SSD based large enterprise applications. This paper presents the relevancy of SSD characteristics with storage features of data warehouses and data marts and proposes the architecture of data storage for variable-length records in and data retrieval using virtual sequential access method by multilevel indexing from flash memory based SSDs for such applications. We prove less overhead with high reliability, and more throughput compare to hard disk drives.},
  Doi                      = {10.1109/ICCAE.2010.5451825},
  File                     = {:PDF\\05451825.pdf:PDF},
  Keywords                 = {DBMS;data marts;data retrieval;data warehouse;flash memory SSD;large enterprise application;multilevel indexing;solid-state-drive;variable-length record;virtual sequential access method;data warehouses;flash memories;storage management;},
  Timestamp                = {2011.05.20}
}

@InProceedings{5674850,
  Title                    = {Flash memory SSD based DBMS for high performance computing embedded and multimedia systems},
  Author                   = {Rizvi, S.S. and Tae-Sun Chung},
  Booktitle                = {Computer Engineering and Systems (ICCES), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {30 2010-dec. 2},
  Pages                    = {183 -188},

  Abstract                 = {Flash memory based large capacity SSDs open the doors on high performance computing applications by offering remarkable throughput and amazing reliability. However, flash memory hardware characteristics like erase-before-write and limited endurance cycles do not allow disk based schemes implication directly. For employing such schemes, we need to revise them on some level to make them effective for flash media. Regarding, the researchers have devoted considerable efforts to implement the DBMSs on flash for fast retrieval of data that is bit indolent using file systems. However, previous techniques require huge main memory space and high computational power for data processing. This paper proposes an advanced DBMS architecture using key sequenced data set, virtual sequential access method and multilevel indexing for flash memory SSD based performance oriented embedded and multimedia applications. Basic database operations plus space reclamation and memory wear-leveling are achieved by taking flash characteristics into account carefully. Main memory based buffer management is implemented to increase throughput and for efficient media utilization. Comprehensive performance evaluations using two modern benchmarks prove less overhead with high reliability and outstanding throughput for flash SSD based DBMSs compare to HDD.},
  Doi                      = {10.1109/ICCES.2010.5674850},
  File                     = {:PDF\\05674850.pdf:PDF},
  Keywords                 = {DBMS architecture;buffer management;data retrieval;embedded systems;endurance cycles;erase-before-write;file systems;flash media;flash memory SSD;flash memory hardware characteristics;flash memory solid-state-drive;high performance computing;key sequenced data set;large capacity SSDs;memory wear-leveling;multilevel indexing;multimedia systems;performance evaluations;reliability;space reclamation;virtual sequential access;buffer storage;database indexing;embedded systems;flash memories;information retrieval;multimedia computing;multimedia databases;performance evaluation;},
  Timestamp                = {2011.05.20}
}

@InProceedings{6125326,
  Title                    = {Sub-optimal importance sampling for fast simulation of linear block codes over BSC channels},
  Author                   = {Romano, G. and Drago, A. and Ciuonzo, D.},
  Booktitle                = {Wireless Communication Systems (ISWCS), 2011 8th International Symposium on},
  Year                     = {2011},
  Month                    = {nov.},
  Pages                    = {141 -145},

  Abstract                 = {Estimation of very low word-error probability of hard-decoded linear block codes can be performed through Monte-Carlo simulation. The computational complexity of the standard method however increases as the probability of error to be estimated decreases. In this paper we propose a general algorithm for fast estimation of probability of error of linear block codes on BSC channels based on the importance sampling and the cross-entropy method for rare-events that can be employed for any hard-decision decoder. When optimal decoding is used the algorithm reduces to a single simulation run that can estimate, with a given accuracy, performances for a whole range of sufficiently high signal-to-noise ratios.},
  Doi                      = {10.1109/ISWCS.2011.6125326},
  File                     = {:PDF\\06125326.pdf:PDF},
  ISSN                     = {2154-0217},
  Keywords                 = {BSC channels;Monte-Carlo simulation;computational complexity;cross-entropy method;hard-decision decoder;hard-decoded linear block codes;optimal decoding;signal-to-noise ratios;suboptimal importance sampling;word-error probability;Monte Carlo methods;block codes;channel coding;computational complexity;decoding;linear codes;probability;},
  Timestamp                = {2012.09.04}
}

@Article{RivestShamirWOM,
  Title                    = {How To Reuse a "Write-Once" Memory},
  Author                   = {Ronald L. Rivest, Adi Shamir},
  Journal                  = {Information and Control},
  Year                     = {1982},

  Month                    = {October},
  Number                   = {1},
  Pages                    = {1-19},
  Volume                   = {55},

  Abstract                 = {Storage media such as digital optical disks, PROMS, or paper tape consist of a number of “write-once" bit positions (wits); each wit initially contains a “0≓ that may later be irreversibly overwritten with a “1.≓ It is demonstrated that such “write-once memories" (woms) can be “rewritten" to a surprising degree. For example, only 3 wits suffice to represent any 2-bit value in a way that can later be updated to represent any other 2-bit value. For large k, 1.29… · k wits suffice to represent a k-bit value in a way that can be similarly updated. Most surprising, allowing t writes of a k-bit value requires only t + o(t) wits, for any fixed k. For fixed t, approximately k · t/log(t) wits are required as k → ∞. An n-wit WOM is shown to have a “capacity≓ (i.e., k · t when writing a k-bit value t times) of up to n · log(n) bits.},
  File                     = {:PDF\\1-s2.0-S0019995882903448-main.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.06.08},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0019995882903448}
}

@Article{4405581,
  Title                    = {Combine LDPC Codes Over GF(q) With q-ary Modulations for Bandwidth Efficient Transmission},
  Author                   = {Bo Rong and Tao Jiang and Xiangming Li and Soleymani, M.R.},
  Journal                  = {Broadcasting, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {78-84},
  Volume                   = {54},

  Abstract                 = {Low-density parity-check (LDPC) codes are playing more and more important role in digital broadcasting standards due to their excellent error correction performance. In this paper, we study the combination of LDPC codes over GF(q) with q-ary modulations for bandwidth efficient transmission over AWGN channel and consider the design of the codes. Specifically, we develop the concept of quasi-regular codes, and propose an improved Monte Carlo method to optimize the quasi-regular codes. To justify the performance of our proposed scheme, simulation results are presented and analysed.},
  Doi                      = {10.1109/TBC.2007.912849},
  File                     = {:PDF\\04405581.pdf:PDF},
  ISSN                     = {0018-9316},
  Keywords                 = {AWGN channels;Monte Carlo methods;electromagnetic wave transmission;parity check codes;AWGN channel;GF q ary modulations;LDPC codes;Monte Carlo method;bandwidth efficient transmission;low density parity check codes;quasi regular codes;Bandwidth;Code standards;Data communication;Digital video broadcasting;Modulation coding;Parity check codes;Phase shift keying;Satellite broadcasting;Sparse matrices;TV broadcasting;Bandwidth efficient transmission;LDPC codes;Monte Carlo method;q-ary modulations},
  Timestamp                = {2015.04.21}
}

@Article{rosenblatt1958perceptron,
  Title                    = {The perceptron: a probabilistic model for information storage and organization in the brain.},
  Author                   = {Rosenblatt, Frank},
  Journal                  = {Psychological review},
  Year                     = {1958},
  Number                   = {6},
  Pages                    = {386},
  Volume                   = {65},

  File                     = {:PDF\\neuronove_siete_priesvitky_02_Q.pdf:PDF},
  Publisher                = {American Psychological Association},
  Timestamp                = {2015.07.30}
}

@Article{1053994,
  Title                    = {A class of majority logic decodable codes (Corresp.)},
  Author                   = {Rudolph, L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1967},

  Month                    = {april },
  Number                   = {2},
  Pages                    = {305 -307},
  Volume                   = {13},

  Abstract                 = {Not available},
  Doi                      = {10.1109/TIT.1967.1053994},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01053994.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Cyclic codes;Majority logic decoding;},
  Timestamp                = {2011.10.24}
}

@InProceedings{6875417,
  Title                    = {On the upper bound on undetected error probability for LDPC code},
  Author                   = {Rybin, P. and Zyablov, V.},
  Booktitle                = {Information Theory (ISIT), 2014 IEEE International Symposium on},
  Year                     = {2014},
  Month                    = {June},
  Pages                    = {3160-3164},

  Abstract                 = {This paper deals with the method of undetected error probability estimation for a low-density parity-check (LDPC) code under any given iterative decoding algorithm. We propose such modification of a given iterative decoding algorithm, that almost preserves a decoding failure exponent and decoding complexity of this algorithm. We obtain the upper bound on the undetected error probability for the modified algorithm. We show how to use the proposed method to estimate the undetected error probability of LDPC code under the belief propagation (BP) algorithm at the end of this paper.},
  Doi                      = {10.1109/ISIT.2014.6875417},
  File                     = {:PDF\\06875417.pdf:PDF},
  Keywords                 = {belief networks;error statistics;iterative decoding;parity check codes;LDPC code;belief propagation algorithm;decoding complexity;decoding failure exponent;iterative decoding algorithm;low density parity check code;undetected error probability estimation;Complexity theory;Error probability;Iterative decoding;Maximum likelihood decoding;Upper bound},
  Timestamp                = {2015.01.07}
}

@InCollection{Saad2003,
  Title                    = {The Theory of On-line Learning - A Statistical Physics Approach},
  Author                   = {Saad, D.},
  Booktitle                = {Exploratory Data Analysis in Empirical Research},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2003},
  Editor                   = {Schwaiger, Manfred and Opitz, Otto},
  Pages                    = {300-308},
  Series                   = {Studies in Classification, Data Analysis, and Knowledge Organization},

  Doi                      = {10.1007/978-3-642-55721-7_31},
  ISBN                     = {978-3-540-44183-0},
  Language                 = {English},
  Timestamp                = {2015.07.15},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-55721-7_31}
}

@InProceedings{5394793,
  Title                    = {On systematic design of universally capacity approaching rate-compatible sequences of LDPC code ensembles over binary-input output-symmetric memoryless channels},
  Author                   = {Saeedi, H. and Pishro-Nik, H. and Banihashemi, A.H.},
  Booktitle                = {Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on},
  Year                     = {2009},
  Month                    = {30 2009-oct 2},
  Pages                    = {400 -407},

  Abstract                 = {Despite tremendous amount of research on the design of low-density parity-check (LDPC) codes with belief propagation decoding over different types of binary-input output-symmetric memoryless (BIOSM) channels, most results on this topic are based on numerical methods and optimization which do not provide much insight into the design process. In particular, systematic design of provably capacity achieving sequences of LDPC code ensembles over the general class of BIOSM channels, has remained a fundamental open problem. For the case of the binary erasure channel, explicit construction of capacity achieving sequences have been proposed based on a property called the flatness condition. In this paper, we propose a systematic method to design universally capacity approaching rate-compatible LDPC code ensemble sequences over BIOSM channels. This is achieved by interpreting the flatness condition over the BEC, as a successive maximization (SM) principle that is generalized to other BIOSM channels to design a sequence of capacity approaching ensembles called the parent sequence. The SM principle is then applied to each ensemble within the parent sequence, this time to design rate-compatible puncturing schemes. As part of our results, we extend the stability condition which was previously derived for degree-2 variable nodes to other variable node degrees as well as to the case of rate-compatible codes. Consequently, we rigorously prove that using the SM principle, one is able to design universally capacity achieving rate-compatible LDPC code ensemble sequences over the BEC. Unlike the previous results on such schemes over the BEC in the literature, the proposed SM approach is naturally extendable to other BIOSM channels. The performance of the rate-compatible schemes designed based on our systematic method is comparable to those designed by optimization.},
  Doi                      = {10.1109/ALLERTON.2009.5394793},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05394793.pdf:PDF},
  Keywords                 = {LDPC code;binary-input output-symmetric memoryless channel;low-density parity-check;rate-compatible puncturing scheme;rate-compatible sequences;successive maximization;binary codes;channel capacity;memoryless systems;optimisation;parity check codes;},
  Timestamp                = {2011.12.02}
}

@Article{476246,
  Title                    = {Algebraic-geometric codes and multidimensional cyclic codes: a unified theory and algorithms for decoding using Grobner bases},
  Author                   = {Saints, K. and Heegard, C.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1995},

  Month                    = {nov},
  Number                   = {6},
  Pages                    = {1733 -1751},
  Volume                   = {41},

  Abstract                 = {It is proved that any algebraic-geometric (AG) code can be expressed as a cross section of an extended multidimensional cyclic code. Both AG codes and multidimensional cyclic codes are described by a unified theory of linear block codes defined over point sets: AG codes are defined over the points of an algebraic curve, and an m-dimensional cyclic code is defined over the points in m-dimensional space. The power of the unified theory is in its description of decoding techniques using Grobner bases. In order to fit an AG code into this theory, a change of coordinates must be applied to the curve over which the code is defined so that the curve is in special position. For curves in special position, all computations can be performed with polynomials and this also makes it possible to use the theory of Grobner bases. Next, a transform is defined for AG codes which generalizes the discrete Fourier transform. The transform is also related to a Grobner basis, and is useful in setting up the decoding problem. In the decoding problem, a key step is finding a Grobner basis for an error locator ideal. For AG codes, multidimensional cyclic codes, and indeed, any cross section of an extended multidimensional cyclic code, Sakata's algorithm can be used to find linear recursion relations which hold on the syndrome array. In this general context, the authors give a self-contained and simplified presentation of Sakata's algorithm, and present a general framework for decoding algorithms for this family of codes, in which the use of Sakata's algorithm is supplemented by a procedure for extending the syndrome array},
  Doi                      = {10.1109/18.476246},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00476246.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Grobner bases;Sakata's algorithm;algebraic curve;algebraic-geometric codes;algorithms;decoding;discrete Fourier transform;error locator;linear block codes;linear recursion relations;m-dimensional space;multidimensional cyclic codes;point set;polynomial;syndrome array;unified theory;algebraic geometric codes;block codes;cyclic codes;decoding;discrete Fourier transforms;error correction codes;linear codes;polynomials;},
  Timestamp                = {2012.05.15}
}

@Article{585557,
  Title                    = {A quantitative analysis of time-decay reproducible stress-induced leakage current in SiO2 films},
  Author                   = {Sakakibara, K. and Ajika, N. and Eikyu, K. and Ishikawa, K. and Miyoshi, H.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1997},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {1002 -1008},
  Volume                   = {44},

  Abstract                 = {In the cases of both Fowler-Nordheim (FN) stress and substrate hot-hole stress, three reproducible stress-induced leakage current (SILC) components have been found for the repeated unipolar gate-voltage scans in 9.2 nm wet oxides. To clarify the mechanisms of these current components, a quantitative analysis has been developed. By precisely modeling the phonon assisted tunneling process, it has been shown that the E-J and t-J characteristics of the reproducible current components can be completely simulated as electron tunneling processes into the neutral traps, each with a single trap level. From this analysis, the physical parameters of the traps have been estimated with a reasonable degree of accuracy. Furthermore, the increase in distribution of the neutral trap density toward both the SiO2 interfaces has also been estimated},
  Doi                      = {10.1109/16.585557},
  File                     = {:PDF\\A_Quatitative_Analysis_of_Time-Decay_Reproducible_Stress-Induced_Leakage_Current_in_SiO2_Films.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {9.2 nm;Fowler-Nordheim stress;SiO2 films;SiO2-Si;current components;electron tunneling processes;modeling;neutral trap density;phonon assisted tunneling process;quantitative analysis;stress-induced leakage current;substrate hot-hole stress;time-decay reproducible SILC;unipolar gate-voltage scans;MOSFET;dielectric thin films;electric breakdown;electron traps;hot carriers;interface states;leakage currents;semiconductor device models;semiconductor-insulator boundaries;silicon compounds;tunnelling;}
}

@Article{585555,
  Title                    = {Identification of stress-induced leakage current components and the corresponding trap models in SiO2 films},
  Author                   = {Sakakibara, K. and Ajika, N. and Hatanaka, M. and Miyoshi, H. and Yasuoka, A.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1997},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {986 -992},
  Volume                   = {44},

  Abstract                 = {Time-decay stress-induced leakage current (SILC) has been systematically investigated for the cases of both Fowler-Nordheim (FN) stress and substrate hot-hole stress. From the three viewpoints of the reproducibility of the-current component for the gate voltage scan, the change of oxide charge during the gate voltage scan, and the resistance of the current component to thermal annealing, it has been found that time-decay stress-induced leakage current is composed of five current components, regardless of stress type. Trap models corresponding to each current component have been proposed. In addition, it has also been proven that holes generate the electron traps related to one of those current components},
  Doi                      = {10.1109/16.585555},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00585555.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {Fowler-Nordheim stress;MOS transistors;SiO2 films;SiO2-Si;electron traps;gate voltage scan;oxide charge;stress-induced leakage current components;substrate hot-hole stress;thermal annealing resistance;time-decay stress-induced leakage current;trap models;MOSFET;dielectric thin films;electric breakdown;electron traps;hot carriers;leakage currents;semiconductor device models;semiconductor-insulator boundaries;silicon compounds;tunnelling;},
  Timestamp                = {2011.09.16}
}

@Article{Sakata1991191,
  Title                    = {Two-dimensional shift register synthesis and Grﾃｶbner bases for polynomial ideals over an integer residue ring },
  Author                   = {Shojiro Sakata},
  Journal                  = {Discrete Applied Mathematics },
  Year                     = {1991},
  Number                   = {1-3},
  Pages                    = {191 - 203},
  Volume                   = {33},

  Doi                      = {http://dx.doi.org/10.1016/0166-218X(91)90115-D},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Sakata1991191.pdf:PDF},
  ISSN                     = {0166-218X},
  Timestamp                = {2014.03.26},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0166218X9190115D}
}

@Article{Sakata1990207,
  Title                    = {Extension of the Berlekamp-Massey algorithm to N dimensions },
  Author                   = {Shojiro Sakata},
  Journal                  = {Information and Computation },
  Year                     = {1990},
  Number                   = {2},
  Pages                    = {207 - 239},
  Volume                   = {84},

  Doi                      = {http://dx.doi.org/10.1016/0890-5401(90)90039-K},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Sakata1990207.pdf:PDF},
  ISSN                     = {0890-5401},
  Timestamp                = {2014.03.26},
  Url                      = {http://www.sciencedirect.com/science/article/pii/089054019090039K}
}

@Article{Sakata1988321,
  Title                    = {Finding a minimal set of linear recurring relations capable of generating a given finite two-dimensional array },
  Author                   = {Shojiro Sakata},
  Journal                  = {Journal of Symbolic Computation },
  Year                     = {1988},
  Number                   = {3},
  Pages                    = {321 - 337},
  Volume                   = {5},

  Doi                      = {http://dx.doi.org/10.1016/S0747-7171(88)80033-6},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Sakata1988321.pdf:PDF},
  ISSN                     = {0747-7171},
  Timestamp                = {2014.03.26},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0747717188800336}
}

@Article{476248,
  Title                    = {Generalized Berlekamp-Massey decoding of algebraic-geometric codes up to half the Feng-Rao bound},
  Author                   = {Sakata, S. and Jensen, H.E. and Hoholdt, T.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1995},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1762-1768},
  Volume                   = {41},

  Abstract                 = {We treat a general class of algebraic-geometric codes and show how to decode these up to half the Feng-Rao bound, using an extension and modification of the Sakata algorithm (1990). The Sakata algorithm is a generalization to N dimensions of the classical Berlekamp-Massey algorithm},
  Doi                      = {10.1109/18.476248},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00476248.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {algebraic geometric codes;decoding;Feng-Rao bound;Sakata algorithm;algebraic-geometric codes;generalized Berlekamp-Massey decoding;Decoding;Differential equations;Error correction;Polynomials;Vectors},
  Timestamp                = {2014.03.25}
}

@Article{SakataJensenHoholdt1995,
  Title                    = {Generalized Berlekamp-Massey decoding of algebraic-geometric codes up to half the Feng-Rao bound},
  Author                   = {Sakata, S. and Jensen, H.E. and Hoholdt, T.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1995},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {1762 -1768},
  Volume                   = {41},

  Abstract                 = {We treat a general class of algebraic-geometric codes and show how to decode these up to half the Feng-Rao bound, using an extension and modification of the Sakata algorithm (1990). The Sakata algorithm is a generalization to N dimensions of the classical Berlekamp-Massey algorithm},
  Doi                      = {10.1109/18.476248},
  File                     = {:PDF\\Generalized_Berlekamp-Massey_Decoding_of_Algebraic-Geometric_Codes_up_to_Half_the_Feng-Rao_Bound.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Feng-Rao bound;Sakata algorithm;algebraic-geometric codes;generalized Berlekamp-Massey decoding;algebraic geometric codes;decoding;}
}

@Article{476240,
  Title                    = {Fast decoding of algebraic-geometric codes up to the designed minimum distance},
  Author                   = {Sakata, S. and Justesen, J. and Madelung, Y. and Jensen, H.E. and Hoholdt, T.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1995},

  Month                    = nov,
  Number                   = {6},
  Pages                    = {1672 -1677},
  Volume                   = {41},

  Abstract                 = {We present a decoding algorithm for algebraic-geometric codes from regular plane curves, in particular the Hermitian curve, which corrects all error patterns of weight less than d*/2 with low complexity. The algorithm is based on the majority scheme of Feng and Rao (1993) and uses a modified version of Sakata's (1988) generalization of the Berlekamp-Massey algorithm},
  Doi                      = {10.1109/18.476240},
  File                     = {:PDF\\Fast_Decoding_of_Algebraic-Geometric_Codes_up_to_the_Designed_Minimum_Distance.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Berlekamp-Massey algorithm;Hermitian curve;algebraic-geometric codes;error patterns correction;fast decoding algorithm;low complexity;majority scheme;minimum distance;regular plane curves;algebraic geometric codes;computational complexity;decoding;}
}

@Article{6528074,
  Title                    = {Dynamic Threshold Schemes for Multi-Level Non-Volatile Memories},
  Author                   = {Sala, F. and Gabrys, R. and Dolecek, L.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {2624-2634},
  Volume                   = {61},

  Abstract                 = {In non-volatile memories, reading stored data is typically done through the use of predetermined fixed thresholds. However, due to problems commonly affecting such memories, including voltage drift, overwriting, and inter-cell coupling, fixed threshold usage often results in significant asymmetric errors. To combat these problems, Zhou, Jiang, and Bruck recently introduced the notion of dynamic thresholds and applied them to the reading of binary sequences. In this paper, we explore the use of dynamic thresholds for multi-level cell (MLC) memories. We provide a general scheme to compute and apply dynamic thresholds and derive performance bounds. We show that the proposed scheme compares favorably with the optimal thresholding scheme. Finally, we develop limited-magnitude error-correcting codes tailored to take advantage of dynamic thresholds.},
  Doi                      = {10.1109/TCOMM.2013.053013.120733},
  ISSN                     = {0090-6778},
  Keywords                 = {binary sequences;error correction codes;random-access storage;MLC memories;asymmetric errors;binary sequences;dynamic threshold schemes;error-correcting codes;inter-cell coupling;multilevel cell memories;multilevel nonvolatile memories;optimal thresholding;overwriting;predetermined fixed thresholds;voltage drift;Non-binary error correction codes;dynamic reading thresholds;non-volatile memory;permutations;sequences},
  Timestamp                = {2014.09.03}
}

@Book{Sala:1412032,
  Title                    = {Gr\"{o}bner Bases, Coding, and Cryptography},
  Author                   = {Sala, Massimiliano and Mora, Teo and Perret, Ludovic},
  Publisher                = {Springer},
  Year                     = {2009},

  Address                  = {Dordrecht},

  Abstract                 = {Coding theory and cryptography allow secure and reliable data transmission, which is at the heart of modern communication. Nowadays, it is hard to find an electronic device without some code inside.

Gröbner bases have emerged as the main tool in computational algebra, permitting numerous applications, both in theoretical contexts and in practical situations.

This book is the first book ever giving a comprehensive overview on the application of commutative algebra to coding theory and cryptography. For example, all important properties of algebraic/geometric coding systems (including encoding, construction, decoding, list decoding) are individually analysed, reporting all significant approaches appeared in the literature. Also, stream ciphers, PK cryptography, symmetric cryptography and Polly Cracker systems deserve each a separate chapter, where all the relevant literature is reported and compared. While many short notes hint at new exciting directions, the reader will find that all chapters fit nicely within a unified notation.},
  ISBN                     = {978-3-540-93805-7},
  Keywords                 = {Coding Theory, Cryptography, Groebner bases, computer algebra},
  Timestamp                = {2012.06.06},
  Url                      = {http://www.springer.com/mathematics/algebra/book/978-3-540-93805-7}
}

@InProceedings{DeepBoltzmannMachineMCMC,
  Title                    = {Learning in Deep Boltzmann Machines using Adaptive MCMC},
  Author                   = {Ruslan Salakhutdinov},
  Booktitle                = {In 27th International Conference on Machine Learning (ICML-2010)},
  Year                     = {2010},

  File                     = {:PDF\\DeepBoltzmannMachineMCMC.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.23}
}

@Article{ELDBM2010,
  Title                    = {Efficient Learning of Deep Boltzmann Machines},
  Author                   = {Ruslan Salakhutdinov and Hugo Larochelle},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2010},
  Pages                    = {693-700},
  Volume                   = {9},

  Abstract                 = {We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM's), a generative model with many layers of hidden variables. The algorithm learns a separate ``recognition'' model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM's practical. Finally, we demonstrate that the DBM's trained using the proposed approximate inference algorithm perform well compared to DBN's and SVM's on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.},
  File                     = {:PDF\\dbmrec.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.23}
}

@Book{salinas,
  Title                    = {Introduction to Statistical Physics},
  Author                   = {Silvio R. A. Salinas},
  Publisher                = {Springer},
  Year                     = {2001},

  File                     = {:PDF\\978-1-4757-3508-6.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.02.06}
}

@InProceedings{6033740,
  Title                    = {On concentration of measures for LDPC code ensembles},
  Author                   = {Sason, I. and Eshel, R.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on},
  Year                     = {2011},
  Pages                    = {1268-1272},

  Abstract                 = {This work considers the concentration of measures for low-density parity-check (LDPC) code ensembles. The two results derived in this paper follow from Azuma's inequality for Doob martingales with bounded differences. The first result is a tightened concentration inequality for the conditional entropy (originally derived by Méasson et al.), and the second result is a concentration inequality for the cardinality of the fundamental systems of cycles of a bipartite graph from the ensemble.},
  Doi                      = {10.1109/ISIT.2011.6033740},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\06033740.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {entropy;graph theory;parity check codes;Azuma inequality;Doob martingales;LDPC code ensembles;bipartite graph;cardinality;conditional entropy;fundamental systems;low-density parity-check code ensembles;tightened concentration inequality;Bipartite graph;Channel capacity;Entropy;Iterative decoding;Upper bound;Azuma's inequality;concentration of measures;low-density parity-check (LDPC) codes;martingales},
  Timestamp                = {2013.12.17}
}

@Article{945248,
  Title                    = {On improved bounds on the decoding error probability of block codes over interleaved fading channels, with applications to turbo-like codes},
  Author                   = {Sason, I. and Shamai, S.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Sep},
  Number                   = {6},
  Pages                    = {2275-2299},
  Volume                   = {47},

  Abstract                 = {We derive here improved upper bounds on the decoding error probability of block codes which are transmitted over fully interleaved Rician fading channels, coherently detected and maximum-likelihood (ML) decoded. We assume that the fading coefficients during each symbol are statistically independent (due to a perfect channel interleaver), and that perfect estimates of these fading coefficients are provided to the receiver. The improved upper bounds on the block and bit error probabilities are derived for fully interleaved fading channels with various orders of space diversity, and are found by generalizing some previously introduced upper bounds for the binary-input additive white Gaussian nose (AWGN) channel. The advantage of these bounds over the ubiquitous union bound is demonstrated for some ensembles of turbo codes and low-density parity-check (LDPC) codes, and it is especially pronounced in a portion of the rate region exceeding the cutoff rate. Our generalization of the Duman and Salehi bound (Duman and Salehi 1998, Duman 1998) which is based on certain variations of Gallager's (1965) bounding technique, is demonstrated to be the tightest reported upper bound. We therefore apply it to calculate numerically upper bounds on the thresholds of some ensembles of turbo-like codes, referring to the optimal ML decoding. For certain ensembles of uniformly interleaved turbo codes, the upper bounds derived here also indicate good match with computer simulation results of efficient iterative decoding algorithms},
  Doi                      = {10.1109/18.945248},
  File                     = {:PDF\\00945248.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Rician channels;block codes;channel coding;diversity reception;error statistics;maximum likelihood decoding;turbo codes;Duman and Salehi bound;Gallager bounding technique;LDPC codes;Rician fading channels;block codes;coherently detected channel;decoding error probability;fading coefficients;interleaved fading channels;iterative decoding;low-density parity-check codes;maximum-likelihood decoding;rate region;space diversity;turbo-like codes;upper bounds;AWGN;Block codes;Error probability;Fading;Iterative decoding;Maximum likelihood decoding;Parity check codes;Rician channels;Turbo codes;Upper bound}
}

@Article{285029,
  Title                    = {Hole injection SiO2 breakdown model for very low voltage lifetime extrapolation},
  Author                   = {Schuegraf, K.F. and Chenming Hu},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {may},
  Number                   = {5},
  Pages                    = {761 -767},
  Volume                   = {41},

  Abstract                 = {In this paper, we present a model for silicon dioxide breakdown characterization, valid for a thickness range between 25 Aring; and 130 Aring;, which provides a method for predicting dielectric lifetime for reduced power supply voltages and aggressively scaled oxide thicknesses. This model, based on hole injection from the anode, accurately predicts QBD and tBD behavior including a fluence in excess of 107 C/cm2 at an oxide voltage of 2.4 V for a 25 Aring; oxide. Moreover, this model is a refinement of and fully complementary with the well known 1/E model, while offering the ability to predict oxide reliability for low voltages},
  Doi                      = {10.1109/16.285029},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00285029.pdf:PDF},
  ISSN                     = {0018-9383},
  Keywords                 = {1/E model;25 to 130 A;SiO2;SiO2 breakdown model;breakdown characterization;dielectric lifetime;hole injection;low voltage lifetime extrapolation;oxide reliability prediction;scaled oxide thicknesses;dielectric thin films;electric breakdown of solids;reliability;semiconductor device models;semiconductor-insulator boundaries;silicon compounds;},
  Timestamp                = {2011.09.30}
}

@InProceedings{996606,
  Title                    = {Physical description of anomalous charge loss in floating gate based NVM's and identification of its dominant parameter},
  Author                   = {Schuler, F. and Degraeve, R. and Hendrickx, P. and Wellekens, D.},
  Booktitle                = {Reliability Physics Symposium Proceedings, 2002. 40th Annual},
  Year                     = {2002},
  Pages                    = { 26 - 33},

  Abstract                 = { A model for anomalous charge loss is presented based on the physical description of charge transport through the tunnel oxide. This physics based model considers phonon-assistance as well as arbitrary 3-dimensional distributions of oxide defects (electron traps). After identifying the trap-trap distance as the most important parameter the 3-dimensional model can be simplified to a tunneling model, which describes one tunneling step only. The consistency of the simplified 1-step model with the percolation model for anomalous charge loss description is shown. Also the accelerated testing method for anomalous charge loss is confirmed and validated by these models.},
  Doi                      = {10.1109/RELPHY.2002.996606},
  File                     = {:PDF\\Physical_Description_of_Anomalous_Charge_Loss_in_Floating_Gate_Based_NVM's_and_Identification_of_its_Dominant_Parameter.pdf:PDF},
  ISSN                     = { },
  Keywords                 = { accelerated testing; charge loss; charge transport; electron trap; floating gate nonvolatile memory; one-step model; percolation model; phonon assistance; three-dimensional defect distribution; tunnel oxide; tunneling model; electron traps; electron-phonon interactions; integrated memory circuits; life testing; percolation; tunnelling;}
}

@Article{RevModPhys.36.856,
  Title                    = {Two-Dimensional Ising Model as a Soluble Problem of Many Fermions},
  Author                   = {SCHULTZ, T. D. and MATTIS, D. C. and LIEB, E. H.},
  Journal                  = {Rev. Mod. Phys.},
  Year                     = {1964},

  Month                    = {Jul},
  Pages                    = {856--871},
  Volume                   = {36},

  Doi                      = {10.1103/RevModPhys.36.856},
  File                     = {:PDF\\RevModPhys.36.856.pdf:PDF},
  Issue                    = {3},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.11.13},
  Url                      = {http://link.aps.org/doi/10.1103/RevModPhys.36.856}
}

@Article{59946,
  Title                    = {The trace description of irreducible quasi-cyclic codes},
  Author                   = {Seguin, G.E. and Drolet, G.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1990},

  Month                    = {nov},
  Number                   = {6},
  Pages                    = {1463 -1466},
  Volume                   = {36},

  Abstract                 = {The notion of a q-ary irreducible quasi-cyclic code of block length n and index r is introduced. A trace description of such a code is provided in a fashion similar to the trace description of irreducible cyclic codes. In particular, it is shown that an irreducible quasi-cyclic code of dimension k is completely described by an irreducible cyclic code and r elements from a field of cardinality qk. Using this fact, a number of binary irreducible quasi-cyclic codes of index 2 are constructed and their weight spectra obtained},
  Doi                      = {10.1109/18.59946},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00059946.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {binary codes;irreducible quasi-cyclic code;q-ary codes;trace description;weight spectra;error correction codes;},
  Timestamp                = {2011.11.10}
}

@Article{PhysRevLett.35.1792,
  Title                    = {Solvable Model of a Spin-Glass},
  Author                   = {Sherrington, David and Kirkpatrick, Scott},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {1975},

  Month                    = {Dec},
  Pages                    = {1792--1796},
  Volume                   = {35},

  Doi                      = {10.1103/PhysRevLett.35.1792},
  File                     = {:PDF\\PhysRevLett.35.1792.pdf:PDF},
  Issue                    = {26},
  Numpages                 = {0},
  Publisher                = {American Physical Society},
  Timestamp                = {2015.02.23},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevLett.35.1792}
}

@InProceedings{5746281,
  Title                    = {A 4Mb embedded SLC resistive-RAM macro with 7.2ns read-write random-access time and 160ns MLC-access capability},
  Author                   = {Shyh-Shyuan Sheu and Meng-Fan Chang and Ku-Feng Lin and Che-Wei Wu and Yu-Sheng Chen and Pi-Feng Chiu and Chia-Chen Kuo and Yih-Shan Yang and Pei-Chia Chiang and Wen-Pin Lin and Che-He Lin and Heng-Yuan Lee and Pei-Yi Gu and Sum-Min Wang and Chen, F.T. and Keng-Li Su and Chen-Hsin Lien and Kuo-Hsing Cheng and Hsin-Tun Wu and Tzu-Kun Ku and Ming-Jer Kao and Ming-Jinn Tsai},
  Booktitle                = {Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 2011 IEEE International},
  Year                     = {2011},
  Month                    = {feb.},
  Pages                    = {200 -202},

  Abstract                 = {This work proposes process/resistance variation-insensitive read schemes for embedded RRAM to achieve fast read speeds with high yields. An embedded mega-bit scale (4Mb), single-level-cell (SLC) RRAM macro with sub-8ns read-write random access time is presented. Multi-level-cell (MLC) operation with 160ns write-verify operation is demonstrated.},
  Doi                      = {10.1109/ISSCC.2011.5746281},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05746281.pdf:PDF},
  ISSN                     = {0193-6530},
  Keywords                 = {MLC-access capability;embedded RRAM;embedded SLC resistive-RAM macro;read-write random-access time;single-level-cell RRAM macro;embedded systems;random-access storage;},
  Timestamp                = {2011.08.24}
}

@Article{5590231,
  Title                    = {Fast-Write Resistive RAM (RRAM) for Embedded Applications},
  Author                   = {Shyh-Shyuan Sheu and Kuo-Hsing Cheng and Meng-Fan Chang and Pei-Chia Chiang and Wen-Pin Lin and Heng-Yuan Lee and Pang-Shiu Chen and Yu-Sheng Chen and Tai-Yuan Wu and Chen, F.T. and Keng-Li Su and Ming-Jer Kao and Ming-Jinn Tsai},
  Journal                  = {Design Test of Computers, IEEE},
  Year                     = {2011},

  Month                    = {jan.-feb. },
  Number                   = {1},
  Pages                    = {64 -71},
  Volume                   = {28},

  Abstract                 = {Especially for microcontroller and mobile applications, embedded nonvolatile memory is an important technology offering to reduce power and provide local persistent storage. This article describes a new resistive RAM device with fast write operation to improve the speed of embedded nonvolatile memories.},
  Doi                      = {10.1109/MDT.2010.96},
  File                     = {:PDF\\05590231.pdf:PDF},
  ISSN                     = {0740-7475},
  Keywords                 = {embedded applications;embedded nonvolatile memory;fast-write resistive RAM;microcontroller;mobile applications;storage;microcontrollers;random-access storage;},
  Timestamp                = {2011.05.23}
}

@Article{ieice_e88-a_5_1346,
  Title                    = {Iterative Decoding Based on the Concave-Convex Procedure},
  Author                   = {Tomoharu SHIBUYA and Ken HARADA and Ryosuke TOHYAMA and Kohichi SAKANIWA},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2005},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1346-1364},
  Volume                   = {E88-A},

  Abstract                 = {New decoding algorithms for binary linear codes based on the concave-convex procedure are presented. Numerical experiments show that the proposed decoding algorithms surpass Belief Propagation (BP) decoding in error performance. Average computational complexity of one of the proposed decoding algorithms is only a few times greater than that of the BP decoding.},
  File                     = {:PDF\\ieice_e88-a_5_1346.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2014.09.04}
}

@Article{ieice_e86-a_10_2601,
  Title                    = {Performance of a Decoding Algorithm for LDPC Codes Based on the Concave-Convex Procedure},
  Author                   = {Tomoharu SHIBUYA and Kohichi SAKANIWA},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2003},

  Month                    = {October},
  Number                   = {20},
  Pages                    = {2601-2606},
  Volume                   = {E86-A},

  Abstract                 = {In this letter, we show the effectiveness of a double-loop algorithm based on the concave-convex procedure (CCCP) in decoding linear codes. For this purpose, we numerically compare the error performance of CCCP-based decoding algorithm with that of a conventional iterative decoding algorithm based on belief propagation (BP). We also investigate computational complexity and its relation to the error performance.},
  File                     = {:PDF\\ieice_e86-a_10_2601.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2014.09.04}
}

@Article{shimizuIEICEvolE89-Ano4,
  Title                    = {Partially-Parallel LDPC Decoder Achieving High-Efficiency Message-Passing Schedule},
  Author                   = {Kazunori Shimizu and Tatsuyuki Ishikawa and Nozomu Togawa and Takeshi Ikenaga Satoshi Goto},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2006},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {969-978},
  Volume                   = {E89-A},

  Abstract                 = {In this paper, we propose a partially-parallel LDPC decoder which achieves a high-efficiency message-passing schedule. The proposed LDPC decoder is characterized as follows: (i) The column operations follow the row operations in a pipelined architecture to ensure that the row and column operations are performed concurrently. (ii) The proposed parallel pipelined bit functional unit enables the column operation module to compute every message in each bit node which is updated by the row operations. These column operations can be performed without extending the single iterative decoding delay when the row and column operations are performed concurrently. Therefore, the proposed decoder performs the column operations more frequently in a single iterative decoding, and achieves a high-efficiency message-passing schedule within the limited decoding delay time. Hardware implementation on an FPGA and simulation results show that the proposed partially-parallel LDPC decoder improves the decoding throughput and bit error performance with a small hardware overhead.},
  File                     = {:PDF\\shimizuIEICE_volE89-A_no4.pdf:PDF},
  Keywords                 = {low-density parity-check codes, partially-parallel LDPC decoder, message-passing algorithm, FPGA},
  Timestamp                = {2011.04.26},
  Url                      = {http://search.ieice.org/bin/summary.php?id=e89-a_4_969&category=A&lang=E&year=2006}
}

@InProceedings{1524200,
  Title                    = {Partially-parallel LDPC decoder based on high-efficiency message-passing algorithm},
  Author                   = {Shimizu, K. and Ishikawa, T. and Togawa, Nozomu and Ikenaga, T. and Goto, S.},
  Booktitle                = {Computer Design: VLSI in Computers and Processors, 2005. ICCD 2005. Proceedings. 2005 IEEE International Conference on},
  Year                     = {2005},
  Month                    = {Oct},
  Pages                    = {503-510},

  Abstract                 = {This paper proposes a partially-parallel LDPC decoder based on a high-efficiency message-passing algorithm. Our proposed partially-parallel LDPC decoder performs the column operations for bit nodes in conjunction with the row operations for check nodes. Bit functional unit with pipeline architecture in our LDPC decoder allows us to perform column operations for every bit node connected to each of check nodes which are updated by the row operations in parallel. Our proposed LDPC decoder improves the tuning when the column operations are performed, accordingly it improves the message-passing efficiency within the limited number of iterations for decoding. We implemented the proposed partially-parallel LDPC decoder on an FPGA, and simulated its decoding performance. Practical simulation shows that our proposed LDPC decoder reduces the number of iterations for decoding, and it improves the bit error performance with a small hardware overhead.},
  Doi                      = {10.1109/ICCD.2005.83},
  File                     = {:PDF\\01693779.pdf:PDF;:PDF\\01693779.pdf:PDF},
  Keywords                 = {decoding;field programmable gate arrays;message passing;parity check codes;FPGA;LDPC decoder;column operations;message passing algorithm;pipeline architecture;Computational modeling;Computer errors;Field programmable gate arrays;Hardware;Iterative algorithms;Iterative decoding;Parity check codes;Pipelines;Production systems;Timing},
  Timestamp                = {2015.02.03}
}

@Article{1411019,
  Title                    = {Generalization of Tanner's minimum distance bounds for LDPC codes},
  Author                   = { Min-Ho Shin and Joon-Sung Kim and Hong-Yeop Song},
  Journal                  = {Communications Letters, IEEE},
  Year                     = {2005},

  Month                    = march,
  Number                   = {3},
  Pages                    = { 240 - 242},
  Volume                   = {9},

  Abstract                 = { Tanner derived minimum distance bounds of regular codes in terms of the eigenvalues of the adjacency matrix by using some graphical analysis on the associated graph of the code. In this letter, we generalize Tanner's results by deriving a bit-oriented bound and a parity-oriented bound on the minimum distances of both regular and block-wise irregular LDPC codes.},
  Doi                      = {10.1109/LCOMM.2005.03002},
  File                     = {:PDF\\Generalization_of_Tanner's_Minimum_Distance_Bounds_for_LDPC_Codes.pdf:PDF},
  ISSN                     = {1089-7798},
  Keywords                 = { LDPC codes; Tanner derived minimum distance bounds; adjacency matrix; bit-oriented bound; block-wise irregular LDPC codes; code graph; eigenvalues; graphical analysis; low-density parity check codes; minimum distances; parity-oriented bound; regular LDPC codes; regular codes; block codes; eigenvalues and eigenfunctions; error correction codes; graph theory; matrix algebra; parity check codes;}
}

@Article{10.1.1.89.2279,
  Title                    = {The Structure of Generalized Quasi-Cyclic Codes},
  Author                   = {I rfan Siap and Nilgun Kulhan},
  Journal                  = {Applied Mathematics Journal E-Notes},
  Year                     = {2005},

  Month                    = {March},
  Pages                    = {24-30},
  Volume                   = {5},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\10.1.1.89.2279.pdf:PDF},
  Timestamp                = {2011.10.12},
  Url                      = {http://www.math.nthu.edu.tw/~amen/}
}

@InProceedings{6766441,
  Title                    = {Generalized Belief Propagation to break trapping sets in LDPC codes},
  Author                   = {Sibel, J.C. and Reynal, S. and Declercq, D.},
  Booktitle                = {Communications Theory Workshop (AusCTW), 2014 Australian},
  Year                     = {2014},
  Month                    = {Feb},
  Pages                    = {132-137},

  Abstract                 = {In this paper, we focus on the Generalized Belief Propagation (GBP) algorithm to solve trapping sets in Low-Density Parity-Check (LDPC) codes. Trapping sets are topological structures in Tanner graphs of LDPC codes that are not correctly decoded by Belief Propagation (BP), leading to exhibit an error-floor in the Bit-Error Rate (BER). Stemming from statistical physics of spin glasses, GBP consists in passing messages between clusters of Tanner graph nodes in another graph called the region-graph. Here, we introduce a specific clustering of nodes, based on a novel local loopfree principle, that breaks trapping sets such that the resulting region-graph is locally loopfree. We then construct a hybrid decoder made of BP and GBP that proves to be a powerful decoder as it clearly improves the BER and defeats the error-floor.},
  Doi                      = {10.1109/AusCTW.2014.6766441},
  File                     = {:PDF\\06766441.pdf:PDF},
  Keywords                 = {belief networks;error statistics;parity check codes;LDPC codes;Tanner graphs;bit error rate;generalized belief propagation;hybrid decoder;loopfree principle;low density parity check codes;region graph;trapping sets;Belief propagation;Charge carrier processes;Decoding;Equations;Glass;Iterative decoding;Generalized Belief Propagation;LDPC codes;error-floor;local clustering;trapping sets},
  Timestamp                = {2015.02.23}
}

@InCollection{Silverman1999,
  Title                    = {Fast Multiplication in Finite Fields GF(2n)},
  Author                   = {Silverman, JosephH.},
  Booktitle                = {Cryptographic Hardware and Embedded Systems},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {1999},
  Editor                   = {Koc, CetinK. and Paar, Christof},
  Pages                    = {122-134},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {1717},

  Doi                      = {10.1007/3-540-48059-5_12},
  File                     = {:PDF\\10.1007-F3-540-48059-5_12.pdf:PDF},
  ISBN                     = {978-3-540-66646-2},
  Language                 = {English},
  Timestamp                = {2015.05.15},
  Url                      = {http://dx.doi.org/10.1007/3-540-48059-5_12}
}

@Book{silverman,
  Title                    = {はじめての数論 原著第3版},
  Author                   = {Joseph H. Silverman},
  Publisher                = {ピアソンエデュケーション},
  Year                     = {2007}
}

@Article{Simmons1986287,
  Title                    = {Concepts of gain at an oxide-semiconductor interface and their application to the TETRAN-A tunnel emitter transistornd-And to the MIS switching device},
  Author                   = {J.G. Simmons and G.W. Taylor},
  Journal                  = {Solid-State Electronics},
  Year                     = {1986},
  Number                   = {3},
  Pages                    = {287 - 303},
  Volume                   = {29},

  Abstract                 = {The idea of current gain at an insulator interface is discussed using the tunnel-oxide as an example upon which to base calculations. A novel approach to the calculation of tunnel components is introduced which accurately describes the electron tunnel component when the metal Fermi level is both above and below the semiconductor conduction band edge. A new form of bipolar amplifier (the TETRAN) is proposed and its performance is compared from a logic and memory point of view with existing transistors. As a logic element the device does not perform well when compared with MOS transistors because the current densities that one can obtain are too low to charge the relatively large tunnel-oxide capacitance. For the memory comparison, the concepts of gain necessary for TETRAN operation are applied to the MIS switching device. Preliminary experimental verification of the concept is reported.},
  Doi                      = {10.1016/0038-1101(86)90207-8},
  ISSN                     = {0038-1101},
  Timestamp                = {2011.09.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0038110186902078}
}

@Article{556667,
  Title                    = {Expander codes},
  Author                   = {Sipser, M. and Spielman, D.A.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1996},
  Number                   = {6},
  Pages                    = {1710-1722},
  Volume                   = {42},

  Abstract                 = {Using expander graphs, we construct a new family of asymptotically good, linear error-correcting codes. These codes have linear time sequential decoding algorithms and logarithmic time parallel decoding algorithms that use a linear number of processors. We present both randomized and explicit constructions of these codes. Experimental results demonstrate the good performance of the randomly chosen codes},
  Doi                      = {10.1109/18.556667},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\00556667.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {error correction codes;graph theory;linear codes;random processes;sequential decoding;asymptotically good linear error-correcting codes;expander codes;expander graphs;explicit constructions;linear time sequential decoding algorithms;logarithmic time parallel decoding algorithms;randomized constructions;randomly chosen codes;Algorithm design and analysis;Bipartite graph;Circuits;Computational modeling;Decoding;Error correction codes;Fault tolerance;Graph theory;Linear code;Parity check codes},
  Timestamp                = {2013.11.11}
}

@InProceedings{1644559,
  Title                    = {Low-Density Parity-Check Codes for Discretized Min-Sum Decoding},
  Author                   = {Smith, B. and Kschischang, F.R. and Yu, W.},
  Booktitle                = {Communications, 2006 23rd Biennial Symposium on},
  Year                     = {2006},
  Month                    = {0-0},
  Pages                    = {14 -17},

  Abstract                 = {The performance of low-density parity-check (LDPC) codes transmitted over a memoryless binary-input continuous output additive white Gaussian noise (AWGN) channel and decoded with quantized min-sum decoding is strongly influenced by the decoder's quantization scheme. This paper presents an efficient algorithm that determines the best uniform scalar quantizer for a particular code. To maximize performance, it is necessary to determine degree distributions that best match the characteristics of the quantized min-sum decoder. Toward this end, an iterative optimization framework that jointly optimizes the degree distributions and the quantizer is presented},
  Doi                      = {10.1109/BSC.2006.1644559},
  File                     = {:PDF\\Memory_System_Optimization_for_FPGA-Based_Implementation_of_Quaso-Cyclic_LDPC_Codes_Decoder.pdf:PDF;:PDF\\01644559.pdf:PDF},
  Keywords                 = {AWGN channel;LDPC codes;additive white Gaussian noise;discretized min-sum decoding;iterative optimization;low-density parity-check codes;memoryless binary-input;quantization scheme;AWGN channels;binary codes;iterative decoding;memoryless systems;parity check codes;quantisation (signal);}
}

@Article{KJC1969122,
  Title                    = {On the p-rank of the incidence matrix of points and hyperplanes in a finite projective geometry},
  Author                   = {K. J. C. Smith},
  Journal                  = {Journal of Combinatorial Theory},
  Year                     = {1969},
  Number                   = {2},
  Pages                    = {122 - 129},
  Volume                   = {7},

  Abstract                 = {The rank over GF(p), or p-rank, of the incidence matrix of points and hyperplanes in the finite projective geometry $PG(t, p^n)$ is shown to be equal to $\binom{p+t-1}{t}^{n}+1$. This result is obtained by representing the points of the geometry by elements of the field $K=GF(p^{n(t+1)})$ and by representing hyperplanes in terms of linear functionals on K expressed in terms of the trace from $K$ to $GF(p^n)$.},
  Doi                      = {10.1016/S0021-9800(69)80046-3},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\KJC1969122.pdf:PDF},
  ISSN                     = {0021-9800},
  Timestamp                = {2011.11.10},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0021980069800463}
}

@Article{585771,
  Title                    = {Quick simulation: a review of importance sampling techniques in communications systems},
  Author                   = {Smith, P.J. and Shafi, M. and Hongsheng Gao},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {1997},

  Month                    = {May},
  Number                   = {4},
  Pages                    = {597-613},
  Volume                   = {15},

  Abstract                 = {Importance sampling (IS) is a simulation technique which aims to reduce the variance (or other cost function) of a given simulation estimator. In communication systems, this usually, but not always, means attempting to reduce the variance of the bit error rate (BER) estimator. By reducing the variance, IS estimators can achieve a given precision from shorter simulation runs; hence the term “quick simulation.” The idea behind IS is that certain values of the input random variables in a simulation have more impact on the parameter being estimated than others. If these “important” values are emphasized by sampling more frequently, then the estimator variance can be reduced. Hence, the basic methodology in IS is to choose a distribution which encourages the important values. This use of a “biased” distribution will, of course, result in a biased estimator if applied directly in the simulation. However, there is a simple procedure whereby the simulation outputs are weighted to correct for the use of the biased distribution, and this ensures that the new IS estimator is unbiased. Hence, the “art” of designing quick simulations via IS is entirely dependent on the choice of biased distribution. Over the last 50 years, IS techniques have flourished, but it is only in the last decade that coherent design methods have emerged. The outcome of these developments is that at the expense of increasing technical content, modern techniques can offer substantial run-time saving for a very broad range of problems. We present a comprehensive history and survey of IS methods. In addition, we offer a guide to the strengths and weaknesses of the techniques, and hence indicate which techniques are suitable for various types of communications systems. We stress that simple approaches can still yield useful savings, and so the simulation practitioner as well as the technical researcher should consider IS as a possible simulation tool},
  Doi                      = {10.1109/49.585771},
  File                     = {:PDF\\00585771.pdf:PDF},
  ISSN                     = {0733-8716},
  Keywords                 = {digital communication;digital simulation;error statistics;parameter estimation;signal sampling;simulation;statistical analysis;telecommunication computing;BER estimator;IS estimators;biased distribution;biased estimator;bit error rate;coherent design methods;cost function;digital communications systems;importance sampling techniques;input random variables;parameter estimation;quick simulation;run-time saving;simulation estimator;simulation technique;simulation tool;variance reduction;weighted simulation outputs;Bit error rate;Cost function;Design methodology;History;Monte Carlo methods;Parameter estimation;Random variables;Runtime;Sampling methods;Stress},
  Timestamp                = {2014.08.28}
}

@Book{Snygg2012,
  Title                    = {A New Approach to Differential Geometry Using Clifford's Geometric Algebra},
  Author                   = {John Snygg},
  Publisher                = {Birkhauser},
  Year                     = {2012},

  Timestamp                = {2012.09.05}
}

@Article{1190073,
  Title                    = {Reduced-complexity decoding of Q-ary LDPC codes for magnetic recording},
  Author                   = {Hongzin Song and Cruz, J.R.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {1081-1087},
  Volume                   = {39},

  Abstract                 = {Binary low-density parity-check (LDPC) codes perform very well on magnetic recording channels (MRCs) with additive white Gaussian noise (AWGN). However, an MRC is subject to other impairments, such as media defects and thermal asperities. Binary LDPC codes may not be able to cope with these impairments without the help of a Reed-Solomon code. A better form of coding may be Q-ary LDPC codes, which have been shown to outperform binary LDPC codes and Reed-Solomon codes on the AWGN channel. In this paper, we report on our investigation of Q-ary LDPC coded MRCs, both with AWGN and with burst impairments, and we present a new reduced-complexity decoding algorithm for Q-ary LDPC codes. We show that Q-ary LDPC codes outperform binary LDPC codes in the presence of burst impairments.},
  Doi                      = {10.1109/TMAG.2003.808600},
  File                     = {:PDF\\01190073.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = {AWGN channels;computational complexity;digital magnetic recording;error statistics;iterative decoding;parity check codes;AWGN channel;AWGN impairments;Q-ary LDPC codes;additive white Gaussian noise;burst impairments;iterative decoding;low-density parity-check codes;magnetic recording channels;media defects;reduced-complexity decoding algorithm;thermal asperities;AWGN channels;Additive white noise;Fading;Gaussian noise;Iterative algorithms;Iterative decoding;Magnetic recording;Matrices;Parity check codes;Thermal degradation},
  Timestamp                = {2015.05.13}
}

@Article{908351,
  Title                    = {Low density parity check codes for magnetic recording channels},
  Author                   = {Hongzin Song and Todd, R.M. and Cruz, J.R.},
  Journal                  = {Magnetics, IEEE Transactions on},
  Year                     = {2000},

  Month                    = sep,
  Number                   = {5},
  Pages                    = {2183 -2186},
  Volume                   = {36},

  Abstract                 = {We propose a system for magnetic recording, using a low density parity check (LDPC) code as the error-correcting-code, in conjunction with a rate 16/17 quasi-maximum-transition-run channel code and a modified E2PR4-equalized channel. Iterative decoding between the partial response channel and the LDPC code is performed. Simulations show that this system can achieve a 5.9 dB gain over uncoded EPR4. The algorithms used to design this LDPC code are also discussed},
  Doi                      = {10.1109/20.908351},
  File                     = {:PDF\\Low_Density_Parity_Check_Codes_for_Magnetic_Recording_Channels.pdf:PDF},
  ISSN                     = {0018-9464},
  Keywords                 = {5.9 dB;coding gain;error-correcting-code;iterative decoding;low density parity check code;magnetic recording channels;modified E2PR4-equalized channel;partial response channel;rate 16/17 quasi-maximum-transition-run channel code;channel coding;digital magnetic recording;equalisers;error correction codes;iterative decoding;partial response channels;}
}

@Article{1046102,
  Title                    = {10- and 40-Gb/s forward error correction devices for optical communications},
  Author                   = {Leilei Song and Meng-Lin Yu and Shaffer, M.S.},
  Journal                  = {Solid-State Circuits, IEEE Journal of},
  Year                     = {2002},

  Month                    = {nov.},
  Number                   = {11},
  Pages                    = { 1565 - 1573},
  Volume                   = {37},

  Doi                      = {10.1109/JSSC.2002.803931},
  ISSN                     = {0018-9200},
  Keywords                 = { 0.16 micron; 1.5 V; 2.5 to 40 Gbit/s; 343 mW; 360 mW; CMOS technology; RS decoder blocks; Reed-Solomon codes; asynchronous channels; complexity reduction; forward error correction devices; low-power parallel implementation; modified Euclidean algorithm; optical communication systems; parallel processing; performance monitoring functions; standard FEC devices; standard compliant framing; CMOS digital integrated circuits; Reed-Solomon codes; VLSI; circuit complexity; decoding; digital communication; digital signal processing chips; forward error correction; low-power electronics; monitoring; optical communication equipment; parallel algorithms; parallel architectures; telecommunication computing;}
}

@Article{4768575,
  Title                    = {A unified approach to the construction of binary and nonbinary quasi-cyclic LDPC codes based on finite fields},
  Author                   = {Shumei Song and Bo Zhou and Shu Lin and Abdel-Ghaffar, K.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {January},
  Number                   = {1},
  Pages                    = {84 -93},
  Volume                   = {57},

  Abstract                 = {A unified approach for constructing binary and nonbinary quasi-cyclic LDPC codes under a single framework is presented. Six classes of binary and nonbinary quasi-cyclic LDPC codes are constructed based on primitive elements, additive subgroups, and cyclic subgroups of finite fields. Numerical results show that the codes constructed perform well over the AWGN channel with iterative decoding.},
  Doi                      = {10.1109/TCOMM.2009.0901.060129},
  File                     = {:PDF\\A_Unified_Approach_to_the_Construction_of_Binary_and_Nonbinary_Quasi-Cyclic_LDPC_Codes_Based_on_Finite_Field.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {AWGN channel;additive white Gaussian noise channel;binary code;finite fields;iterative decoding;low density parity check code;nonbinary code;quasicyclic LDPC code;AWGN channels;binary codes;cyclic codes;iterative decoding;parity check codes;}
}

@Article{5281735,
  Title                    = {A class of quasi-cyclic LDPC codes over GF(2m)},
  Author                   = {Spagnol, C. and Marnane, W.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2009},

  Month                    = september,
  Number                   = {9},
  Pages                    = {2524 -2527},
  Volume                   = {57},

  Abstract                 = {Low Density Parity Check (LDPC) codes over GF(2m,)are an extension of binary LDPC codes. Performances of GF(2m,) LDPC codes have been shown to be higher than binary LDPC codes, but the complexity of the encoders/decoders increases. Hence there is a substantial lack of implementations for LDPC codes over GF(2m,)codes. This paper presents a class of quasi-cyclic LDPC codes over GF(2m,). These codes can alleviate the encoding/decoding complexity without excessive loss of performances. It is shown how, from a performance point of view, such codes are better than m times bigger binary codes and as good as (2m,)longer binary codes.},
  Doi                      = {10.1109/TCOMM.2009.09.070644},
  File                     = {:PDF\\05281735.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {binary codes;permutational codes;quasi-cyclic low density parity check codes;binary codes;cyclic codes;parity check codes;}
}

@InProceedings{4036023,
  Title                    = {Instanton analysis of Low-Density Parity-Check codes in the error-floor regime},
  Author                   = {Stepanov, M. and Chertkov, Michael},
  Booktitle                = {Information Theory, 2006 IEEE International Symposium on},
  Year                     = {2006},
  Month                    = {July},
  Pages                    = {552-556},

  Abstract                 = {In this paper we develop instanton method introduced in V. Chernyak et al. (2004), M.G. Stepanov et al. (2005) to analyze quantitatively performance of low-density parity-check (LDPC) codes decoded iteratively in the so-called error-floor regime. We discuss statistical properties of the numerical instanton-amoeba scheme focusing on detailed analysis and comparison of two regular LDPC codes: Tanner's [155,64,20] and Margulis' [672,336,16] codes. In the regime of moderate values of the signal-to-noise ratio we critically compare results of the instanton-amoeba evaluations against the standard Monte Carlo calculations of the frame-error-rate},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04036023.pdf:PDF},
  Keywords                 = {iterative decoding;parity check codes;statistical analysis;LDPC;error-floor regime;instanton analysis;iterative decoding;low-density parity-check codes;numerical instanton-amoeba scheme;signal-to-noise ratio;statistical properties;AWGN;Automation;Iterative decoding;Laboratories;Maximum likelihood decoding;Monte Carlo methods;Parity check codes;Performance analysis;Signal to noise ratio;Testing},
  Timestamp                = {2014.03.05}
}

@InCollection{10.1007BFb0019850,
  Title                    = {A method for finding codewords of small weight},
  Author                   = {Stern, Jacques},
  Booktitle                = {Coding Theory and Applications},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {1989},
  Editor                   = {Cohen, Gerard and Wolfmann, Jacques},
  Pages                    = {106-113},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {388},

  Doi                      = {10.1007/BFb0019850},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\10.1007BFb0019850.pdf:PDF},
  ISBN                     = {978-3-540-51643-9},
  Timestamp                = {2014.03.20},
  Url                      = {http://dx.doi.org/10.1007/BFb0019850}
}

@Article{EuclidOnECC,
  Title                    = {A method for solving key equation for decoding goppa codes},
  Author                   = {Yasuo Sugiyama and Masao Kasahara and Shigeichi Hirasawa and Toshihiko Namekawa},
  Journal                  = {Information and Control},
  Year                     = {1975},

  Month                    = {January},
  Pages                    = {87-99},
  Volume                   = {27},

  File                     = {:PDF\\A_method_for_solving_key_equation_for_decoding_Goppa_codes.pdf:PDF},
  Issue                    = {1}
}

@InProceedings{5167041,
  Title                    = {Performance Analysis of NAND Flash-Based SSD for Designing a Hybrid Filesystem},
  Author                   = {Jinsun Suk and Jaechun No},
  Booktitle                = {High Performance Computing and Communications, 2009. HPCC '09. 11th IEEE International Conference on},
  Year                     = {2009},
  Month                    = {june},
  Pages                    = {539 -544},

  Abstract                 = {As density doubles with the rapidly dropping price each year for the past seven years (currently 32 Gbits/chip), NAND flash memory has virtually replaced HDDs (hard disk drives) in battery-operated consumer devices such as cellular phones, PMPs, and PDAs. This trend has also enabled the introduction of so-called flash memory SSDs (solid state disks) that have an interface identical to that of HDDs but use NAND flash memory inside as storage media. The ordinary filesystems designed for HDDs are no longer suitable for SSDs because SSDs have many different features from HDDs. We designed a hybrid filesystem, called HybridFS, that uses two kinds of storages - HDDs and SSDs. This is accomplished by distributing data into two partitions based on their type. In HybridFS, the data blocks of a large regular file are stored in a data partition in HDDs, while the metadata being stored in the partition of SSDs. Separating data into the different storages of disk partitions makes it possible to produce high I/O performance by taking appropriate I/O approach,according to the data characteristics.},
  Doi                      = {10.1109/HPCC.2009.69},
  File                     = {:PDF\\05167041.pdf:PDF},
  Keywords                 = {HybridFS;I/O performance;NAND flash memory;NAND flash-based SSD;data block;data distribution;data partition;disk partition;hard disk drive;hybrid filesystem;memory SSD;metadata;solid state disk;storage media;disc drives;flash memories;hard discs;meta data;storage management;},
  Timestamp                = {2011.05.20}
}

@InProceedings{4253085,
  Title                    = {VLSI Decoder Architecture for High Throughput, Variable Block-size and Multi-rate LDPC Codes},
  Author                   = {Yang Sun and Karkooti, M. and Cavallaro, J.R.},
  Booktitle                = {Circuits and Systems, 2007. ISCAS 2007. IEEE International Symposium on},
  Year                     = {2007},
  Month                    = {may},
  Pages                    = {2104 -2107},

  Abstract                 = {A low-density parity-check (LDPC) decoder architecture that supports variable block sizes and multiple code rates is presented. The proposed architecture is based on the structured quasi-cyclic (QC-LDPC) codes whose performance compares favorably with that of randomly constructed LDPC codes for short to moderate block sizes. The main contribution of this work is to address the variable block-size and multi-rate decoder hardware complexity that stems from the irregular LDPC codes. The overall decoder, which was synthesized, placed and routed on TSMC 0.13-micron CMOS technology with a core area of 4.5 square millimeters, supports variable code lengths from 360 to 4200 bits and multiple code rates between frac14 and 9/10. The average throughput can achieve 1 Gbps at 2.2 dB SNR.},
  Doi                      = {10.1109/ISCAS.2007.378514},
  File                     = {:PDF\\04253085.pdf:PDF},
  Keywords                 = {0.13 micron;1 Gbit/s;360 to 4200 bit;CMOS technology;TSMC;VLSI decoder;low-density parity-check decoder;multirate LDPC codes;structured quasicyclic codes;variable block-size;variable code lengths;CMOS integrated circuits;VLSI;parity check codes;},
  Timestamp                = {2012.10.22}
}

@Article{PhysRevLett.105.120603,
  Title                    = {Markov Chain Monte Carlo Method without Detailed Balance},
  Author                   = {Suwa, Hidemaro and Todo, Synge},
  Journal                  = {Phys. Rev. Lett.},
  Year                     = {2010},

  Month                    = {Sep},
  Pages                    = {120603},
  Volume                   = {105},

  Doi                      = {10.1103/PhysRevLett.105.120603},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevLett.105.120603.pdf:PDF},
  Issue                    = {12},
  Numpages                 = {4},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.04.03},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevLett.105.120603}
}

@InProceedings{5634934,
  Title                    = {Prolongation of Lifetime and the Evaluation Method of Dependable SSD},
  Author                   = {Tai, K. and Kitakami, M.},
  Booktitle                = {Defect and Fault Tolerance in VLSI Systems (DFT), 2010 IEEE 25th International Symposium on},
  Year                     = {2010},
  Month                    = {oct},
  Pages                    = {373 -381},

  Abstract                 = {Since high-density flash memory has high error rate, strong error control is necessary for the solid-state drive (SSD). The number of erasure cycles of each memory cell is limited, where the cell should be erased before writing. Wear-leveling is used for leveling the erasure cycles in a flash memory. Since the existing wear-leveling is executed in a chip, it is not effective if write operations are concentrated into specified chips. This paper proposes wear-leveling and error control method by using redundant flash memories in order to improve reliability and lifetime of the SSD. In the proposed method, error control is usually executed, and wear-leveling among the chips is executed when the bias in the erasure cycles is large. The execution frequency of wear-leveling is adjusted considering deterioration of the cell. Evaluations of bit error rate and lifetime show that the proposed method has high reliability and durability.},
  Doi                      = {10.1109/DFT.2010.60},
  File                     = {:PDF\\05634934.pdf:PDF},
  ISSN                     = {1550-5774},
  Keywords                 = {SSD;bit error rate;erasure cycle;error control method;high-density flash memory;redundant flash memories;solid-state drive;wear-leveling;disc drives;error statistics;flash memories;},
  Timestamp                = {2011.05.20}
}

@Article{ieice_e97-a_4_975,
  Title                    = {Message Passing Decoder with Decoding on Zigzag Cycles for Non-binary LDPC Codes},
  Author                   = {Takayuki NOZAKI, Kenta KASAI, Kohichi SAKANIWA},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2014},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {975-984},
  Volume                   = {E97-A},

  Abstract                 = {In this paper, we propose a message passing decoding algorithm which lowers decoding error rates in the error floor regions for non-binary low-density parity-check (LDPC) codes transmitted over the binary erasure channel (BEC) and the memoryless binary-input output-symmetric (MBIOS) channels. In the case for the BEC, this decoding algorithm is a combination with belief propagation (BP) decoding and maximum a posteriori (MAP) decoding on zigzag cycles, which cause decoding errors in the error floor region. We show that MAP decoding on the zigzag cycles is realized by means of a message passing algorithm. Moreover, we extend this decoding algorithm to the MBIOS channels. Simulation results demonstrate that the decoding error rates in the error floor regions by the proposed decoding algorithm are lower than those by the BP decoder.},
  File                     = {:PDF\\ieice_e97-a_4_975.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2014.09.04}
}

@Article{ieice_e95-a_1_381,
  Title                    = {Analysis of Error Floors of Non-binary LDPC Codes over BEC},
  Author                   = {Takayuki NOZAKI, Kenta KASAI, Kohichi SAKANIWA},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2012},

  Month                    = {January},
  Number                   = {1},
  Pages                    = {381-390},
  Volume                   = {E95-A},

  File                     = {:PDF\\ieice_e95-a_1_381.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2014.09.04}
}

@Article{ieice_e95-a_12_2113,
  Title                    = {Analysis of Error Floors for Non-binary LDPC Codes over General Linear Group through q-Ary Memoryless Symmetric Channels},
  Author                   = {Takayuki NOZAKI, Kenta KASAI, Kohichi SAKANIWA},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2012},

  Month                    = {December},
  Number                   = {12},
  Pages                    = {2113-2121},
  Volume                   = {E95-A},

  Abstract                 = {In this paper, we compare the decoding error rates in the error floors for non-binary low-density parity-check (LDPC) codes over general linear groups with those for non-binary LDPC codes over finite fields transmitted through the q-ary memoryless symmetric channels under belief propagation decoding. To analyze non-binary LDPC codes defined over both the general linear group GL(m, F2) and the finite field F2m, we investigate non-binary LDPC codes defined over GL(m3, F2m4). We propose a method to lower the error floors for non-binary LDPC codes. In this analysis, we see that the non-binary LDPC codes constructed by our proposed method defined over general linear group have the same decoding performance in the error floors as those defined over finite field. The non-binary LDPC codes defined over general linear group have more choices of the labels on the edges which satisfy the condition for the optimization.},
  File                     = {:PDF\\ieice_e95-a_12_2113.pdf:PDF},
  Timestamp                = {2014.09.04}
}

@Article{5437480,
  Title                    = {A 32-Mb SPRAM With 2T1R Memory Cell, Localized Bi-Directional Write Driver and `1'/`0' Dual-Array Equalized Reference Scheme},
  Author                   = {Takemura, R. and Kawahara, T. and Miura, K. and Yamamoto, H. and Hayakawa, J. and Matsuzaki, N. and Ono, K. and Yamanouchi, M. and Ito, K. and Takahashi, H. and Ikeda, S. and Hasegawa, H. and Matsuoka, H. and Ohno, H.},
  Journal                  = {Solid-State Circuits, IEEE Journal of},
  Year                     = {2010},

  Month                    = april,
  Number                   = {4},
  Pages                    = {869 -879},
  Volume                   = {45},

  Abstract                 = {A 32-Mb SPin-transfer torque RAM (SPRAM) chip was demonstrated with an access time of 32 ns and a cell write-time of 40 ns at a supply voltage of 1.8 V. The chip was fabricated with 150-nm CMOS and a 100 Ã 200-nm tunnel magneto-resistive (TMR) device element. A required thermal stability of 67 of the TMR device was estimated by taking into account the disturbances during read operations and data retention periods of 10 years for nonvolatile operation. The 32-Mb SPRAM chip features three circuit technologies suitable for a large-scale array: 1) a two-transistor, one-resistor (2T1R) type memory cell for achieving a sufficiently large write current despite the small cell size, 2) a compact read/write separated hierarchy bit/source-line structure with a localized bi-directional write driver for efficiently distributing write current, and 3) a '1'/'0' dual-array equalized reference scheme for stable read operation.},
  Doi                      = {10.1109/JSSC.2010.2040120},
  File                     = {:PDF\\A_32-Mb_SPRAM_With_2T1R_Memory_Cell_Localized_Bi-Directional_Write_Driver_and_1_0_Dual-Array_Equalized_Reference_Scheme.pdf:PDF},
  ISSN                     = {0018-9200},
  Keywords                 = {2T1R memory cell;CMOS integrated circuit;SPRAM chip;bit/source-line structure;data retention periods;dual-array equalized reference scheme;localized bi-directional write driver;nonvolatile operation;one-resistor type memory cell;size 150 nm;spin-transfer torque RAM;storage capacity 32 Mbit;thermal stability;time 32 ns;time 40 ns;tunnel magneto-resistive device element;two-transistor type memory cell;voltage 1.8 V;CMOS integrated circuits;driver circuits;random-access storage;thermal stability;write-once storage;}
}

@InProceedings{6658450,
  Title                    = {Scaling challenges of NAND flash memory and hybrid memory system with storage class memory amp; NAND flash memory},
  Author                   = {Takeuchi, K.},
  Booktitle                = {Custom Integrated Circuits Conference (CICC), 2013 IEEE},
  Year                     = {2013},
  Month                    = {Sept},
  Pages                    = {1-6},

  Abstract                 = {This paper summarizes the scaling challenges of the conventional 2D floating-gate cell NAND flash memories [1, 2]. The scaling trends and limits of the bulk and SOI NAND flash memories are investigated in terms of short channel effects and channel boosting leakage from 20nm to below 10nm generation using 3D-device simulation. In the bulk NAND cell, 13nm generation is the scaling limit for realizing both channel boosting during program-inhibit and SCE suppression. The SOI NAND cell scaling limit is decreased to 8nm generation. Then, scaling problems and device design for 3D-stackable NAND flash memory are investigated [3]. Control gate length (Lg) and spacing (Lspace) are paid attention since they can be separately varied in 3D NAND and significantly affect the cell area of the 3D NAND as well as the electrical characteristics. Lg and Lspace should be the same to cope with the tradeoff between memory window and disturbance. If the number of stacked layers is 18 with the layer pitch of 40nm, the effective cell size of the 3D NAND corresponds to that of 15nm planar NAND technology. Then, this paper discusses an error prediction (EP) low density parity check (LDPC) error correcting code (ECC) which realizes an over 10-times extended lifetime [4, 5]. As the design rule shrinks, the floating gate (FG)-FG capacitive coupling among neighboring memory cells seriously degrades the memory cell reliability. The EP-LDPC ECC calibrates the inter-cell coupling without access time penalty. Finally, this paper overviews a state-of-the-art hybrid memory solution with storage class memory (SCM) and NAND flash memory for the big data solid-state storage system [5, 6]. Data fragmentation of MLC NAND flash memory is suppressed and efficient MLC NAND flash usage is realized by storing small hot data to SCM. The 3D TSV hybrid SSD realizes 11 times performance increase, 6.9 times endurance enhancement and 93% write energy reduction.},
  Doi                      = {10.1109/CICC.2013.6658450},
  File                     = {:PDF\\06658450.pdf:PDF},
  Keywords                 = {NAND circuits;error correction codes;flash memories;parity check codes;random-access storage;silicon-on-insulator;three-dimensional integrated circuits;2D floating-gate cell;3D TSV hybrid SSD;3D-device simulation;3D-stackable NAND flash memory;EP low density parity check ECC;MLC NAND flash memory;SCE suppression;SCM;SOI NAND cell scaling limit;big data solid-state storage system;bulk NAND cell;channel boosting leakage;data fragmentation;error prediction LDPC error correcting code;floating gate-FG capacitive coupling;intercell coupling;memory cell reliability;scaling challenges;short channel effects;size 13 nm;size 15 nm;size 40 nm;state-of-the-art hybrid memory solution;storage class memory;Boosting;Computer architecture;Error correction codes;Flash memories;Junctions;Parity check codes;Three-dimensional displays},
  Timestamp                = {2015.06.10}
}

@InProceedings{5993667,
  Title                    = {Green high performance storage class memory amp; NAND flash memory hybrid SSD system},
  Author                   = {Takeuchi, Ken},
  Booktitle                = {Low Power Electronics and Design (ISLPED) 2011 International Symposium on},
  Year                     = {2011},
  Month                    = {aug},
  Pages                    = {369 -370},

  Abstract                 = {SSDs and emerging storage class non-volatile semiconductor memories such as PCRAM, FeRAM, RRAM and MRAM have enabled innovations in various nano-scale VLSI memory systems for personal computers, multimedia applications and enterprise servers [1,2]. This paper provides a comprehensive review on various state-of-the-art memory system architectures and related memory circuits for the green high performance computing.},
  Doi                      = {10.1109/ISLPED.2011.5993667},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05993667.pdf:PDF},
  ISSN                     = {Pending},
  Timestamp                = {2011.08.28}
}

@Article{Takeuchi_co-design,
  Title                    = {Novel co-design of NAND Flash Memory and NAND Flash Controller Circuits for sub-30 nm Low-Power High-Speed Solid State Drives (SSD)},
  Author                   = {Ken Takeuchi},
  Journal                  = {IEEE Journal of Solid-State Circuits},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {1227-1234},
  Volume                   = {44},

  File                     = {:PDF\\Novel_co-design_of_NAND_Flash_Controller_Circuits_for_Sub-30nm_Low-Power_High-Speed_Solid-State_Drives.pdf:PDF},
  Publisher                = {IEEE}
}

@Article{s00354-008-0069-1,
  Title                    = {Decoding Algorithm of Low-density Parity-check Codes based on Bowman-Levin Approximation},
  Author                   = {Tamura, Ken-ichi and Komiya, Miho and Inoue, Masato and Kabashima, Yoshiyuki},
  Journal                  = {New Generation Computing},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {347-363},
  Volume                   = {27},

  Doi                      = {10.1007/s00354-008-0069-1},
  File                     = {:PDF\\s00354-008-0069-1.pdf:PDF},
  ISSN                     = {0288-3635},
  Keywords                 = {Bethe Free Energy; Error Correcting Code; Low-density Parity-check (LDPC) Codes; Bowman-Levin Approximation; Belief Propagation (BP)},
  Language                 = {English},
  Publisher                = {Verlag Omsha Tokio},
  Url                      = {http://dx.doi.org/10.1007/s00354-008-0069-1}
}

@InProceedings{6177074,
  Title                    = {Over-10X-extended-lifetime 76%-reduced-error solid-state drives (SSDs) with error-prediction LDPC architecture and error-recovery scheme},
  Author                   = {Tanakamaru, S. and Yanagihara, Y. and Takeuchi, K.},
  Booktitle                = {Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 2012 IEEE International},
  Year                     = {2012},
  Month                    = {feb.},
  Pages                    = {424 -426},

  Abstract                 = {This paper presents solid-state drives (SSDs) with two high reliability techniques. First, an error-prediction (EP) low-density-parity-check (LDPC) error-correcting code (ECC) that realizes an over 10X; extended lifetime. Second, an error-recovery (ER) scheme that decreases the program-disturb error rate and the data-retention error rate by 74% and 56%, respectively.},
  Doi                      = {10.1109/ISSCC.2012.6177074},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\06177074.pdf:PDF},
  ISSN                     = {0193-6530},
  Keywords                 = {data-retention error rate;error-prediction LDPC architecture;error-recovery scheme;high reliability techniques;low-density-parity-check error-correcting code;program-disturb error rate;solid-state drives;disc drives;error correction codes;integrated memory circuits;parity check codes;},
  Timestamp                = {2012.04.25}
}

@Article{1302304,
  Title                    = {On algebraic construction of Gallager and circulant low-density parity-check codes},
  Author                   = {Heng Tang and Jun Xu and Yu Kou and Lin, S. and Abdel-Ghaffar, K.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {june},
  Number                   = {6},
  Pages                    = { 1269 - 1279},
  Volume                   = {50},

  Abstract                 = { This correspondence presents three algebraic methods for constructing low-density parity-check (LDPC) codes. These methods are based on the structural properties of finite geometries. The first method gives a class of Gallager codes and a class of complementary Gallager codes. The second method results in two classes of circulant-LDPC codes, one in cyclic form and the other in quasi-cyclic form. The third method is a two-step hybrid method. Codes in these classes have a wide range of rates and minimum distances, and they perform well with iterative decoding.},
  Doi                      = {10.1109/TIT.2004.828088},
  File                     = {:PDF\\01302304.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Euclidean geometry; Gallager codes; LDPC codes; algebraic construction; circulant low-density parity-check codes; cyclic code; finite geometries; iterative decoding; projective geometry; quasicyclic code; sum-product algorithm; two-step hybrid method; cyclic codes; iterative decoding; parity check codes;},
  Timestamp                = {2011.04.15}
}

@Article{1386528,
  Title                    = {Codes on finite geometries},
  Author                   = {Tang, H. and Xu, J. and Lin, S. and Abdel-Ghaffar, K.A.S.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2005},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {572 -596},
  Volume                   = {51},

  Abstract                 = {New algebraic methods for constructing codes based on hyperplanes of two different dimensions in finite geometries are presented. The new construction methods result in a class of multistep majority-logic decodable codes and three classes of low-density parity-check (LDPC) codes. Decoding methods for the class of majority-logic decodable codes, and a class of codes that perform well with iterative decoding in spite of having many cycles of length 4 in their Tanner graphs, are presented. Most of the codes constructed can be either put in cyclic or quasi-cyclic form and hence their encoding can be implemented with linear shift registers.},
  Doi                      = {10.1109/TIT.2004.840867},
  File                     = {:PDF\\Codes_on_Finite_Geometries.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Euclidean geometry;LDPC;Tanner graphs;finite geometry;iterative decoding;linear shift registers;low-density parity-check codes;multistep majority-logic decodable codes;projective geometry;quasicyclic codes;sum product algorithm;cyclic codes;geometric codes;geometry;graph theory;iterative decoding;majority logic;parity check codes;shift registers;}
}

@Article{910591,
  Title                    = {Minimum-distance bounds by graph analysis},
  Author                   = {Tanner, R.M.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2001},

  Month                    = feb,
  Number                   = {2},
  Pages                    = {808 -821},
  Volume                   = {47},

  Abstract                 = {The parity-check matrix of a linear code is used to define a bipartite code constraint (Tanner) graph in which bit nodes are connected to parity-check nodes. The connectivity properties of this graph are analyzed using both local connectivity and the eigenvalues of the associated adjacency matrix. A simple lower bound on the minimum distance of the code is expressed in terms of the two largest eigenvalues. For a more powerful bound, local properties of the subgraph corresponding to a minimum-weight word in the code are used to create an optimization problem whose solution is a lower bound on the code's minimum distance. Linear programming gives one bound. The technique is illustrated by applying it to sparse block codes with parameters [7,3,4] and [42,23,6]},
  Doi                      = {10.1109/18.910591},
  File                     = {:PDF\\Minimum-Distance_Bounds_by_Graph_analysis.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Tanner graph;associated adjacency matrix;bipartite code constraint graph;bit nodes are;connectivity properties;eigenvalues;graph analysis;linear code;linear programming;local connectivity;lower bound;minimum-distance bounds;minimum-weight word;optimization problem;parity-check matrix;parity-check nodes;sparse block codes;subgraph;binary codes;block codes;eigenvalues and eigenfunctions;error correction codes;graph theory;linear codes;linear programming;matrix algebra;}
}

@Booklet{SAGE_Tutorial,
  Title                    = {Sage Tutorial},
  Author                   = {The Sage Development Team},
  Month                    = {July},
  Year                     = {2009}
}

@Book{978-1461269441,
  Title                    = {Gr\"{o}bner Bases: A Computational Approach to Commutative Algebra},
  Author                   = {Thomas Becker,Volker Weispfenning},
  Publisher                = {Springer},
  Year                     = {2012},

  Abstract                 = {This book provides a comprehensive treatment of Groebner bases theory embedded in an introduction to commutative algebra from a computational point of view. The centerpiece of Gr bner bases theory is the Buchberger algorithm, which provides a common generalization of the Euclidean algorithm and the Gaussian elimination algorithm to multivariate polynomial rings. The book explains how the Buchberger algorithm and the theory surrounding it are eminently important both for the mathematical theory and for computational applications. A number of results such as optimized version of the Buchberger algorithm are presented in textbook format for the first time. This book requires no prerequisites other than the mathematical maturity of an advanced undergraduate and is therefore well suited for use as a textbook. At the same time, the comprehensive treatment makes it a valuable source of reference on Groebner bases theory for mathematicians, computer scientists, and others. Placing a strong emphasis on algorithms and their verification, while making no sacrifices in mathematical rigor, the book spans a bridge between mathematics and computer science.},
  ISBN                     = {978-1461269441},
  Timestamp                = {2013.05.23}
}

@Article{1327837,
  Title                    = {Selective avoidance of cycles in irregular LDPC code construction},
  Author                   = {Tao Tian and Jones, C.R. and Villasenor, J.D. and Wesel, R.D.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {aug},
  Number                   = {8},
  Pages                    = { 1242 - 1247},
  Volume                   = {52},

  Abstract                 = { This letter explains the effect of graph connectivity on error-floor performance of low-density parity-check (LDPC) codes under message-passing decoding. A new metric, called extrinsic message degree (EMD), measures cycle connectivity in bipartite graphs of LDPC codes. Using an easily computed estimate of EMD, we propose a Viterbi-like algorithm that selectively avoids small cycle clusters that are isolated from the rest of the graph. This algorithm is different from conventional girth conditioning by emphasizing the connectivity as well as the length of cycles. The algorithm yields codes with error floors that are orders of magnitude below those of random codes with very small degradation in capacity-approaching capability.},
  Doi                      = {10.1109/TCOMM.2004.833048},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01327837.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = { LDPC code construction; Viterbi-like algorithm; bipartite graphs; capacity-approaching capability; error-floor performance; extrinsic message degree; girth conditioning; graph connectivity; graph cycles; iterative decoding; low-density parity-check codes; message-passing decoding; small cycle cluster; stopping sets; unstructured graph construction; decoding; error statistics; graph theory; message passing; parity check codes;},
  Timestamp                = {2012.04.10}
}

@Article{jphyslet:019800041018044700,
  Title                    = {On the mean field theory of mixed spin glass-ferromagnetic phases},
  Author                   = {{Toulouse, G.}},
  Journal                  = {J. Physique Lett.},
  Year                     = {1980},
  Number                   = {18},
  Pages                    = {447-449},
  Volume                   = {41},

  Doi                      = {10.1051/jphyslet:019800041018044700},
  File                     = {:PDF\\ajp-jphyslet_1980_41_18_447_0.pdf:PDF},
  Timestamp                = {2015.02.23},
  Url                      = {http://dx.doi.org/10.1051/jphyslet:019800041018044700}
}

@Article{6279525,
  Title                    = {Efficient Design and Decoding of Polar Codes},
  Author                   = {Trifonov, P.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {November},
  Number                   = {11},
  Pages                    = {3221-3227},
  Volume                   = {60},

  Abstract                 = {Polar codes are shown to be instances of both generalized concatenated codes and multilevel codes. It is shown that the performance of a polar code can be improved by representing it as a multilevel code and applying the multistage decoding algorithm with maximum likelihood decoding of outer codes. Additional performance improvement is obtained by replacing polar outer codes with other ones with better error correction performance. In some cases this also results in complexity reduction. It is shown that Gaussian approximation for density evolution enables one to accurately predict the performance of polar codes and concatenated codes based on them.},
  Doi                      = {10.1109/TCOMM.2012.081512.110872},
  File                     = {:PDF\\06279525.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {Gaussian processes;concatenated codes;design;maximum likelihood decoding;Gaussian approximation;concatenated codes;density evolution;design;maximum likelihood decoding;multilevel codes;multistage decoding algorithm;polar codes;Approximation algorithms;Concatenated codes;Constellation diagram;Error probability;Maximum likelihood decoding;Vectors;Polar codes;concatenated codes;multilevel codes}
}

@InProceedings{5424408,
  Title                    = {High density and ultra small cell size of Contact ReRAM (CR-RAM) in 90nm CMOS logic technology and circuits},
  Author                   = {Yuan Heng Tseng and Chia-En Huang and Kuo, C.-H. and Chih, Y.-D. and Chrong Jung Lin},
  Booktitle                = {Electron Devices Meeting (IEDM), 2009 IEEE International},
  Year                     = {2009},
  Month                    = {dec.},
  Pages                    = {1 -4},

  Abstract                 = {A new contact RRAM cell realized by TiN/TiON layers stacked between the W-plug and n+ diffusion region inside a small 80 Ã 80 nm contact hole is demonstrated using 90nm CMOS logic technology. This work reports the first time a resistive switching characteristics of the TiON layers sandwiched between the metal and Si substrate. The new Contact ReRAM cell exhibits highly stable read window and very small cell size of 0.19Â¿m2. By limiting the active ReRAM film in a small contact hole region, the cell effectively operates under a very low set voltage of 4V and a reset current of 150Â¿A, while achieving fast set and reset speed of less than 100ns and 10us, respectively. Excellent endurance of more than 1000k cycles and stable data retention characteristics further support the new Contact ReRAM (CR-RAM) cell will be a superior NVM technology for the future.},
  Doi                      = {10.1109/IEDM.2009.5424408},
  File                     = {:PDF\\05424408.pdf:PDF},
  Keywords                 = {CMOS logic technology;Si;Si substrate;TiN-TiON;contact ReRAM;data retention;n+ diffusion region;resistive switching;size 90 nm;ultrasmall cell size;CMOS logic circuits;random-access storage;switching;titanium compounds;},
  Timestamp                = {2011.05.23}
}

@Article{breakVGbounds,
  Title                    = {On Goppa Codes Wihch Are Better than the Varshamov-Gilbert Bound},
  Author                   = {M. Tsfasman and S.Vladut and T.Zink},
  Journal                  = {Math. Nachr.},
  Year                     = {1982},
  Pages                    = {21-28},
  Volume                   = {109},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\MANA19821090103.pdf:PDF}
}

@InProceedings{4480418,
  Title                    = {Mechanism of Hot-Electron-Induced NMOSFET's Degradation},
  Author                   = {Tsuchiya, Toshiaki},
  Booktitle                = {VLSI Technology, 1987. Digest of Technical Papers. Symposium on},
  Year                     = {1987},
  Month                    = {may},
  Pages                    = {53 -54}
}

@InProceedings{5351273,
  Title                    = {Generalized quasi-cyclic low-density parity-check codes based on finite geometries},
  Author                   = {Van, V.T. and Matsui, H. and Mita, S.},
  Booktitle                = {Information Theory Workshop, 2009. ITW 2009. IEEE},
  Year                     = {2009},
  Month                    = oct,
  Pages                    = {158 -162},

  Abstract                 = {In this study, we proved that several promising classes of codes based on finite geometries cannot be classified as quasi-cyclic (QC) codes but should be included in broader generalized quasi-cyclic (GQC) codes. Further, we proposed an algorithm (transpose algorithm) for the computation of the Grobner bases from the parity check matrices of GQC codes. Because of the GQC structure of such codes, they can be encoded systematically using GroÂ¿bner bases and their encoder can be implemented using simple feedback-shift registers. In order to demonstrate the efficiency of our encoder, we proved that the number of circuit elements in the encoder architecture is proportional to the code length for finite geometry (FG) LDPC codes. For codes constructed using points and lines of finite geometries, the hardware complexity of the serial-in serial-out encoder architecture of the codes is linear order O(n). To encode a binary codeword of length n, less than 2n adder and 3n memory elements are required.},
  Doi                      = {10.1109/ITW.2009.5351273},
  File                     = {:PDF\\05351273.pdf:PDF},
  Keywords                 = {binary codeword;circuit elements;code length;feedback-shift registers;finite geometry bLDPC codes;generalized quasi-cyclic low-density parity-check codes;hardware complexity;linear order code;parity check matrices;serial-in serial-out encoder architecture;communication complexity;cyclic codes;linear codes;matrix algebra;parity check codes;}
}

@InProceedings{5683369,
  Title                    = {A Class of Generalized Quasi-Cyclic LDPC Codes: High-Rate and Low-Complexity Encoder for Data Storage Devices},
  Author                   = {Vo Tam Van and Matsui, H. and Mita, S.},
  Booktitle                = {GLOBECOM 2010, 2010 IEEE Global Telecommunications Conference},
  Year                     = {2010},
  Month                    = dec,
  Pages                    = {1 -6},

  Abstract                 = {In this paper, we study no 4-cycle, high-rate LDPC codes based on finite geometries for use in data storage devices and prove that these codes cannot be classified as quasi-cyclic (QC) codes but should be considered as broader generalized quasi-cyclic (GQC) codes. Because of the GQC structure of such codes, they can be systematically encoded using Groebner bases and their encoder can be implemented using simple feedback-shift registers. In order to demonstrate the efficiency of the encoder, we show that the hardware complexity of the serial-in serial-out encoder architecture of these codes is of linear order O(n). To encode a binary codeword of length n, less than 2n adders and 3n memory elements are required. Furthermore, we evaluated the error performances of these codes with sum product algorithm (SPA) decoding over additive white Gaussian noise (AWGN) channels. At a bit error rate (BER) of 10-5, they perform 1-dB away from the Shannon limit after 10 decoding iterations.},
  Doi                      = {10.1109/GLOCOM.2010.5683369},
  File                     = {:PDF\\A_Class_of_generalized_quasi-cyclic_LDPC_codes_high-rate_and_low-complexity_encoder_for_data_storage_devices.pdf:PDF},
  ISSN                     = {1930-529X},
  Keywords                 = {AWGN channel;additive white Gaussian noise channel;bit error rate;data storage device;low complexity encoder;quasi cyclic LDPC codes;quasi cyclic code;AWGN channels;cyclic codes;parity check codes;}
}

@Article{ieice_trans_A_volE92_No9_2009_pp2353-2359,
  Title                    = {Computation of Gr\"{o}bner Basis for Systematic Encoding of Generalized Quasi-Cyclic Codes},
  Author                   = {Vo TAM VAN and Hajime MATSUI and Seiichi MITA},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2009},

  Month                    = {September},
  Number                   = {9},
  Pages                    = {2345--2359},
  Volume                   = {E92},

  Abstract                 = {Generalized quasi-cyclic (GQC) codes form a wide and useful class of linear codes that includes thoroughly quasi-cyclic codes, finite geometry (FG) low density parity check (LDPC) codes, and Hermitian codes. Although it is known that the systematic encoding of GQC codes is equivalent to the division algorithm in the theory of Grobner basis of modules, there has been no algorithm that computes Grobner basis for all types of GQC codes. In this paper, we propose two algorithms to compute Grobner basis for GQC codes from their parity check matrices; we call them echelon canonical form algorithm and transpose algorithm. Both algorithms require sufficiently small number of finite-field operations with the order of the third power of code-length. Each algorithm has its own characteristic. The first algorithm is composed of elementary methods and is appropriate for low-rate codes. The second algorithm is based on a novel formula and has smaller computational complexity than the first one for high-rate codes with the number of orbits (cyclic parts) less than half of the code length. Moreover, we show that a serial-in serial-out encoder architecture for FG LDPC codes is composed of linear feedback shift registers with the size of the linear order of code-length; to encode a binary codeword of length n, it takes less than 2n adder and 2n memory elements.},
  File                     = {:PDF\\ieice_trans_A_volE92_No9_2009_pp2345-2359.pdf:PDF},
  Keywords                 = {automorphism group, Buchberger's algorithm, division algorithm, circulant matrix, finite geometry low density parity check (LDPC) codes},
  Timestamp                = {2012.06.19}
}

@Article{DBLP:journals/corr/abs-0811-4033,
  Title                    = {Computation of Grobner basis for systematic encoding of generalized quasi-cyclic codes},
  Author                   = {Vo Tam Van and Hajime Matsui and Seiichi Mita},
  Journal                  = {CoRR},
  Year                     = {2008},
  Volume                   = {abs/0811.4033},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://arxiv.org/abs/0811.4033},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\0811.4033v1.pdf:PDF},
  Timestamp                = {2011.10.11}
}

@InProceedings{1023584,
  Title                    = {Combinatorial constructions of low-density parity check codes for iterative decoding},
  Author                   = {Vasic, B.},
  Booktitle                = {Information Theory, 2002. Proceedings. 2002 IEEE International Symposium on},
  Year                     = {2002},
  Pages                    = {312-},

  Abstract                 = {We introduce a combinatorial construction of regular low-density parity check (LDPC) codes based on balanced incomplete block designs, or more specifically on cyclic difference families of Abelian groups and affine geometries. Several constructions are presented, and the bounds on minimal distance are derived by using the concept of Pasch configurations.},
  Doi                      = {10.1109/ISIT.2002.1023584},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\01023584.pdf:PDF},
  Keywords                 = {combinatorial mathematics;cyclic codes;group codes;iterative decoding;parity check codes;Abelian groups;LDPC codes;Pasch configurations;affine geometries;balanced incomplete block designs;combinatorial construction;cyclic difference families;iterative decoding;low-density parity check codes;minimal distance;regular codes;Computational geometry;Hardware;Iterative decoding;Lattices;Magnetic recording;Optical design;Optical fiber communication;Parity check codes;Ultraviolet sources;Welding},
  Timestamp                = {2013.11.08}
}

@InProceedings{5394825,
  Title                    = {Trapping set ontology},
  Author                   = {Vasić, B. and Chilappagari, S.K. and Nguyen, D.V. and Planjery, S.K.},
  Booktitle                = {Communication, Control, and Computing, 2009. Allerton 2009. 47th Annual Allerton Conference on},
  Year                     = {2009},
  Month                    = {Sept},
  Pages                    = {1-7},

  Abstract                 = {The failures of iterative decoders for low-density parity-check (LDPC) codes on the additive white Gaussian noise channel (AWGNC) and the binary symmetric channel (BSC) can be understood in terms of combinatorial objects known as trapping sets. In this paper, we derive a systematic method to identify the most relevant trapping sets for decoding over the BSC in the error floor region. We elaborate on the notion of the critical number of a trapping set and derive a classification of trapping sets. We then develop the trapping set ontology, a database of trapping sets that summarizes the topological relations among trapping sets. We elucidate the usefulness of the trapping set ontology in predicting the error floor as well as in designing better codes.},
  Doi                      = {10.1109/ALLERTON.2009.5394825},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05394825.pdf:PDF},
  Keywords                 = {AWGN channels;channel coding;parity check codes;AWGNC;BSC;LDPC;additive white Gaussian noise channel;binary symmetric channel;error floor region;low-density parity-check codes;trapping set ontology;Additive white noise;Algorithm design and analysis;Databases;Degradation;Error analysis;H infinity control;Iterative algorithms;Iterative decoding;Ontologies;Parity check codes;Coding theory;iterative coding techniques},
  Timestamp                = {2014.03.04}
}

@Unpublished{Vilfan,
  Title                    = {Lecture Note On Statistical Mechanics},
  Author                   = {I. Vilfan},
  Note                     = {Lecture Note for Centre of Theoritical Physics},
  Year                     = {2002},

  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\Lecture_Note_On_Statistical_Mechanics.pdf:PDF},
  Timestamp                = {2014.06.18},
  Url                      = {http://www-f1.ijs.si/~vilfan/SM/}
}

@Article{5106465,
  Title                    = {Electric field effect on the thermal emission of traps in semiconductor junctions},
  Author                   = {Vincent, G. and Chantre, A. and Bois, D.},
  Journal                  = {Journal of Applied Physics},
  Year                     = {1979},

  Month                    = {aug },
  Number                   = {8},
  Pages                    = {5484 -5487},
  Volume                   = {50},

  Abstract                 = {Electric field effects on the thermal emission of traps in a diode have been studied. Calculations were performed and compared with experimental data on deep centers in GaAs. The results are consistent with a thermal equivalent of the optical Franz #x2010;Keldysh effect.},
  Doi                      = {10.1063/1.326601},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\05106465.pdf:PDF},
  ISSN                     = {0021-8979},
  Timestamp                = {2011.09.14}
}

@Book{viterbi2009principles,
  Title                    = {Principles of digital communication and coding},
  Author                   = {Viterbi, Andrew J and Omura, Jim K},
  Publisher                = {Courier Dover Publications},
  Year                     = {2009},

  File                     = {:PDF\\Principles_of_Digital_Communication_and_Coding.pdf:PDF},
  Timestamp                = {2015.01.21}
}

@Article{springerlink:10.1007/BF01083182,
  Title                    = {Number of points of an algebraic curve},
  Author                   = {Vladut, S. G. and Drinfel'd, V. G.},
  Journal                  = {Functional Analysis and Its Applications},
  Year                     = {1983},
  Note                     = {10.1007/BF01083182},
  Pages                    = {53-54},
  Volume                   = {17},

  File                     = {:PDF\\1007BF01083182.pdf:PDF},
  ISSN                     = {0016-2663},
  Issue                    = {1},
  Keyword                  = {Mathematics and Statistics},
  Publisher                = {Springer New York},
  Timestamp                = {2011.07.01},
  Url                      = {http://dx.doi.org/10.1007/BF01083182}
}

@Article{5464236,
  Title                    = {Low-complexity decoding for non-binary LDPC codes in high order fields},
  Author                   = {Voicila, A. and Declercq, D. and Verdier, F. and Fossorier, M. and Urard, P.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1365-1375},
  Volume                   = {58},

  Abstract                 = {In this paper, we propose a new implementation of the Extended Min-Sum (EMS) decoder for non-binary LDPC codes. A particularity of the new algorithm is that it takes into accounts the memory problem of the non-binary LDPC decoders, together with a significant complexity reduction per decoding iteration. The key feature of our decoder is to truncate the vector messages of the decoder to a limited number nm of values in order to reduce the memory requirements. Using the truncated messages, we propose an efficient implementation of the EMS decoder which reduces the order of complexity to Â¿(nm log2 nm). This complexity starts to be reasonable enough to compete with binary decoders. The performance of the low complexity algorithm with proper compensation is quite good with respect to the important complexity reduction, which is shown both with a simulated density evolution approach and actual simulations.},
  Doi                      = {10.1109/TCOMM.2010.05.070096},
  File                     = {:PDF\\05464236.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {iterative decoding;parity check codes;binary decoders;complexity reduction;decoding iteration;extended min-sum decoder;low-complexity decoding;nonbinary LDPC codes;simulated density evolution approach;truncated messages;Channel capacity;Code standards;Digital video broadcasting;Galois fields;Iterative decoding;Medical services;Modulation coding;Parity check codes;Performance gain;WiMAX;Iterative decoding, non-binary LDPC codes, low complexity algorithm},
  Timestamp                = {2015.05.13}
}

@Article{1676616,
  Title                    = {VLSI Architectures for Computing Multiplications and Inverses in GF(2m)},
  Author                   = {Wang, C.C. and Troung, T.K. and Shao, H.M. and Deutsch, L.J. and Omura, J.K. and Reed, I.S.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1985},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {709 -717},
  Volume                   = {C-34},

  Abstract                 = {Finite field arithmetic logic is central in the implementation of Reed-Solomon coders and in some cryptographic algorithms. There is a need for good multiplication and inversion algorithms that can be easily realized on VLSI chips. Massey and Omura [1] recently developed a new multiplication algorithm for Galois fields based on a normal basis representation. In this paper, a pipeline structure is developed to realize the Massey-Omura multiplier in the finite field GF(2m). With the simple squaring property of the normal basis representation used together with this multiplier, a pipeline architecture is also developed for computing inverse elements in GF(2m). The designs developed for the Massey-Omura multiplier and the computation of inverse elements are regular, simple, expandable, and therefore, naturally suitable for VLSI implementation.},
  Doi                      = {10.1109/TC.1985.1676616},
  File                     = {:PDF\\VLSI_Architectures_for_Computing_Multiplications_and_Inverses_in_GF(2^m).pdf:PDF},
  ISSN                     = {0018-9340},
  Keywords                 = {Finite field inverse;Massey-Omura multiplier;finite field multiplication;finite field multiplier;inverse;normal basis, normal basis multiplier;pipeline;systolic array;}
}

@Article{1542413,
  Title                    = {Density evolution for asymmetric memoryless channels},
  Author                   = {Wang, C.-C. and Kulkarni, S.R. and Poor, H.V.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {dec. },
  Number                   = {12},
  Pages                    = {4216 -4236},
  Volume                   = {51},

  Abstract                 = {Density evolution (DE) is one of the most powerful analytical tools for low-density parity-check (LDPC) codes and graph codes with message passing decoding algorithms. With channel symmetry as one of its fundamental assumptions, density evolution has been widely and successfully applied to different channels, including binary erasure channels (BECs), binary symmetric channels (BSCs), binary additive white Gaussian noise (BiAWGN) channels, etc. This paper generalizes density evolution for asymmetric memoryless channels, which in turn broadens the applications to general memoryless channels, e.g., z-channels, composite white Gaussian noise channels, etc. The central theorem underpinning this generalization is the convergence to perfect projection for any fixed-size supporting tree. A new iterative formula of the same complexity is then presented and the necessary theorems for the performance concentration theorems are developed. Several properties of the new density evolution method are explored, including stability results for general asymmetric memoryless channels. Simulations, code optimizations, and possible new applications suggested by this new density evolution method are also provided. This result is also used to prove the typicality of linear LDPC codes among the coset code ensemble when the minimum check node degree is sufficiently large. It is shown that the convergence to perfect projection is essential to the belief propagation (BP) algorithm even when only symmetric channels are considered. Hence, the proof of the convergence to perfect projection serves also as a completion of the theory of classical density evolution for symmetric memoryless channels.},
  Doi                      = {10.1109/TIT.2005.858931},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01542413.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {BEC;BSC;BiAWGN;belief propagation algorithm;binary additive white Gaussian noise channel;binary erasure channel;binary symmetric channel;convergence;density evolution method;graph code;iterative formula;linear LDPC code;low-density parity-check code;memoryless channel;message passing decoding algorithm;sum-product algorithm;AWGN channels;binary codes;channel coding;convergence of numerical methods;iterative decoding;linear codes;message passing;parity check codes;},
  Timestamp                = {2012.02.29}
}

@Article{PhysRevE.64.056101,
  Title                    = {Determining the density of states for classical statistical models: A random walk algorithm to produce a flat histogram},
  Author                   = {Wang, Fugao and Landau, D. P.},
  Journal                  = {Phys. Rev. E},
  Year                     = {2001},

  Month                    = {Oct},
  Pages                    = {056101},
  Volume                   = {64},

  Doi                      = {10.1103/PhysRevE.64.056101},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\cond-mat0107006v1.pdf:PDF;:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\PhysRevE.64.056101.pdf:PDF},
  Issue                    = {5},
  Numpages                 = {16},
  Publisher                = {American Physical Society},
  Timestamp                = {2012.03.27},
  Url                      = {http://link.aps.org/doi/10.1103/PhysRevE.64.056101}
}

@InProceedings{6134417,
  Title                    = {Soft Information for LDPC Decoding in Flash: Mutual-Information Optimized Quantization},
  Author                   = {Jiadong Wang and Courtade, T. and Shankar, H. and Wesel, R.D.},
  Booktitle                = {Global Telecommunications Conference (GLOBECOM 2011), 2011 IEEE},
  Year                     = {2011},
  Month                    = {dec.},
  Pages                    = {1 -6},

  Abstract                 = {High-capacity NAND flash memory can achieve high density storage by using multi-level cells (MLC) to store more than one bit per cell. Although this larger storage capacity is certainly beneficial, the increased density also increases the raw bit error rate (BER), making powerful error correction coding necessary. Traditional flash memories employ simple algebraic codes, such as BCH codes, that can correct a fixed, specified number of errors. This paper investigates the application of low-density parity-check (LDPC) codes which are well known for their ability to approach capacity in the AWGN channel. We obtain soft information for the LDPC decoder by performing multiple cell reads with distinct word-line voltages. The values of the word-line voltages (also called reference voltages) are optimized by maximizing the mutual information between the input and output of the multiple-read channel. Our results show that using this soft information in the LDPC decoder provides a significant benefit and enables us to outperform BCH codes over a range of block error rates.},
  Doi                      = {10.1109/GLOCOM.2011.6134417},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\06134417.pdf:PDF},
  ISSN                     = {1930-529X},
  Keywords                 = {AWGN channel;BCH codes;BER;LDPC decoding;NAND flash memory;algebraic codes;block error rates;block length;comparable rate;error correction coding;high density storage;low-density parity-check codes;multilevel cells;multiple-read channel;mutual-information optimized quantization;raw bit error rate;reference voltages;soft information;word-line voltages;AWGN channels;BCH codes;NAND circuits;algebraic codes;decoding;error correction codes;error statistics;flash memories;parity check codes;},
  Timestamp                = {2012.03.29}
}

@InProceedings{1493353,
  Title                    = {On importance sampling for iteratively decoded linear block codes},
  Author                   = {Nuo Wang and Srinivasan, R.},
  Booktitle                = {Communications, Circuits and Systems, 2005. Proceedings. 2005 International Conference on},
  Year                     = {2005},
  Month                    = {may},
  Pages                    = { 18 - 23 Vol. 1},
  Volume                   = {1},

  Abstract                 = { We introduce an importance sampling (IS) scheme for fast performance evaluation of the linear block codes with message-passing decoding. This novel scheme overcomes the existing difficulties in IS to some extent when code length is large and requires codebook information. Experiments show very high IS gains for iteratively decoded linear block codes such as single parity-check (SPC) codes and block product codes, as well as low density parity-check (LDPC) codes. We also provide a scheme based on our IS method to find the minimum Hamming distance and pick up those codewords which are very useful when calculating asymptotic performance.},
  Doi                      = {10.1109/ICCCAS.2005.1493353},
  File                     = {:PDF\\01493353.pdf:PDF},
  ISSN                     = { },
  Keywords                 = { LDPC codes; SPC codes; asymptotic performance; block product codes; importance sampling; iterative decoding; linear block codes; low density parity-check codes; message-passing decoding; minimum Hamming distance; performance evaluation; single parity-check codes; Hamming codes; block codes; importance sampling; iterative decoding; linear codes; message passing; parity check codes; product codes;},
  Timestamp                = {2012.09.04}
}

@Article{6868264,
  Title                    = {Privacy-Preserving Data Storage in Cloud Using Array BP-XOR Codes},
  Author                   = {Wang, Y.},
  Journal                  = {Cloud Computing, IEEE Transactions on},
  Year                     = {2014},
  Number                   = {99},
  Pages                    = {1-1},
  Volume                   = {PP},

  Abstract                 = {LDPC codes, LT codes, and digital fountain techniques have received significant attention from both academics and industry in the past few years. By employing the underlying ideas of efficient Belief Propagation (BP) decoding process in LDPC and LT codes, this paper designs the BP-XOR codes and use them to design three classes of secret sharing schemes called BP-XOR secret sharing schemes, pseudo-BP-XOR secret sharing schemes, and LDPC secret sharing schemes. By establishing the equivalence between the edge-colored graph model and degreetwo BP-XOR secret sharing schemes, we are able to design novel perfect and ideal 2-out-of-n BP-XOR secret sharing schemes. By employing techniques from array code design, we are also able to design other (n; k) threshold LDPC secret sharing schemes. In the efficient (pseudo) BP-XOR/LDPC secret sharing schemes that we will construct, only linear number of XOR (exclusive-or) operations on binary strings are required for both secret distribution phase and secret reconstruction phase. For a comparison, we should note that Shamir secret sharing schemes require O(n log n) field operations for the secret distribution phase and O(n2) field operations for the secret reconstruction phase. Furthermore, our schemes achieve the optimal update complexity for secret sharing schemes. By update complexity for a secret sharing scheme, we mean the average number of bits in the participant’s shares that needs to be revised when certain bit of the master secret is changed. The extremely efficient secret sharing schemes discussed in this paper could be used for massive data storage in cloud environments achieving privacy and reliability without employing encryption techniques.},
  Doi                      = {10.1109/TCC.2014.2344662},
  File                     = {:PDF\\06868264.pdf:PDF},
  ISSN                     = {2168-7161},
  Timestamp                = {2015.06.11}
}

@InProceedings{6620241,
  Title                    = {Array BP-XOR codes for reliable cloud storage systems},
  Author                   = {Yongge Wang},
  Booktitle                = {Information Theory Proceedings (ISIT), 2013 IEEE International Symposium on},
  Year                     = {2013},
  Month                    = {July},
  Pages                    = {326-330},

  Abstract                 = {Low Density Parity Check (LDPC) codes such as LT codes have received significant attention from both academics and industry in the past few years. By employing the underlying ideas of efficient Belief Propagation (BP) decoding process in LT codes, this paper introduces array BP-XOR codes and shows the equivalence between the edge-colored graph model and degree-one-and-two encoding symbol based array BP-XOR codes. Using this equivalence result, novel [n, n-2] and [n, 2] MDS array BP-XOR codes are designed in this paper.},
  Doi                      = {10.1109/ISIT.2013.6620241},
  File                     = {:PDF\\06620241.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {belief maintenance;cloud computing;decoding;graph colouring;parity check codes;redundancy;software fault tolerance;storage management;BP decoding process;LDPC;LT codes;belief propagation decoding process;cloud storage system reliability;degree-one-and-two encoding symbol based array BP-XOR codes;edge-colored graph model;fault tolerance;low density parity check codes;minimal data storage system redundancy;Arrays;Color;Cryptography;Encoding;Generators;Parity check codes},
  Timestamp                = {2015.06.11}
}

@InProceedings{6970885,
  Title                    = {Efficient secret sharing schemes achieving optimal information rate},
  Author                   = {Yongge Wang and Desmedt, Y.},
  Booktitle                = {Information Theory Workshop (ITW), 2014 IEEE},
  Year                     = {2014},
  Month                    = {Nov},
  Pages                    = {516-520},

  Abstract                 = {One of the important problems in secret sharing schemes is to establish bounds on the size of the shares to be given to participants in secret sharing schemes. The other important problem in secret sharing schemes is to reduce the computational complexity in both secret distribution phase and secret reconstruction phase. In this paper, we design efficient threshold (n, k) secret sharing schemes to achieve both of the above goals. In particular, we show that if the secret size |s| is larger than max{1 + log2 n, n(n - k)/(n - 1)}, then ideal secret sharing schemes exist. In the efficient ideal secret sharing schemes that we will construct, only XOR-operations on binary strings are required (which is the best we could achieve). These schemes will have many applications both in practice and in theory. For example, they could be used to design very efficient verifiable secret sharing schemes which will have broad applications in secure multi-party computation and could be used to design efficient privacy preserving data storage in cloud systems.},
  Doi                      = {10.1109/ITW.2014.6970885},
  File                     = {:PDF\\06970885.pdf:PDF},
  ISSN                     = {1662-9019},
  Keywords                 = {cloud computing;computational complexity;data privacy;storage management;binary strings;cloud systems;computational complexity;optimal information rate;privacy preserving data storage;secret distribution phase;secret reconstruction phase;secret sharing schemes;secure multiparty computation;Arrays;Cryptography;Generators;Information rates;Polynomials;Reed-Solomon codes},
  Timestamp                = {2015.06.11}
}

@Article{4114369,
  Title                    = {Low-Complexity High-Speed Decoder Design for Quasi-Cyclic LDPC Codes},
  Author                   = {Zhongfeng Wang and Zhiqiang Cui},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2007},

  Month                    = jan.,
  Number                   = {1},
  Pages                    = {104 -114},
  Volume                   = {15},

  Abstract                 = {This paper studies low-complexity high-speed decoder architectures for quasi-cyclic low density parity check (QC-LDPC) codes. Algorithmic transformation and architectural level optimization are incorporated to reduce the critical path. Enhanced partially parallel decoding architectures are proposed to linearly increase the throughput of conventional partially parallel decoders through introducing a small percentage of extra hardware. Based on the proposed architectures, a (8176, 7154) Euclidian geometry-based QC-LDPC code decoder is implemented on Xilinx field programmable gate array (FPGA) Virtex-II 6000, where an efficient nonuniform quantization scheme is employed to reduce the size of memories storing soft messages. FPGA implementation results show that the proposed decoder can achieve a maximum (source data) decoding throughput of 172 Mb/s at 15 iterations},
  Doi                      = {10.1109/TVLSI.2007.891098},
  File                     = {:PDF\\Low-Complexity_High-Speed_Decoder_Design_for_Quasi-Cyclic_LDPC_Codes.pdf:PDF},
  ISSN                     = {1063-8210},
  Keywords                 = {LDPC codes;error correction codes;field programmable gate arrays;low density parity check codes;parallel processing;quasi-cyclic codes;cyclic codes;error correction codes;field programmable gate arrays;parallel processing;parity check codes;quantisation (signal);}
}

@InProceedings{5205972,
  Title                    = {On the capacity of bounded rank modulation for flash memories},
  Author                   = {Zhiying Wang and Anxiao Jiang and Bruck, J.},
  Booktitle                = {Information Theory, 2009. ISIT 2009. IEEE International Symposium on},
  Year                     = {2009},
  Month                    = {June},
  Pages                    = {1234-1238},

  Abstract                 = {Rank modulation has been introduced as a new information representation scheme for flash memories. Given the charge levels of a group of flash cells, sorting is used to induce a permutation, which in turn represents data. Motivated by the lower sorting complexity of smaller cell groups, we consider bounded rank modulation, where a sequence of permutations of given sizes are used to represent data. We study the capacity of bounded rank modulation under the condition that permutations can overlap for higher capacity.},
  Doi                      = {10.1109/ISIT.2009.5205972},
  File                     = {:PDF\\05205972.pdf:PDF},
  Keywords                 = {flash memories;modulation;bounded rank modulation capacity;flash memories;information representation scheme;lower sorting complexity;Computer science;Error correction codes;Flash memory;Hardware;Information representation;Modulation coding;Nonvolatile memory;Robustness;Sorting;Tunneling},
  Timestamp                = {2014.09.03}
}

@InProceedings{5599244,
  Title                    = {FlashCoop: A Locality-Aware Cooperative Buffer Management for SSD-Based Storage Cluster},
  Author                   = {Qingsong Wei and Bozhao Gong and Pathak, S. and Tay, Y.C.},
  Booktitle                = {Parallel Processing (ICPP), 2010 39th International Conference on},
  Year                     = {2010},
  Month                    = {sept.},
  Pages                    = {634 -643},

  Abstract                 = {Random writes significantly limit the application of flash-based Solid State Drive (SSD) in enterprise environment due to its poor latency, negative impact on SSD lifetime and high garbage collection overhead. To release above limitations, we propose a locality-aware cooperative buffer scheme referred to as FlashCoop (Flash Cooperation), which leverages free memory of neighboring storage server to buffer writes over high speed network. Both temporal and sequential localities of access pattern are exploited in the design of cooperative buffer management. Leveraging the filtering effect of the cooperative buffer, FlashCoop can efficiently shape the I/O request stream and improve the sequentiality of the write accesses passed to the SSD. FlashCoop has been extensively evaluated under various enterprise workloads. Our benchmark results conclusively demonstrate that FlashCoop can achieve 52.3% performance improvement and 56.5% garbage collection overhead reduction compared to the system without FlashCoop.},
  Doi                      = {10.1109/ICPP.2010.71},
  File                     = {:PDF\\05599244.pdf:PDF},
  ISSN                     = {0190-3918},
  Keywords                 = {FlashCoop;I/O request stream;SSD based storage cluster;flash based solid state drive;flash cooperation;high garbage collection overhead;locality aware cooperative buffer scheme;neighboring storage server;random writes;buffer storage;flash memories;},
  Timestamp                = {2011.05.20}
}

@Article{1055987,
  Title                    = {Continued fractions and Berlekamp's algorithm},
  Author                   = { Welch, L. and Scholtz, R.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1979},

  Month                    = jan,
  Number                   = {1},
  Pages                    = { 19 - 27},
  Volume                   = {25},

  Abstract                 = { Theorems are presented concerning the optimality of rational approximations using non-Archimedean norms. The algorithm for developing the rational approximations is based on continued fraction techniques and is virtually equivalent to an algorithm employed by Berlekamp for decoding BCH codes. Several variations of the continued fraction technique and Berlekamp's algorithm are illustrated on a common example.},
  Doi                      = {10.1109/TIT.1979.1055987},
  ISSN                     = {0018-9448},
  Keywords                 = { Approximation methods; BCH codes; Continued fractions; Decoding;}
}

@Article{1054128,
  Title                    = {New generalizations of the Reed-Muller codes--II: Nonprimitive codes},
  Author                   = { Weldon, E., Jr.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1968},

  Month                    = mar,
  Number                   = {2},
  Pages                    = { 199 - 205},
  Volume                   = {14},

  Abstract                 = { In this paper a class of nonprimitive cyclic codes quite similar in structure to the original Reed-Muller codes is presented. These codes, referred to herein as nonprimitive Reed-Muller codes, are shown to possess many of the properties of the primitive codes. Specifically, two major results are presented. First the code length, number of information symbols, and minimum distance are shown to be related by means of a parameter known as the order of the code. These relationships show that for given values of code length and rate the codes have relatively large minimum distances. It is also shown that the codes are subcodes of the BCH codes of the same length and guaranteed minimum distance; thus in general the codes are not as powerful as the BCH codes. However, for most interesting values of code length and rate the difference between the two types of codes is slight. The second result is the observation that the codes can be decoded with a variation of the original algorithm proposed by Reed for the Reed-Muller codes. In other words, they areL-step orthogonalizable. Because of their large minimum distances and the simplicity of their decoders, nonprimitive Reed-Muller codes seem attractive for use in error-control systems requiring multiple random-error correction.},
  Doi                      = {10.1109/TIT.1968.1054128},
  File                     = {:C\:\\Users\\Public\\Documents\\My eBooks\\refereces\\PDF\\01054128.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Cyclic codes; Reed-Muller codes;}
}

@InCollection{welling2002new,
  Title                    = {A new learning algorithm for mean field Boltzmann machines},
  Author                   = {Welling, Max and Hinton, Geoffrey E},
  Booktitle                = {Artificial Neural Networks—ICANN 2002},
  Publisher                = {Springer},
  Year                     = {2002},
  Pages                    = {351--357},

  File                     = {:PDF\\tr01-002.pdf:PDF},
  Timestamp                = {2015.07.30}
}

@Article{Wood1989277,
  Title                    = {Spinor groups and algebraic coding theory },
  Author                   = {Jay A Wood},
  Journal                  = {Journal of Combinatorial Theory, Series A },
  Year                     = {1989},
  Number                   = {2},
  Pages                    = {277 - 313},
  Volume                   = {51},

  Abstract                 = {The purpose of this paper is to explore the equivalence between the abelian subgroups of Ṽ(n), the “diagonal” extra-special 2-group of the compact, simple, simply-connected Lie group Spin(n), and the self-orthogonal linear binary codes of algebraic coding theory. In particular, the basic abstract structure theory of the abelian subgroups of Ṽ(n) is reflected in the distinction between ordinary self-orthogonal codes and even self-orthogonal codes. Work of Quillen on the equivariant cohomology of Spin(n) affects the classification of even self-orthogonal codes. },
  Doi                      = {http://dx.doi.org/10.1016/0097-3165(89)90053-8},
  File                     = {:PDF\\Wood1989277.pdf:PDF},
  ISSN                     = {0097-3165},
  Timestamp                = {2013.06.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0097316589900538}
}

@InProceedings{SVM-KBA2,
  Title                    = {Adaptive Feature-Space Conformal Transformation for Imbalanced Data Learning},
  Author                   = {Wu, Gang and Chang, Edward Y.},
  Booktitle                = {Proceedings of the Twentieth International Conference on Machine Learning},
  Year                     = {2003},
  Number                   = {2},
  Pages                    = {816--823},
  Volume                   = {20},

  Citeulike-article-id     = {10229830},
  File                     = {:PDF\\ICML03-106.pdf:PDF},
  Keywords                 = {imbalanced, multiple-kernel-learning, svm},
  Posted-at                = {2012-01-16 10:58:28},
  Timestamp                = {2015.07.23}
}

@Article{raey,
  Title                    = {Conformal Transformation of Kernel Functions: A Data-Dependent Way to Improve Support Vector Machine Classifiers},
  Author                   = {Wu, Si and Amari, Shun-Ichi},
  Journal                  = {Neural Processing Letters},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {59-67},
  Volume                   = {15},

  Doi                      = {10.1023/A:1013848912046},
  File                     = {:PDF\\10.1023A1013848912046.pdf:PDF},
  ISSN                     = {1370-4621},
  Keywords                 = {kernel function; pattern recognition; Riemannian geometry and conformal transformation; support vector machine},
  Language                 = {English},
  Publisher                = {Kluwer Academic Publishers},
  Timestamp                = {2015.07.23}
}

@Article{Wu:2010:DEH:1880037.1880040,
  Title                    = {Design exploration of hybrid caches with disparate memory technologies},
  Author                   = {Wu, Xiaoxia and Li, Jian and Zhang, Lixin and Speight, Evan and Rajamony, Ram and Xie, Yuan},
  Journal                  = {ACM Trans. Archit. Code Optim.},
  Year                     = {2010},

  Month                    = {December},
  Pages                    = {15:1--15:34},
  Volume                   = {7},

  Acmid                    = {1880040},
  Address                  = {New York, NY, USA},
  Articleno                = {15},
  Doi                      = {http://doi.acm.org/10.1145/1880037.1880040},
  File                     = {:PDF\\a15-wu.pdf:PDF},
  ISSN                     = {1544-3566},
  Issue                    = {3},
  Issue_date               = {December 2010},
  Keywords                 = {Hybrid cache architecture, Three-dimensional IC design, cache hierarchy, embedded DRAM, magnetic RAM, nonuniform cache architecture, phase-change RAM, power, thermal},
  Numpages                 = {34},
  Publisher                = {ACM},
  Timestamp                = {2011.06.03},
  Url                      = {http://doi.acm.org/10.1145/1880037.1880040}
}

@InProceedings{5513379,
  Title                    = {Low complexity codes for writing a write-once memory twice},
  Author                   = {Wu, Y.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {1928-1932},

  Abstract                 = {A write-once memory (wom) is a storage medium formed by a number of “write-once” binary cells, where each cell initially is in a `0' state and can be changed to a `1' state irreversibly. Examples of write-once memories include SLC flash memories and optical disks. This paper presents some low complexity codes for writing such write-once memories twice.},
  Doi                      = {10.1109/ISIT.2010.5513379},
  File                     = {:PDF\\05513379.pdf:PDF},
  Keywords                 = {codes;computational complexity;optical disc storage;write-once storage;SLC flash memories;low-complexity codes;optical disks;storage medium;write-once binary cells;write-once memory;Bipartite graph;Block codes;Decoding;Delay;Encoding;Error analysis;Flash memory;Measurement;Vectors;Writing},
  Timestamp                = {2015.06.08}
}

@Article{5773064,
  Title                    = {Position Modulation Code for Rewriting Write-Once Memories},
  Author                   = {Yunnan Wu and Anxiao Jiang},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {3692-3697},
  Volume                   = {57},

  Abstract                 = {A write-once memory (wom) is a storage medium formed by a number of “write-once” bit positions (wits), where each wit initially is in a “0” state and can be changed to a “1” state irreversibly. Examples of write-once memories include SLC flash memories and optical disks. This paper presents a low complexity coding scheme for rewriting such write-once memories, which is applicable to general problem configurations. The proposed scheme is called the position modulation code, as it uses the positions of the zero symbols to encode some information. The proposed technique can achieve code rates higher than state-of-the-art practical solutions for some configurations. For instance, there is a position modulation code that can write 56 bits 10 times on 278 wits, achieving rate 2.01. In addition, the position modulation code is shown to achieve a rate at least half of the optimal rate.},
  Doi                      = {10.1109/TIT.2011.2134370},
  File                     = {:PDF\\05773064.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {flash memories;modulation coding;optical disc storage;SLC flash memories;low complexity coding scheme;optical disks;position modulation code;write-once memories;Ash;Complexity theory;Decoding;Encoding;Indexes;Modulation;Polynomials;Flash memories;position modulation;write-once memories},
  Timestamp                = {2015.06.08}
}

@InProceedings{1312606,
  Title                    = {Log-domain decoding of LDPC codes over GF(q)},
  Author                   = {Wymeersch, H. and Steendam, H. and Moeneclaey, M.},
  Booktitle                = {Communications, 2004 IEEE International Conference on},
  Year                     = {2004},
  Month                    = {June},
  Pages                    = {772-776 Vol.2},
  Volume                   = {2},

  Abstract                 = {This paper introduces a log-domain decoding scheme for LDPC codes over GF(q). While this scheme is mathematically equivalent to the conventional sum-product decoder, log-domain decoding has advantages in terms of implementation, computational complexity and numerical stability. Further, a suboptimal variant of the log-domain decoding algorithm is proposed, yielding a lower computational complexity. The proposed algorithms and the sum-product algorithm are compared both in terms of simulated BER performance and computational complexity.},
  Doi                      = {10.1109/ICC.2004.1312606},
  File                     = {:PDF\\01312606.pdf:PDF},
  Keywords                 = {Galois fields;computational complexity;decoding;error statistics;matrix algebra;numerical stability;parity check codes;BER performance;LDPC codes;computational complexity;conventional sum-product decoder;log-domain decoding;Bit error rate;Computational complexity;Computational modeling;Decoding;Galois fields;Numerical stability;Parity check codes;Quantization;Sparse matrices;Sum product algorithm},
  Timestamp                = {2015.04.21}
}

@InProceedings{1204566,
  Title                    = {On importance sampling for linear block codes},
  Author                   = {Bo Xia and Ryan, W.E.},
  Booktitle                = {Communications, 2003. ICC '03. IEEE International Conference on},
  Year                     = {2003},
  Month                    = {may},
  Pages                    = { 2904 - 2908 vol.4},
  Volume                   = {4},

  Abstract                 = {We introduce an importance sampling scheme for linear block codes with message-passing decoding. This novel scheme overcomes an existing difficulty in the IS practice that requires codebook information. Experiments show large IS gains for single parity-check codes and short-length block codes. For medium-length block codes, IS gains in the order of 103 and higher are observed at high signal-to-noise ratio.},
  Doi                      = {10.1109/ICC.2003.1204566},
  File                     = {:PDF\\01204566.pdf:PDF},
  Keywords                 = { codebook information; linear block codes; medium-block length codes; message-passing decoding; sampling scheme; short-length block codes; signal-to-noise ratios; single-parity check codes; block codes; decoding; linear codes; message passing; noise; parity check codes; sampling methods;},
  Timestamp                = {2012.09.04}
}

@Article{5089484,
  Title                    = {Error rate estimation of low-density parity-check codes on binary symmetric channels using cycle enumeration},
  Author                   = {Hua Xiao and Banihashemi, A.H.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2009},
  Number                   = {6},
  Pages                    = {1550-1555},
  Volume                   = {57},

  Abstract                 = {The performance of low-density parity-check (LDPC) codes decoded by hard-decision iterative decoding algorithms can be accurately estimated if the weight J and the number |EJ| of the smallest error patterns that cannot be corrected by the decoder are known. To obtain J and |EJ|, one would need to perform the direct enumeration of error patterns with weight i les J. The complexity of enumeration increases exponentially with J, essentially as nJ, where n is the code block length. This limits the application of direct enumeration to codes with small n and J. In this letter, we approximate J and |EJ | by enumerating and testing the error patterns that are subsets of short cycles in the code's Tanner graph. This reduces the computational complexity by several orders of magnitude compared to direct enumeration, making it possible to estimate the error rates for almost any practical LDPC code. To obtain the error rate estimates, we propose an algorithm that progressively improves the estimates as larger cycles are enumerated. Through a number of examples, we demonstrate that the proposed method can accurately estimate both the bit error rate (BER) and the frame error rate (FER) of regular and irregular LDPC codes decoded by a variety of hard-decision iterative decoding algorithms.},
  Doi                      = {10.1109/TCOMM.2009.06.070048},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\05089484.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {binary codes;computational complexity;error statistics;iterative decoding;parity check codes;LDPC code;Tanner graph;binary symmetric channels;bit error rate;code block length;computational complexity;cycle enumeration;direct enumeration;error patterns;error rate estimation;frame error rate;hard decision iterative decoding;low density parity check codes;Bit error rate;Communications Society;Computational complexity;Error analysis;Error correction codes;Estimation error;Iterative algorithms;Iterative decoding;Parity check codes;Testing;Binary symmetric channels (BSC), low-density parity-check (LDPC) codes, finite-length LDPC codes, error rate estimation of finite-length LDPC codes, error floor, hard-decision decoding algorithms, iterative decoding, Tanner graph cycles.},
  Timestamp                = {2013.12.17}
}

@InProceedings{4259758,
  Title                    = {Estimation of Bit and Frame Error Rates of Low-Density Parity-Check Codes on Binary Symmetric Channels},
  Author                   = {Hua Xiao and Banihashemi, A.H.},
  Booktitle                = {Information Theory, 2007. CWIT '07. 10th Canadian Workshop on},
  Year                     = {2007},
  Pages                    = {73-76},

  Abstract                 = {A method for estimating the performance of low-density parity-check (LDPC) codes decoded by hard-decision iterative decoding algorithms on binary symmetric channels (BSC) is proposed. Based on the enumeration of the smallest weight error patterns that cannot be all corrected by the decoder, this method estimates both the frame error rate (FER) and the bit error rate (BER) of a given LDPC code with very good precision for all crossover probabilities of practical interest. Through a number of examples, we show that the proposed method can be effectively applied to both regular and irregular LDPC codes and to a variety of hard-decision iterative decoding algorithms. Compared with the conventional Monte Carlo simulation, the proposed method has a much smaller computational complexity, particularly for lower error rates.},
  Doi                      = {10.1109/CWIT.2007.375704},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\04259758.pdf:PDF},
  Keywords                 = {Monte Carlo methods;binary codes;channel coding;error statistics;iterative decoding;parity check codes;Monte Carlo simulation;binary symmetric channel;bit error rate;channel coding;frame error rate;iterative decoding algorithm;low-density parity-check codes;Bit error rate;Computational complexity;Error analysis;Error correction;Error correction codes;Iterative algorithms;Iterative decoding;Parity check codes;Performance analysis},
  Timestamp                = {2013.12.20}
}

@Article{5352236,
  Title                    = {Design of Last-Level On-Chip Cache Using Spin-Torque Transfer RAM (STT RAM)},
  Author                   = {Wei Xu and Hongbin Sun and Xiaobin Wang and Yiran Chen and Tong Zhang},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {march },
  Number                   = {3},
  Pages                    = {483 -493},
  Volume                   = {19},

  Abstract                 = {Because of its high storage density with superior scalability, low integration cost and reasonably high access speed, spin-torque transfer random access memory (STT RAM) appears to have a promising potential to replace SRAM as last-level on-chip cache (e.g., L2 or L3 cache) for microprocessors. Due to unique operational characteristics of its storage device magnetic tunneling junction (MTJ), STT RAM is inherently subject to a write latency versus read latency tradeoff that is determined by the memory cell size. This paper first quantitatively studies how different memory cell sizing may impact the overall computing system performance, and shows that different computing workloads may have conflicting expectations on memory cell sizing. Leveraging MTJ device switching characteristics, we further propose an STT RAM architecture design method that can make STT RAM cache with relatively small memory cell size perform well over a wide spectrum of computing benchmarks. This has been well demonstrated using CACTI-based memory modeling and computing system performance simulations using SimpleScalar. Moreover, we show that this design method can also reduce STT RAM cache energy consumption by up to 30% over a variety of benchmarks.},
  Doi                      = {10.1109/TVLSI.2009.2035509},
  File                     = {:PDF\\05352236.pdf:PDF},
  ISSN                     = {1063-8210},
  Keywords                 = {CACTI-based memory modeling;STT RAM;SimpleScalar;high access speed;last-level on-chip cache;magnetic tunneling junction;random access memory;spin-torque transfer RAM;storage device;magnetic tunnelling;random-access storage;},
  Timestamp                = {2011.06.03}
}

@Article{5497218,
  Title                    = {A Time-Aware Fault Tolerance Scheme to Improve Reliability of Multilevel Phase-Change Memory in the Presence of Significant Resistance Drift},
  Author                   = {Xu, W. and Zhang, T.},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2010},
  Number                   = {99},
  Pages                    = {1 -11},
  Volume                   = {PP},

  Abstract                 = {Because of its promising scalability potential and support of multilevel per cell storage, phase-change memory has become a topic of great current interest. However, recent studies show that structural relaxation effect makes the resistance of phase-change material drift over the time, which can severely degrade multilevel per cell phase-change memory storage reliability. This makes powerful memory fault tolerance solutions indispensable, where error correction code (ECC) will play an essential role. This work aims to develop fault tolerance solutions that can effectively compensate memory cell resistance drift. First, based upon information-theoretical study, we show that conventional use of ECC, which is unaware of memory content lifetime, can only achieve the performance with a big gap from the information-theoretical bounds. This motivates us to study the potential of time-aware memory fault tolerance, where the basic idea is to keep track the memory content lifetime and use this lifetime information to accordingly adjust how memory cell resistance is quantized and interpreted for ECC decoding. Under this time-aware fault tolerance framework, we study the use of two types of ECCs, including classical codes such as BCH that only demand hard-decision input and advanced codes such as low-density parity-check (LDPC) codes that demand soft-decision probability input. Using hypothetical four-level per cell and eight-level per cell phase-change memory with BCH and LDPC codes as test vehicles, we carry out extensive analysis and simulations, which demonstrate very significant performance advantages of such time-aware memory fault tolerance strategy in the presence of significant memory cell resistance drift.},
  Doi                      = {10.1109/TVLSI.2010.2052640},
  File                     = {:PDF\\A_Time-Aware_Fault_Tolerance_Scheme_to_Improve_Reliability_of_Multilevel_Phase-Change_Memory_in_the_Presence_of_Significant_Resistance_Drift.pdf:PDF},
  ISSN                     = {1063-8210}
}

@Article{6203417,
  Title                    = {Codes for Write-Once Memories},
  Author                   = {Yaakobi, E. and Kayser, S. and Siegel, P.H. and Vardy, A. and Wolf, J.K.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {5985-5999},
  Volume                   = {58},

  Abstract                 = {A write-once memory (WOM) is a storage device that consists of cells that can take on q values, with the added constraint that rewrites can only increase a cell's value. A length- n , t -write WOM-code is a coding scheme that allows t messages to be stored in n cells. If on the i th write we write one of M_{i} messages, then the rate of this write is the ratio of the number of written bits to the total number of cells, i.e., \log _{2}M_{i}/n . The sum-rate of the WOM-code is the sum of all individual rates on all writes. A WOM-code is called a fixed-rate WOM-code if the rates on all writes are the same, and otherwise, it is called a variable-rate WOM-code. We address two different problems when analyzing the sum-rate of WOM-codes. In the first one, called the fixed-rate WOM-code problem, the sum-rate is analyzed over all fixed-rate WOM-codes, and in the second problem, called the unrestricted-rate WOM-code problem, the sum-rate is analyzed over all fixed-rate and variable-rate WOM-codes. In this paper, we first present a family of two-write WOM-codes. The construction is inspired by the coset coding scheme, which was used to construct multiple-write WOM-codes by Cohen and recently by Wu, in order to construct from each linear code a two-write WOM-code. This construction improves the best known sum-rates for the fixed- and unrestricted-rate WOM-code problems. We also show how to take advantage of two-write WOM-codes in order - o construct codes for the Blackwell channel. The two-write construction is generalized for two-write WOM-codes with q levels per cell, which is used with ternary cells to construct three- and four-write binary WOM-codes. This construction is used recursively in order to generate a family of t -write WOM-codes for all t . A further generalization of these t -write WOM-codes yields additional families of efficient WOM-codes. Finally, we show a recursive method that uses the previously constructed WOM-codes in order to construct fixed-rate WOM-codes. We conclude and show that the WOM-codes constructed here outperform all previously known WOM-codes for 2\leq\slant t\leq\slant 10 for both the fixed- and unrestricted-rate WOM-code problems.},
  Doi                      = {10.1109/TIT.2012.2200291},
  File                     = {:PDF\\06203417.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {Decoding;Educational institutions;Linear code;Radio frequency;Upper bound;Vectors;Coding theory;WOM-codes;flash memories;write-once memories (WOMs)},
  Timestamp                = {2015.05.27}
}

@InProceedings{5513373,
  Title                    = {Multiple error-correcting WOM-codes},
  Author                   = {Yaakobi, E. and Siegel, P.H. and Vardy, A. and Wolf, J.K.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {1933-1937},

  Abstract                 = {A Write Once Memory (WOM) is a storage medium with binary memory elements, called cells, that can change from the zero state to the one state only once. Examples of WOMs are punch cards, optical disks, and more recently flash memories. WOM-codes were first presented by Rivest and Shamir and are designed for efficiently storing and updating data in the WOM. A WC[n, k, t] WOM-Code CW is a coding scheme for storing k information bits in n cells t times. At each write, the state of each cell can be changed, provided that the cell is changed from the zero state to the one state. The WOM-Rate of CW, defined to be Rt(CW) = kt/n, indicates the total amount of information that is possible to store in a cell in t writes. Two WOM-code constructions that can correct a single cell-error were presented by Zémor and Cohen. In this paper, we present another construction of a single-error-correcting WOM-codes with a better WOM-rate. Our construction can be adjusted also for single-error-detection, double-error-correction, and triple-error-correction. For the latter case, we use triple-error-correcting BCH-like codes, which were showed by Kasami and more recently described again by Bracken and Helleseth.},
  Doi                      = {10.1109/ISIT.2010.5513373},
  File                     = {:PDF\\05513373.pdf:PDF},
  Keywords                 = {BCH codes;error correction codes;write-once storage;binary memory element;double-error-correction;multiple error-correcting WOM-codes;single-error-correcting WOM-code;storage medium;triple-error-correcting BCH-like code;write once memory;Atom optics;Electron optics;Flash memory;Lifting equipment;Linear code;Nonvolatile memory},
  Timestamp                = {2015.06.09}
}

@InProceedings{843915,
  Title                    = {Analysis of detrap current due to oxide traps to improve flash memory retention},
  Author                   = {Yamada, R. and Mori, Y. and Okuyama, Y. and Yugami, J. and Nishimoto, T. and Kume, H.},
  Booktitle                = {Reliability Physics Symposium, 2000. Proceedings. 38th Annual 2000 IEEE International},
  Year                     = {2000},
  Pages                    = {200 -204},

  Doi                      = {10.1109/RELPHY.2000.843915},
  Keywords                 = {1 year;Fowler-Nordheim stressing;MOS capacitors;MOSFET;Si-SiO2;conduction mechanism;constant current stress;constant voltage stress;deep traps;detrap current;direct tunneling;electron injection;flash memory retention characteristics;hole injection;metal-oxide-semiconductor structures;oxide conduction band;oxide traps;shallow traps;thermally excited electron tunneling;threshold voltage shift;MOS capacitors;MOSFET;electron traps;flash memories;hole traps;integrated circuit reliability;interface states;tunnelling;}
}

@InProceedings{934976,
  Title                    = {A novel analysis method of threshold voltage shift due to detrap in a multi-level flash memory},
  Author                   = {Yamada, R. and Sekiguchi, T. and Okuyama, Y. and Yugami, J. and Kume, H.},
  Booktitle                = {VLSI Technology, 2001. Digest of Technical Papers. 2001 Symposium on},
  Year                     = {2001},
  Pages                    = {115 -116},

  Doi                      = {10.1109/VLSIT.2001.934976},
  Keywords                 = {charge detrapping;detrap;detrap centroid;detrapping;detrapping parameter;electron detrapping;flash-memory retention characteristics;hole detrapping;multi-level flash memory;multi-level flash memory design;programmed memory cell;threshold voltage shift;tunnel oxide;tunnel-oxide degradation;carrier lifetime;electron traps;flash memories;hole traps;integrated circuit design;integrated memory circuits;tunnelling;}
}

@InProceedings{1577854,
  Title                    = {VLSI implementation of a low-error-floor and capacity-approaching low-density parity-check code decoder with multi-rate capacity},
  Author                   = {Lei Yang and Hui Liu and Shi, C.-J.R.},
  Booktitle                = {Global Telecommunications Conference, 2005. GLOBECOM '05. IEEE},
  Year                     = {2005},
  Month                    = nov.-2 # dec.,
  Pages                    = { 6 pp.},
  Volume                   = {3},

  Abstract                 = {With the superior error correction capability, low-density parity-check (LDPC) codes have initiated wide scale interests in wireless communication and storage fields. In the past, various structures of single code-rate LDPC decoders have been reported. However, to cover a wide range of service requirements and diverse interference conditions in wireless applications, LDPC decoders that can operate at both high and low code rates are desirable. In this paper, a 9k code length multi-rate LDPC decoder architecture is presented and implemented on a Xilinx FPGA device. Using pin selection, three operating modes, namely, the irregular 1/2 code, the regular 5/8 code and the regular 7/8 code, are supported. Furthermore, to suppress the error floor level, a characterization on the conditions for short cycles in a LDPC code matrix expanded from a small base matrix is presented, and a cycle elimination algorithm is developed to detect and break such short cycles. The effectiveness of the cycle elimination algorithm has been verified by both simulation and hardware measurements, which show that the error floor is suppressed to a much lower level without incurring any performance penalty. The implemented decoder is tested in an experimental LDPC-OFDM system and achieves the superior measured performance of block error rate below 10-7 at SNR 1.8 dB.},
  Doi                      = {10.1109/GLOCOM.2005.1577854},
  File                     = {:PDF\\VLSI_Implementation_of_a_Low-Error-Floor_and_Capacity-Approaching_Low-Density_Parity-Check-Code_Decoder_with_Multi-Rate_Capacity.pdf:PDF},
  Keywords                 = { LDPC; OFDM; VLSI implementation; Xilinx FPGA device; capacity-approaching decoder; cycle elimination algorithm; low-density parity-check code decoder; low-error-floor decoder; multi-rate capacity; small base matrix; OFDM modulation; VLSI; decoding; field programmable gate arrays; parity check codes;}
}

@Article{1291797,
  Title                    = {Design of efficiently encodable moderate-length high-rate irregular LDPC codes},
  Author                   = {Yang, M. and Ryan, W.E. and Yan Li},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2004},

  Month                    = april,
  Number                   = {4},
  Pages                    = { 564 - 571},
  Volume                   = {52},

  Abstract                 = {This paper presents a new class of irregular low-density parity-check (LDPC) codes of moderate length (103 le;n le;104) and high rate (R ge;3/4). Codes in this class admit low-complexity encoding and have lower error-rate floors than other irregular LDPC code-design approaches. It is also shown that this class of LDPC codes is equivalent to a class of systematic serial turbo codes and is an extension of irregular repeat-accumulate codes. A code design algorithm based on the combination of density evolution and differential evolution optimization with a modified cost function is presented. Moderate-length, high-rate codes with no error-rate floors down to a bit-error rate of 10-9 are presented. Although our focus is on moderate-length, high-rate codes, the proposed coding scheme is applicable to irregular LDPC codes with other lengths and rates.},
  Doi                      = {10.1109/TCOMM.2004.826367},
  File                     = {:PDF\\Design_of_Efficiently_Encodable_Moderate-Length_High-Rate_Irregular_LDPC_Codes.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = { error rate floor; irregular repeat-accumulate codes; low-density parity-check codes; serial turbo codes; error statistics; parity check codes; turbo codes;}
}

@InProceedings{5749736,
  Title                    = {I-CASH: Intelligently Coupled Array of SSD and HDD},
  Author                   = {Qing Yang and Jin Ren},
  Booktitle                = {High Performance Computer Architecture (HPCA), 2011 IEEE 17th International Symposium on},
  Year                     = {2011},
  Month                    = {feb.},
  Pages                    = {278 -289},

  Abstract                 = {This paper presents a new disk I/O architecture composed of an array of a flash memory SSD (solid state disk) and a hard disk drive (HDD) that are intelligently coupled by a special algorithm. We call this architecture I-CASH: Intelligently Coupled Array of SSD and HDD. The SSD stores seldom-changed and mostly read reference data blocks whereas the HDD stores a log of deltas between currently accessed I/O blocks and their corresponding reference blocks in the SSD so that random writes are not performed in SSD during online I/O operations. High speed delta compression and similarity detection algorithms are developed to control the pair of SSD and HDD. The idea is to exploit the fast read performance of SSDs and the high speed computation of modern multi-core CPUs to replace and substitute, to a great extent, the mechanical operations of HDDs. At the same time, we avoid runtime SSD writes that are slow and wearing. An experimental prototype I-CASH has been implemented and is used to evaluate I-CASH performance as compared to existing SSD/HDD I/O architectures. Numerical results on standard benchmarks show that I-CASH reduces the average I/O response time by an order of magnitude compared to existing disk I/O architectures such as RAID and SSD/HDD storage hierarchy, and provides up to 2.8 speedup over state-of-the-art pure SSD storage. Furthermore, I-CASH reduces random writes to SSD implying reduced wearing and prolonged life time of the SSD.},
  Doi                      = {10.1109/HPCA.2011.5749736},
  File                     = {:PDF\\05749736.pdf:PDF},
  ISSN                     = {1530-0897},
  Keywords                 = {I-CASH architecture;delta compression algorithm;disk input-output architecture;flash memory SSD;hard disk drive;intelligently coupled array of SSD and HDD;multicore CPU;similarity detection algorithm;solid state disk;disc drives;flash memories;hard discs;},
  Timestamp                = {2011.05.19}
}

@Article{Yasuda200683,
  Title                    = {Triangular approximation for Ising model and its application to Boltzmann machine },
  Author                   = {Muneki Yasuda and Tsuyoshi Horiguchi},
  Journal                  = {Physica A: Statistical Mechanics and its Applications},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {83 - 95},
  Volume                   = {368},

  Abstract                 = {When we consider a problem in information processing, it is convenient to formulate the problem by using a random Ising model in statistical physics. However, a kind of computational difficulty arises in a case that the number of nodes becomes large. Hence approximation schemes such as a mean field approximation and a Bethe approximation have been used extensively for overcoming the difficulty. When frustration is essential in some problems, the Bethe approximation gives unfavorable results. In those problems, more advanced approximation schemes are needed beyond the Bethe approximation. In the present paper, we present explicitly the triangular approximation, which is the next approximation to the Bethe approximation. We apply the obtained approximation scheme to a Boltzmann machine in order to investigate the validity of the triangular approximation.
},
  Doi                      = {http://dx.doi.org/10.1016/j.physa.2005.12.032},
  ISSN                     = {0378-4371},
  Keywords                 = {Ising model},
  Timestamp                = {2015.07.31},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0378437105012860}
}

@Article{1459044,
  Title                    = {Constructing free-energy approximations and generalized belief propagation algorithms},
  Author                   = {Yedidia, J.S. and Freeman, W.T. and Weiss, Y.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {july},
  Number                   = {7},
  Pages                    = {2282 - 2312},
  Volume                   = {51},

  Abstract                 = { Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a "valid" or "maxent-normal" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the "Bethe method", the "junction graph method", the "cluster variation method", and the "region graph method". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.},
  Doi                      = {10.1109/TIT.2005.850085},
  File                     = {:PDF\\01459044.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = { Bethe approximation; GBP algorithm; Kikuchi free energy; cluster variation method; factor graphs; free energy approximation; generalized belief propagation; inference problem; junction graph method; message passing; region graph method; sum-product algorithm; backpropagation; belief networks; graph theory; inference mechanisms; message passing;},
  Timestamp                = {2012.10.15}
}

@TechReport{TR2001-16,
  Title                    = {Bethe free energy, Kikuchi approximations,and belief propagation algorithms},
  Author                   = {Jonathan S. Yedidia and William T. Freeman and Yair Weiss},
  Institution              = {MITSUBISHI ELECTRIC RESEARCH LABORATORIES},
  Year                     = {2001},
  Month                    = {May},

  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\TR2001-16.pdf:PDF},
  Timestamp                = {2014.06.18},
  Url                      = {http://www.stat.ucla.edu/~sczhu/Workshops/sctv01/TR2001-16.pdf}
}

@Article{MERLTR2000-26,
  Title                    = {Generalized Belief Propagation},
  Author                   = {Jonathan S. Yedidia and William T. Freeman and Yair Weiss},
  Journal                  = {Technical Report of Mitsubishi Electric Research Laboratory},
  Year                     = {2000},
  Pages                    = {26},
  Volume                   = {2000},

  File                     = {:PDF\\MERLTR2000-26.pdf:PDF},
  Timestamp                = {2012.10.15}
}

@Article{1222729,
  Title                    = {Iterative decoder architectures},
  Author                   = {Engling Yeo and Anantharam, V.},
  Journal                  = {Communications Magazine, IEEE},
  Year                     = {2003},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {132-140},
  Volume                   = {41},

  Abstract                 = {Implementation constraints on iterative decoders applying message-passing algorithms are investigated. Serial implementations similar to traditional microprocessor datapaths are compared against architectures with multiple processing elements that exploit the inherent parallelism in the decoding algorithm. Turbo codes and low-density parity check codes, in particular, are evaluated in terms of their suitability for VLSI implementation in addition to their bit error rate performance as a function of signal-to-noise ratio. It is necessary to consider efficient realizations of iterative decoders when area, power, and throughput of the decoding implementation are constrained by practical design issues of communications receivers.},
  Doi                      = {10.1109/MCOM.2003.1222729},
  File                     = {:PDF\\01222729.pdf:PDF},
  ISSN                     = {0163-6804},
  Keywords                 = {VLSI;error statistics;iterative decoding;message passing;parallel architectures;parity check codes;turbo codes;VLSI implementation;bit error rate performance;communications receivers;decoding algorithm;implementation constraints;iterative decoder architectures;low-density parity check codes;message-passing algorithms;parallelism;serial implementations;signal-to-noise ratio;turbo codes;Arithmetic;Delay;Iterative decoding;Parity check codes;Performance gain;Pipeline processing;Samarium;Table lookup;Throughput;Viterbi algorithm},
  Timestamp                = {2015.07.17}
}

@InProceedings{965981,
  Title                    = {High throughput low-density parity-check decoder architectures},
  Author                   = {Yeo, E. and Pakzad, P. and Nikolic, B. and Anantharam, V.},
  Booktitle                = {Global Telecommunications Conference, 2001. GLOBECOM '01. IEEE},
  Year                     = {2001},
  Pages                    = {3019 -3024 vol.5},
  Volume                   = {5},

  Abstract                 = {Two decoding schedules and the corresponding serialized architectures for low-density parity-check (LDPC) decoders are presented. They are applied to codes with parity-check matrices generated either randomly or using geometric properties of elements in Galois fields. Both decoding schedules have low computational requirements. The original concurrent decoding schedule has a large storage requirement that is dependent on the total number of edges in the underlying bipartite graph, while a new, staggered decoding schedule which uses an approximation of the belief propagation, has a reduced memory requirement that is dependent only on the number of bits in the block. The performance of these decoding schedules is evaluated through simulations on a magnetic recording channel},
  Doi                      = {10.1109/GLOCOM.2001.965981},
  File                     = {:PDF\\00965981.pdf:PDF},
  Keywords                 = {Galois fields;LDPC decoders;belief propagation;bipartite graph;concurrent decoding;decoding schedules;low-density parity-check decoders;magnetic recording channel;parity check matrices;serialized architectures;staggered decoding schedule;Galois fields;belief maintenance;decoding;error detection codes;magnetic recording;},
  Timestamp                = {2011.04.22}
}

@InProceedings{Yeo2001,
  Title                    = {High throughput low-density parity-check decoder architectures},
  Author                   = {Yeo, E. and Pakzad, P. and Nikolic, B. and Anantharam, V.},
  Booktitle                = {Global Telecommunications Conference, 2001. GLOBECOM '01. IEEE},
  Year                     = {2001},
  Pages                    = {3019-3024 vol.5},
  Volume                   = {5},

  Abstract                 = {Two decoding schedules and the corresponding serialized architectures for low-density parity-check (LDPC) decoders are presented. They are applied to codes with parity-check matrices generated either randomly or using geometric properties of elements in Galois fields. Both decoding schedules have low computational requirements. The original concurrent decoding schedule has a large storage requirement that is dependent on the total number of edges in the underlying bipartite graph, while a new, staggered decoding schedule which uses an approximation of the belief propagation, has a reduced memory requirement that is dependent only on the number of bits in the block. The performance of these decoding schedules is evaluated through simulations on a magnetic recording channel},
  Doi                      = {10.1109/GLOCOM.2001.965981},
  File                     = {:PDF\\00965981.pdf:PDF},
  Keywords                 = {Galois fields;belief maintenance;decoding;error detection codes;magnetic recording;Galois fields;LDPC decoders;belief propagation;bipartite graph;concurrent decoding;decoding schedules;low-density parity-check decoders;magnetic recording channel;parity check matrices;serialized architectures;staggered decoding schedule;Bipartite graph;Computational modeling;Computer architecture;Galois fields;Iterative decoding;Magnetic recording;Message passing;Parity check codes;Processor scheduling;Throughput},
  Timestamp                = {2015.07.17}
}

@Article{e96-a_12_2562,
  Title                    = {Hybrid Message-Passing Algorithm and Architecture for Decoding Cyclic Non-binary LDPC Codes},
  Author                   = {Yichao LU,Gang HE,Guifen TIAN,Satoshi GOTO},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2013},

  Month                    = {01},
  Number                   = {12},
  Pages                    = {2652-2659},
  Volume                   = {E96-A},

  Abstract                 = {Recently, non-binary low-density parity-check (NB-LDPC) codes starts to show their superiority in achieving significant coding gains when moderate codeword lengths are adopted. However, the overwhelming decoding complexity keeps NB-LDPC codes from being widely employed in modern communication devices. This paper proposes a hybrid message-passing decoding algorithm which consumes very low computational complexity. It achieves competitive error performance compared with conventional Min-max algorithm. Simulation result on a (255,174) cyclic code shows that this algorithm obtains at least 0.5dB coding gain over other state-of-the-art low-complexity NB-LDPC decoding algorithms. A partial-parallel NB-LDPC decoder architecture for cyclic NB-LDPC codes is also developed based on this algorithm. Optimization schemes are employed to cut off hard decision symbols in RAMs and also to store only part of the reliability messages. In addition, the variable node units are redesigned especially for the proposed algorithm. Synthesis results demonstrate that about 24.3% gates and 12% memories can be saved over previous works.},
  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\e96-a_12_2652.pdf:PDF},
  Timestamp                = {2014.04.03}
}

@Article{ieice_e97-a_1356,
  Title                    = {Dynamic Check Message Majority-Logic Decoding Algorithm for Non-binary LDPC Codes},
  Author                   = {Yichao LU, Xiao PENG, Guifen TIAN, Satoshi GOTO},
  Journal                  = {IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1356-1364},
  Volume                   = {E97-A},

  Abstract                 = {Majority-logic algorithms are devised for decoding non-binary LDPC codes in order to reduce computational complexity. However, compared with conventional belief propagation algorithms, majority-logic algorithms suffer from severe bit error performance degradation. This paper presents a low-complexity reliability-based algorithm aiming at improving error correcting ability of majority-logic algorithms. Reliability measures for check nodes are novelly introduced to realize mutual update between variable message and check message, and hence more efficient reliability propagation can be achieved, similar to belief-propagation algorithm. Simulation results on NB-LDPC codes with different characteristics demonstrate that our algorithm can reduce the bit error ratio by more than one order of magnitude and the coding gain enhancement over ISRB-MLGD can reach 0.2-2.0dB, compared with both the ISRB-MLGD and IISRB-MLGD algorithms. Moreover, simulations on typical LDPC codes show that the computational complexity of the proposed algorithm is closely equivalent to ISRB-MLGD algorithm, and is less than 10% of Min-max algorithm. As a result, the proposed algorithm achieves a more efficient trade-off between decoding computational complexity and error performance.},
  File                     = {:PDF\\ieice_e97-a_6_1356.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2014.09.04}
}

@Article{887870,
  Title                    = {Minimum cyclotomic coset representatives and their applications to BCH codes and Goppa codes},
  Author                   = {Dian-Wu Yue and Guang-Zeng Feng},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {nov},
  Number                   = {7},
  Pages                    = {2625 -2628},
  Volume                   = {46},

  Abstract                 = {We consider the minimum cyclotomic coset representatives and derive some of their properties. The results allow more precise estimates of the dimension of Bose-Chaudhuri-Hocquenghem (BCH) and classical Goppa codes of a given designed minimum distance, and more precise estimates of the true designed distance of BCH codes and the minimum distance of classical Goppa codes},
  Doi                      = {10.1109/18.887870},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\00887870.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {BCH codes;Bose-Chaudhuri-Hocquenghem codes;Goppa codes;code dimension;minimum cyclotomic coset representatives;minimum distance;true designed distance;BCH codes;Goppa codes;set theory;},
  Timestamp                = {2012.02.07}
}

@Article{Yuille:2002:CAM:638977.638986,
  Title                    = {CCCP Algorithms to Minimize the Bethe and Kikuchi Free Energies: Convergent Alternatives to Belief Propagation},
  Author                   = {Yuille, A. L.},
  Journal                  = {Neural Comput.},
  Year                     = {2002},

  Month                    = jul,
  Number                   = {7},
  Pages                    = {1691--1722},
  Volume                   = {14},

  Acmid                    = {638986},
  Address                  = {Cambridge, MA, USA},
  Doi                      = {10.1162/08997660260028674},
  File                     = {:PDF\\bethe_nc02J.pdf:PDF},
  ISSN                     = {0899-7667},
  Issue_date               = {July 2002},
  Numpages                 = {32},
  Publisher                = {MIT Press},
  Timestamp                = {2014.09.12},
  Url                      = {http://dx.doi.org/10.1162/08997660260028674}
}

@InProceedings{1188418,
  Title                    = {On implementation of min-sum algorithm for decoding low-density parity-check (LDPC) codes},
  Author                   = {Zarkeshvari, F. and Banihashemi, A.H.},
  Booktitle                = {Global Telecommunications Conference, 2002. GLOBECOM '02. IEEE},
  Year                     = {2002},
  Month                    = nov.,
  Pages                    = { 1349 - 1353 vol.2},
  Volume                   = {2},

  Abstract                 = { This paper is concerned with the implementation issues of the so-called min-sum algorithm (also referred to as max-sum or max-product) for the decoding of low-density parity-check (LDPC) codes. The effects of clipping threshold and the number of quantization bits on the performance of the min-sum algorithm at short and intermediate block lengths are studied. It is shown that min-sum is robust against quantization effects, and in many cases, only four quantization bits suffices to obtain close to ideal performance. We also propose modifications to the min-sum algorithm that improve the performance by a few tenths of a dB with just a small increase in decoding complexity.},
  Doi                      = {10.1109/GLOCOM.2002.1188418},
  File                     = {:PDF\\On_Implementation_of_Min-Sum_Algorithm_for_Decoding_Low-Density_Parity-Check(LDPC)_Codes.pdf:PDF},
  ISSN                     = { },
  Keywords                 = { LDPC codes; block lengths; clipping threshold; decoding; low-density parity-check codes; max-product algorithm; max-sum algorithm; min-sum algorithm; performance; quantization bits; block codes; error statistics; iterative decoding; parity check codes; product codes; quantisation (signal);}
}

@InProceedings{4487366,
  Title                    = {A solution for memory collision in semi-parallel FPGA-based LDPC decoder design},
  Author                   = {Zarubica, R. and Wilson, S.G.},
  Booktitle                = {Signals, Systems and Computers, 2007. ACSSC 2007. Conference Record of the Forty-First Asilomar Conference on},
  Year                     = {2007},
  Month                    = {Nov},
  Pages                    = {982-986},

  Abstract                 = {Low Density Parity Check (LDPC) decoders implementing long blocklength codes require semi-parallel design. One challenge when implementing these codes on Field Programmable Gate Arrays (FPGAs) is efficiently storing messages needed in the iteration process. To meet this challenge, a new class of LDPC codes is presented. They combine regularity of implementation with reduction of time needed for decoding process, and do not suffer of any significant performance loss due to their structure.},
  Doi                      = {10.1109/ACSSC.2007.4487366},
  File                     = {:PDF\\04487366.pdf:PDF},
  ISSN                     = {1058-6393},
  Keywords                 = {decoding;field programmable gate arrays;parity check codes;blocklength codes;decoding process;field programmable gate arrays;iteration process;low density parity check decoders;memory collision;semiparallel FPGA-based LDPC decoder design;semiparallel design;Cities and towns;Code standards;Communication standards;Digital video broadcasting;Field programmable gate arrays;Iterative decoding;Message passing;Parity check codes;Performance loss;Sparse matrices},
  Timestamp                = {2015.07.17}
}

@Article{4489627,
  Title                    = {Transactions Papers - Constructions of Nonbinary Quasi-Cyclic LDPC Codes: A Finite Field Approach},
  Author                   = {Lingqi Zeng and Lan Lan and Tai, Y.Y. and Shumei Song and Shu Lin and Abdel-Ghaffar, K.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2008},

  Month                    = april,
  Number                   = {4},
  Pages                    = {545 -554},
  Volume                   = {56},

  Abstract                 = {This paper is concerned with construction of efficiently encodable nonbinary quasi-cyclic LDPC codes based on finite fields. Four classes of nonbinary quasi-cyclic LDPC codes are constructed. Experimental results show that codes constructed perform well with iterative decoding using a fast Fourier transform based q-ary sum-product algorithm and they achieve significant coding gains over Reed-Solomon codes of the same lengths and rates decoded with either algebraic hard- decision Berlekamp-Massey algorithm or algebraic soft-decision Kotter-Vardy algorithm.},
  Doi                      = {10.1109/TCOMM.2008.060024},
  File                     = {:PDF\\Constructions_of_Nonbinary_Quasi-Cyclic_LDPC_Codes_A_Finite_Field_Approach.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {fast Fourier transform;finite field approach;iterative decoding;nonbinary quasi-cyclic LDPC codes;q-ary sum-product algorithm;fast Fourier transforms;iterative decoding;matrix algebra;parity check codes;}
}

@Article{4471933,
  Title                    = {Construction of nonbinary cyclic, quasi-cyclic and regular LDPC codes: a finite geometry approach},
  Author                   = {Lingqi Zeng and Lan Lan and Ying Yu Tai and Bo Zhou and Shu Lin and Abdel-Ghaffar, K.A.S.},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2008},

  Month                    = march,
  Number                   = {3},
  Pages                    = {378 -387},
  Volume                   = {56},

  Abstract                 = {This paper presents five methods for constructing nonbinary LDPC codes based on finite geometries. These methods result in five classes of nonbinary LDPC codes, one class of cyclic LDPC codes, three classes of quasi-cyclic LDPC codes and one class of structured regular LDPC codes. Experimental results show that constructed codes in these classes decoded with iterative decoding based on belief propagation perform very well over the AWGN channel and they achieve significant coding gains over Reed-Solomon codes of the same lengths and rates with either algebraic hard-decision decoding or Kotter-Vardy algebraic soft-decision decoding at the expense of a larger decoding computational complexity.},
  Doi                      = {10.1109/TCOMM.2008.060025},
  File                     = {:PDF\\04471933.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {AWGN channel;Kotter-Vardy algebraic soft-decision decoding;Reed-Solomon codes;algebraic hard-decision decoding;belief propagation;decoding computational complexity;finite geometry approach;iterative decoding;low density parity check codes;quasicyclic LDPC codes;AWGN channels;iterative decoding;parity check codes;}
}

@InProceedings{5513603,
  Title                    = {LDPC codes for rank modulation in flash memories},
  Author                   = {Fan Zhang and Pfister, H.D. and Anxiao Jiang},
  Booktitle                = {Information Theory Proceedings (ISIT), 2010 IEEE International Symposium on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {859-863},

  Abstract                 = {An LDPC code is proposed for flash memories based on rank modulation. In contrast to previous approaches, this enables the use of long ECCs with fixed-length modulation codes. For ECC design, the rank modulation scheme is treated as part of an equivalent channel. A probabilistic model of the equivalent channel is derived and a simple high-SNR approximation is given. LDPC codes over integer rings and finite fields are designed for the approximate channel and a low-complexity symbol-flipping verification-based (SFVB) message-passing decoding algorithm is proposed to take advantage of the channel structure. Density evolution (DE) is used to calculate decoding thresholds and simulations are used to compare the low-complexity decoder with sum-product decoding.},
  Doi                      = {10.1109/ISIT.2010.5513603},
  File                     = {:PDF\\05513603.pdf:PDF},
  Keywords                 = {approximation theory;channel coding;computational complexity;decoding;flash memories;modulation coding;parity check codes;probability;ECC design;LDPC codes;channel structure;density evolution;equivalent channel;finite fields;flash memories;high-SNR approximation;low-complexity decoder;probabilistic model;rank modulation;sum-product decoding;symbol-flipping verification-based message-passing decoding;Algorithm design and analysis;Computer science;Decoding;Error correction codes;Flash memory;Galois fields;Modulation coding;Nonvolatile memory;Parity check codes;Tunneling},
  Timestamp                = {2014.09.03}
}

@InProceedings{5496999,
  Title                    = {Automated lookahead data migration in SSD-enabled multi-tiered storage systems},
  Author                   = {Gong Zhang and Chiu, L. and Dickey, C. and Ling Liu and Muench, P. and Seshadri, S.},
  Booktitle                = {Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on},
  Year                     = {2010},
  Month                    = {may},
  Pages                    = {1 -6},

  Abstract                 = {The significant IO improvements of Solid State Disks (SSD) over traditional rotational hard disks makes it an attractive approach to integrate SSDs in tiered storage systems for performance enhancement. However, to integrate SSD into multi-tiered storage system effectively, automated data migration between SSD and HDD plays a critical role. In many real world application scenarios like banking and supermarket environments, workload and IO profile present interesting characteristics and also bear the constraint of workload deadline. How to fully release the power of data migration while guaranteeing the migration deadline is critical to maximizing the performance of SSD-enabled multi-tiered storage system. In this paper, we present an automated, deadline-aware, lookahead migration scheme to address the data migration challenge. We analyze the factors that may impact on the performance of lookahead migration efficiency and develop a greedy algorithm to adaptively determine the optimal lookahead window size to optimize the effectiveness of lookahead migration, aiming at improving overall system performance and resource utilization while meeting workload deadlines. We compare our lookahead migration approach with the basic migration model and validate the effectiveness and efficiency of our adaptive lookahead migration approach through a trace driven experimental study.},
  Doi                      = {10.1109/MSST.2010.5496999},
  File                     = {:PDF\\05496999.pdf:PDF},
  Keywords                 = {HDD;IO improvements;SSD-enabled multitiered storage systems;automated data migration;automated lookahead data migration;data migration challenge;greedy algorithm;lookahead migration efficiency;migration deadline;optimal lookahead window size;performance enhancement;resource utilization;solid state disks;traditional rotational hard disks;workload deadline;disc drives;greedy algorithms;hard discs;storage management;},
  Timestamp                = {2011.05.20}
}

@Article{1706484,
  Title                    = {Performance evaluation of LDPC codes in bliss scheme-based storage systems using density evolution},
  Author                   = {Zhang, H. and Hekstra, A.P. and Yin, B.},
  Journal                  = {Consumer Electronics, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {aug},
  Number                   = {3},
  Pages                    = {879 -887},
  Volume                   = {52},

  Abstract                 = {Density evolution algorithms for scaled min-sum decoding of both regular and irregular low-density parity-check (LDPC) codes in Bliss scheme-based storage systems are presented, to evaluate the performance of LDPC codes in terms of noise threshold. Firstly, with the assumption of fixed error propagation factor and independently and identically distributed (i.i.d.) messages, density evolution algorithms of regular LDPC codes are derived to compare performance among LDPC codes with the same code rate but different bit/check node degrees and determine the optimum scaling factor of scaled min-sum decoding. Secondly, with the looser assumption of fixed error propagation and independently distributed messages, density evolution algorithms of irregular LDPC codes are derived for performance evaluation of LDPC codes with arbitrary degree distributions. Comparisons with simulations show density evolution is a proper performance evaluation method for LDPC codes in Bliss scheme-based storage systems},
  Doi                      = {10.1109/TCE.2006.1706484},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\01706484.pdf:PDF},
  ISSN                     = {0098-3063},
  Keywords                 = {Bliss scheme-based storage systems;LDPC codes;density evolution;fixed error propagation factor;independently and identically distributed messages;low-density parity-check;scaled min-sum decoding;decoding;digital storage;parity check codes;},
  Timestamp                = {2011.11.24}
}

@InProceedings{1197141,
  Title                    = {Shuffled belief propagation decoding},
  Author                   = {Juntan Zhang and Fossorier, M.},
  Booktitle                = {Signals, Systems and Computers, 2002. Conference Record of the Thirty-Sixth Asilomar Conference on},
  Year                     = {2002},
  Month                    = nov.,
  Pages                    = { 8 - 15 vol.1},
  Volume                   = {1},

  Abstract                 = { In this paper, we propose a shuffled version of the belief propagation (BP) algorithm for the decoding of low-density parity-check (LDPC) codes. We show that when the Tanner graph of the code is acyclic and connected, the proposed scheme is optimal in the sense of MAP decoding and converges faster (or at least no slower) than the standard BP algorithm. Interestingly, this new version keeps the computational advantages of the forward-backward implementations of BP decoding. Both serial and parallel implementations are considered. We show by simulation that the new schedule offers better performance/complexity trade-offs.},
  Doi                      = {10.1109/ACSSC.2002.1197141},
  File                     = {:PDF\\Shuffled_Belief_Propagation_Decoding.pdf:PDF},
  ISSN                     = {1058-6393 },
  Keywords                 = { BP algorithm; LDPC codes; MAP decoding; Tanner graph; computational complexity; forward-backward implementations; low-density parity-check codes; maximum a posteriori probability; parallel implementation; serial implementation; shuffled belief propagation decoding; computational complexity; graph theory; maximum likelihood decoding; maximum likelihood estimation; parity check codes;}
}

@InProceedings{5937810,
  Title                    = {Low-complexity architectures for reliability-based message-passing non-binary LDPC decoding},
  Author                   = {Xinmiao Zhang and Fang Cai},
  Booktitle                = {Circuits and Systems (ISCAS), 2011 IEEE International Symposium on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {1303-1306},

  Abstract                 = {When the code is not long, non-binary low-density parity- check (NB-LDPC) codes can achieve better error-correcting performance than binary LDPC codes at the cost of higher decoding complexity. The recently developed iterative reliability-based majority-logic NB- LDPC decoding can achieve better performance-complexity tradeoffs than previous algorithms. Many existing NB-LDPC code construction schemes lead to quasi-cyclic or cyclic codes. In this paper, efficient low- complexity NB-LDPC decoder architectures are developed for these two types of codes based on the newly proposed iterative hard reliability-based majority-logic decoding (IHRB-MLGD). Particularly, novel schemes are designed to keep a small proportion of messages in order to reduce the memory requirement without causing noticeable performance loss. Moreover, a shift-message structure is proposed by using memories concatenated with variable node units to enable efficient partial-parallel decoding for cyclic NB-LDPC codes. Compared to previous decoders based on the Min-max algorithm, the proposed IHRB-MLGD decoder architectures can achieve tens of times higher efficiency for codes with similar length and rate with moderate coding gain loss.},
  Doi                      = {10.1109/ISCAS.2011.5937810},
  File                     = {:PDF\\05937810.pdf:PDF},
  ISSN                     = {0271-4302},
  Keywords                 = {cyclic codes;error correction codes;iterative decoding;message passing;minimax techniques;parity check codes;reliability;IHRB;MLGD decoder;NB-LDPC decoder;cyclic codes;error correcting code;iterative hard reliability based decoding;low density parity check;majority-logic decoding;message passing;minimax algorithm;non-binary codes;partial-parallel decoding;quasi-cyclic codes;Complexity theory;Computer architecture;Decoding;Iterative decoding;Random access memory;Reliability},
  Timestamp                = {2015.04.21}
}

@Article{6018324,
  Title                    = {Low-Complexity Reliability-Based Message-Passing Decoder Architectures for Non-Binary LDPC Codes},
  Author                   = {Xinmiao Zhang and Fang Cai and Shu Lin},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {1938-1950},
  Volume                   = {20},

  Abstract                 = {Non-binary low-density parity-check (NB-LDPC) codes can achieve better error-correcting performance than their binary counterparts at the cost of higher decoding complexity when the codeword length is moderate. The recently developed iterative reliability-based majority-logic NB-LDPC decoding has better performance-complexity tradeoffs than previous algorithms. This paper first proposes enhancement schemes to the iterative hard reliability-based majority-logic decoding (IHRB-MLGD). Compared to the IHRB algorithm, our enhanced (E-)IHRB algorithm can achieve significant coding gain with small hardware overhead. Then low-complexity partial-parallel NB-LDPC decoder architectures are developed based on these two algorithms. Many existing NB-LDPC code construction methods lead to quasi-cyclic or cyclic codes. Both types of codes are considered in our design. Moreover, novel schemes are developed to keep a small proportion of messages in order to reduce the memory requirement without causing noticeable performance loss. In addition, a shift-message structure is proposed by using memories concatenated with variable node units to enable efficient partial-parallel decoding for cyclic NB-LDPC codes. Compared to previous designs based on the Min-max decoding algorithm, our proposed decoders have at least tens of times lower complexity with moderate coding gain loss.},
  Doi                      = {10.1109/TVLSI.2011.2164951},
  File                     = {:PDF\\06018324.pdf:PDF},
  ISSN                     = {1063-8210},
  Keywords                 = {codecs;cyclic codes;iterative decoding;message passing;minimax techniques;parity check codes;reliability;E-IHRB algorithm;IHRB algorithm;IHRB-MLGD;NB-LDPC code construction methods;codeword length;coding gain loss;cyclic NB-LDPC codes;decoders;decoding complexity;enhanced IHRB algorithm;error-correcting performance;iterative reliability-based majority-logic NB-LDPC decoding;iterative reliability-based majority-logic decoding;low-complexity partial-parallel NB-LDPC decoder;low-complexity reliability-based message-passing decoder architectures;min-max decoding algorithm;nonbinary LDPC codes;nonbinary low-density parity-check codes;partial-parallel decoding;performance-complexity tradeoffs;quasi-cyclic codes;shift-message structure;Algorithm design and analysis;Complexity theory;Decoding;Encoding;Iterative decoding;Parity check codes;Reliability;Iterative majority-logic decoding;VLSI;low-density parity-check (LDPC) codes;non-binary;partial-parallel},
  Timestamp                = {2015.04.21}
}

@InProceedings{Zhao:2011:DMB:1973009.1973104,
  Title                    = {Design of MRAM based logic circuits and its applications},
  Author                   = {Zhao, Weisheng and Torres, Lionel and Guillemenet, Yoann and Cargnini, Lu\'{\i}s Vit\'{o}rio and Lakys, Yahya and Klein, Jacques-Olivier and Ravelosona, Dafine and Sassatelli, Gilles and Chappert, Claude},
  Booktitle                = {Proceedings of the 21st edition of the great lakes symposium on Great lakes symposium on VLSI},
  Year                     = {2011},

  Address                  = {New York, NY, USA},
  Pages                    = {431--436},
  Publisher                = {ACM},
  Series                   = {GLSVLSI '11},

  Acmid                    = {1973104},
  Doi                      = {http://doi.acm.org/10.1145/1973009.1973104},
  File                     = {:PDF\\p431-zhao.pdf:PDF},
  ISBN                     = {978-1-4503-0667-6},
  Keywords                 = {3d integration, flip-flop, fpga, full-adder, magnetic logic, memory-in-logic, mram, non-volatile},
  Location                 = {Lausanne, Switzerland},
  Numpages                 = {6},
  Timestamp                = {2011.06.03},
  Url                      = {http://doi.acm.org/10.1145/1973009.1973104}
}

@InProceedings{5624881,
  Title                    = {A BER performance-aware early termination scheme for layered LDPC decoder},
  Author                   = {Xiongxin Zhao and Zhixiang Chen and Xiao Peng and Dajiang Zhou and Goto, S.},
  Booktitle                = {Signal Processing Systems (SIPS), 2010 IEEE Workshop on},
  Year                     = {2010},
  Month                    = {oct},
  Pages                    = {416 -419},

  Abstract                 = {This paper presents a novel early termination scheme for layered LDPC decoder. By solving the bit error rate (BER) performance degradation which will occur when other early termination schemes are applied in layered LDPC decoder, the proposed method achieves very fast termination speed without BER performance loss. It is the best solution for BER performance-aware layered LDPC decoders, such as satellite video broadcasting applications.},
  Doi                      = {10.1109/SIPS.2010.5624881},
  File                     = {:PDF\\05624881.pdf:PDF},
  ISSN                     = {1520-6130},
  Keywords                 = {BER performance-aware early termination scheme;bit error rate performance degradation;layered LDPC decoder;low-density parity-check;satellite video broadcasting;error statistics;parity check codes;},
  Timestamp                = {2011.04.26}
}

@InProceedings{6039973,
  Title                    = {Efficient Degree Optimization for High-Rate Structured QC-LDPC},
  Author                   = {Zhaoxia Zheng and Xiong Yin and Yan Hong and Yi Dan},
  Booktitle                = {Wireless Communications, Networking and Mobile Computing (WiCOM), 2011 7th International Conference on},
  Year                     = {2011},
  Month                    = {sept.},
  Pages                    = {1 -4},

  Abstract                 = {In order to efficiently optimize degree distribution of high-rate structured QC-LDPC (SQC-LDPC), based on analysis to the special structure of SQC-LDPC, this paper modifies previous degree distribution optimization method. At first, it proposes a new set of global optimization constraints for Differential Evolution (DE) algorithm. Furthermore, when Gaussian Approximate (GA) is used to compute the threshold of a given degree distribution pair, binary search rather than usually used fixed step search is adopted to speed up degree distribution optimization. Simulation results indicate that proposed degree distribution optimization method can obtain good degree distribution pairs of high-rate SQC-LDPC with less iteration times, and that high-rate SQC-LDPC designed by good degree distribution pairs embraces good performance.},
  Doi                      = {10.1109/wicom.2011.6039973},
  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\06039973.pdf:PDF},
  ISSN                     = {2161-9646},
  Keywords                 = {DE algorithm;GA;Gaussian approximation;binary search;degree distribution optimization method;differential evolution algorithm;fixed step search;global optimization constraint;high-rate SQC-LDPC;high-rate structured QC-LDPC;iteration time;Gaussian processes;approximation theory;cyclic codes;evolutionary computation;parity check codes;radio networks;search problems;},
  Timestamp                = {2012.01.11}
}

@InProceedings{6753772,
  Title                    = {Towards GPU-Accelerated Large-Scale Graph Processing in the Cloud},
  Author                   = {Jianlong Zhong and Bingsheng He},
  Booktitle                = {Cloud Computing Technology and Science (CloudCom), 2013 IEEE 5th International Conference on},
  Year                     = {2013},
  Month                    = {Dec},
  Pages                    = {9-16},
  Volume                   = {1},

  Doi                      = {10.1109/CloudCom.2013.8},
  File                     = {:PDF\\06753772.pdf:PDF},
  Keywords                 = {cloud computing;graph theory;graphics processing units;operating system kernels;processor scheduling;resource allocation;software performance evaluation;storage management;virtual machines;Amazon EC2 virtual cluster;G2 in-memory graph processing engine;GPU-accelerated large-scale graph processing;GPU-based graph processing engine;cloud computing;cloud-based graph processing;concurrent kernel executions;data-intensive applications;graph partition based approach;graph tasks;graphics processors;heterogeneous CPU architectures;heterogeneous GPU architectures;heterogeneous computing environments;load balancing;massive thread parallelism;nontrivial GPU-specific optimizations;performance improvement;runtime system;throughput improvement;transparent memory management;Cloud computing;Engines;Graphics processing units;Kernel;Optimization;Parallel processing;Programming;GPGPU;GPU accelerations;Large-scale graph processing;cloud computing;graph partitioning},
  Timestamp                = {2015.07.27}
}

@Article{5089504,
  Title                    = {Construction of non-binary quasi-cyclic LDPC codes by arrays and array dispersions - [transactions papers]},
  Author                   = {Bo Zhou and Jingyu Kang and Shumei Song and Shu Lin and Abdel-Ghaffar, K. and Meina Xu},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1652-1662},
  Volume                   = {57},

  Abstract                 = {This paper presents two algebraic methods for constructing high performance and efficiently encodable nonbinary quasi-cyclic LDPC codes based on arrays of special circulant permutation matrices and multi-fold array dispersions. Codes constructed based on these methods perform well over the AWGN and other types of channels with iterative decoding based on belief-propagation. Experimental results show that over the AWGN channel, these non-binary quasi-cyclic LDPC codes significantly outperform Reed-Solomon codes of the same lengths and rates decoded with either algebraic hard-decision Berlekamp-Massey algorithm or algebraic soft-decision Kotter-Vardy algorithm. Also presented in this paper is a class of asymptotically optimal LDPC codes for correcting bursts of erasures. Codes constructed also perform well over flat fading channels. Non-binary quasi-cyclic LDPC codes have a great potential to replace Reed-Solomon codes in some applications in communication environments and storage systems for combating mixed types of noises and interferences.},
  Doi                      = {10.1109/TCOMM.2009.06.070313},
  File                     = {:PDF\\05089504.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {algebraic codes;fading channels;matrix algebra;parity check codes;algebraic method;array dispersions;circulant permutation matrix;fading channel;multifold array dispersion;nonbinary quasicyclic LDPC code;AWGN channels;Additive white noise;Fading;Gaussian noise;Interference;Iterative algorithms;Iterative decoding;Parity check codes;Reed-Solomon codes;Working environment noise;Non-binary quasi-cyclic LDPC codes, Berlekamp-Massey algorithm, KÃ¶tter-Vardy algorithm, arrays. array dispersions.},
  Timestamp                = {2015.04.21}
}

@Article{4939224,
  Title                    = {High Performance Non-Binary Quasi-Cyclic LDPC Codes on Euclidean Geometries LDPC Codes on Euclidean Geometries},
  Author                   = {Bo Zhou and Jingyu Kang and Ying Tai and Shu Lin and Zhi Ding},
  Journal                  = {Communications, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {may },
  Number                   = {5},
  Pages                    = {1298 -1311},
  Volume                   = {57},

  Abstract                 = {This paper presents algebraic methods for constructing high performance and efficiently encodable non-binary quasi-cyclic LDPC codes based on flats of finite Euclidean geometries and array masking. Codes constructed based on these methods perform very well over the AWGN channel. With iterative decoding using a fast Fourier transform based sum-product algorithm, they achieve significantly large coding gains over Reed-Solomon codes of the same lengths and rates decoded with either algebraic hard-decision Berlekamp-Massey algorithm or algebraic soft-decision Kotter-Vardy algorithm. Due to their quasi-cyclic structure, these non-binary LDPC codes on Euclidean geometries can be encoded using simple shift-registers with linear complexity. Structured non-binary LDPC codes have a great potential to replace Reed-Solomon codes for some applications in either communication or storage systems for combating mixed types of noise and interferences.},
  Doi                      = {10.1109/TCOMM.2009.05.070240},
  File                     = {:\\\\homepd\\pd6\\ユニット管理\\refereces\\PDF\\04939224.pdf:PDF},
  ISSN                     = {0090-6778},
  Keywords                 = {AWGN channel;Reed-Solomon codes;algebraic method;algebraic soft-decision Kotter-Vardy algorithm;array masking;encodable nonbinary quasicyclic LDPC code;fast Fourier transform;finite Euclidean geometries;hard-decision Berlekamp-Massey algorithm;iterative decoding;linear complexity;sum-product algorithm;AWGN channels;Reed-Solomon codes;algebraic geometric codes;channel coding;cyclic codes;fast Fourier transforms;iterative decoding;parity check codes;},
  Timestamp                = {2011.12.06}
}

@InProceedings{4601044,
  Title                    = {Non-binary LDPC codes vs. Reed-Solomon codes},
  Author                   = {Bo Zhou and Li Zhang and Jingyu Kang and Qin Huang and Tai, Y.Y. and Shu Lin and Meina Xu},
  Booktitle                = {Information Theory and Applications Workshop, 2008},
  Year                     = {2008},
  Month                    = {Jan},
  Pages                    = {175-184},

  Abstract                 = {This paper investigates the potential of non-binary LDPC codes to replace widely used Reed-Solomon (RS) codes for applications in communication and storage systems for combating mixed types of noise and interferences. The investigation begins with presentation of four algebraic constructions of RS-based non-binary quasi-cyclic (QC)-LDPC codes. Then, the performances of some codes constructed based on the proposed methods with iterative decoding are compared with those of RS codes of the same lengths and rates decoded with the hard-decision Berlekamp-Massey (BM)-algorithm and the algebraic soft-decision Kotter-Vardy (KV)-algorithm over both the AWGN and a Rayleigh fading channels. Comparison shows that the constructed non-binary QC-LDPC codes significantly outperform their corresponding RS codes decoded with either the BM-algorithm or the KV-algorithm. Most impressively, the orders of decoding computational complexity of the constructed non-binary QC-LDPC codes decoded with 5 and 50 iterations of a Fast Fourier Transform based sum-product algorithm are much smaller than those of their corresponding RS codes decoded with the KV-algorithm, while achieve 1:5 to 3 dB coding gains. The comparison shows that well designed non-binary LDPC codes have a great potential to replace RS codes for some applications in communication or storage systems, at least before a very efficient algorithm for decoding RS codes is devised.},
  Doi                      = {10.1109/ITA.2008.4601044},
  File                     = {:PDF\\04601044.pdf:PDF},
  Keywords                 = {AWGN channels;Rayleigh channels;Reed-Solomon codes;fast Fourier transforms;parity check codes;AWGN channels;Berlekamp-Massey algorithm;Kotter-Vardy algorithm;Rayleigh fading channels;Reed-Solomon codes;algebraic constructions;communication systems;computational complexity;fast Fourier transform;hard-decision algorithm;interferences;nonbinary LDPC codes;quasicyclic-LDPC codes;storage systems;AWGN;Additive white noise;Computational complexity;Fading;Fast Fourier transforms;Gaussian noise;Interference;Iterative decoding;Parity check codes;Reed-Solomon codes}
}

@InProceedings{6284106,
  Title                    = {Systematic error-correcting codes for rank modulation},
  Author                   = {Hongchao Zhou and Anxiao Jiang and Bruck, J.},
  Booktitle                = {Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on},
  Year                     = {2012},
  Month                    = {July},
  Pages                    = {2978-2982},

  Abstract                 = {The rank modulation scheme has been proposed recently for efficiently writing and storing data in nonvolatile memories. Error-correcting codes are very important for rank modulation, and they have attracted interest among researchers. In this work, we explore a new approach, systematic error-correcting codes for rank modulation. In an (n, k) systematic code, we use the permutation induced by the levels of n cells to store data, and the permutation induced by the first k cells (k <; n) has a one-to-one mapping to information bits. Systematic codes have the benefits of enabling efficient information retrieval and potentially supporting more efficient encoding and decoding procedures. We study systematic codes for rank modulation equipped with the Kendall's τ-distance. We present (k + 2, k) systematic codes for correcting one error, which have optimal sizes unless perfect codes exist. We also study the design of multi-error-correcting codes, and prove that for any 2 ≤ k <; n, there always exists an (n, k) systematic code of minimum distance n-k. Furthermore, we prove that for rank modulation, systematic codes achieve the same capacity as general error-correcting codes.},
  Doi                      = {10.1109/ISIT.2012.6284106},
  File                     = {:PDF\\06284106.pdf:PDF},
  ISSN                     = {2157-8095},
  Keywords                 = {decoding;encoding;error correction codes;information retrieval;random-access storage;efficient decoding;efficient encoding;information retrieval;multierror correcting codes;nonvolatile memory;rank modulation;systematic codes;Encoding;Error correction codes;Measurement;Modulation;Systematics;Tin;Vectors},
  Timestamp                = {2014.09.03}
}

@Article{6937135,
  Title                    = {Systematic Error-Correcting Codes for Rank Modulation},
  Author                   = {Hongchao Zhou and Schwartz, M. and Jiang, A.A. and Bruck, J.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {17-32},
  Volume                   = {61},

  Abstract                 = {The rank-modulation scheme has been recently proposed for efficiently storing data in nonvolatile memories. In this paper, we explore [n, k, d] systematic error-correcting codes for rank modulation. Such codes have length n, k information symbols, and minimum distance d. Systematic codes have the benefits of enabling efficient information retrieval in conjunction with memory-scrubbing schemes. We study systematic codes for rank modulation under Kendall's T-metric as well as under the ℓ∞-metric. In Kendall's T-metric, we present [k + 2, k, 3] systematic codes for correcting a single error, which have optimal rates, unless systematic perfect codes exist. We also study the design of multierror-correcting codes, and provide a construction of [k + t + 1, k, 2t + 1] systematic codes, for large-enough k. We use nonconstructive arguments to show that for rank modulation, systematic codes achieve the same capacity as general error-correcting codes. Finally, in the ℓ∞-metric, we construct two [n, k, d] systematic multierror-correcting codes, the first for the case of d = 0(1) and the second for d = Θ(n). In the latter case, the codes have the same asymptotic rate as the best codes currently known in this metric.},
  Doi                      = {10.1109/TIT.2014.2365499},
  File                     = {:PDF\\06937135.pdf:PDF},
  ISSN                     = {0018-9448},
  Keywords                 = {error correction codes;matrix algebra;modulation coding;random-access storage;ℓ∞-metrix;Kendall T-metrix;data storage;information retrieval;multierror-correcting codes;nonvolatile memory scrubbing scheme;rank modulation scheme;systematic multierror correcting code;Error correction codes;Measurement;Modulation;Redundancy;Systematics;Tin;Vectors;$ell _infty $ -metric;ℓ¥-metric;Flash memory;Kendall’s τ- metric;Kendall???s $tau $ -metric;error-correcting codes;errorcorrecting codes;flash memory;metric embeddings;permutations;rank modulation;systematic codes},
  Timestamp                = {2015.03.31}
}

@Book{miyake,
  Title                    = {入門代数学},
  Author                   = {三宅 敏恒},
  Publisher                = {培風館},
  Year                     = {1999}
}

@Article{三浦晋示:1999-08-25,
  Title                    = {代数幾何符号の数理 (代数曲線とその応用論文小特集)},
  Author                   = {三浦 晋示 and 岩垂 好裕 and 今井 秀樹},
  Journal                  = {電子情報通信学会論文誌. A, 基礎・境界},
  Year                     = {1999-08-25},
  Number                   = {8},
  Pages                    = {1223-1238},
  Volume                   = {82},

  File                     = {:PDF\\代数幾何符号の数理.pdf:PDF},
  ISSN                     = {09135707},
  Publisher                = {社団法人電子情報通信学会},
  Url                      = {http://ci.nii.ac.jp/naid/110003313391/}
}

@Article{nikkei_ele20090330,
  Title                    = {HDD対フラッシュ　携帯機器を巡り競合から共存へ},
  Author                   = {今井 拓司},
  Journal                  = {日経エレクトロニクス},
  Year                     = {2009},

  Month                    = {March},
  Pages                    = {102-109}
}

@Book{functions-th,
  Title                    = {物理と関数論},
  Author                   = {今村 勤},
  Publisher                = {岩波書店},
  Year                     = {1983},

  Timestamp                = {2011.09.18}
}

@Article{How_to_use_SSD,
  Title                    = {どう付き合うかSSD},
  Author                   = {佐伯　真也 and 大石　基之},
  Journal                  = {日経エレクトロニクス},
  Year                     = {2009},

  Month                    = {April},
  Pages                    = {29-51}
}

@Book{uchida,
  Title                    = {有限体と符号理論},
  Author                   = {内田　興二},
  Publisher                = {サイエンス社},
  Year                     = {2000},
  Month                    = {January}
}

@Article{wadayama:2001-12-13,
  Title                    = {低密度パリティ検査符号とその復号法について},
  Author                   = {和田山 正},
  Journal                  = {映像情報メディア学会技術報告},
  Year                     = {2001-12-13},
  Number                   = {81},
  Pages                    = {39-46},
  Volume                   = {25},

  ISSN                     = {13426893},
  Publisher                = {社団法人映像情報メディア学会},
  Url                      = {http://ci.nii.ac.jp/naid/110003689689/}
}

@Book{wadayama-book,
  Title                    = {低密度パリティ検査符号とその復号法},
  Author                   = {和田山　正},
  Publisher                = {トリケップス},
  Year                     = {2002},

  ISBN                     = {978-4886572227},
  Yomi                     = {Tadashi Wadayama}
}

@Book{978-4-339-02446-3,
  Title                    = {代数系と符号理論入門},
  Author                   = {坂庭 好一 and 渋谷 智治},
  Publisher                = {コロナ社},
  Year                     = {2010},
  Month                    = {April}
}

@Book{ogiso-AG2002,
  Title                    = {代数曲線論},
  Author                   = {小木曽 啓示},
  Publisher                = {朝倉書店},
  Year                     = {2002},

  Timestamp                = {2012.06.11}
}

@Article{yamanishi_fermat_1989,
  Title                    = {Fermat符号の構成と性能について},
  Author                   = {山西 健司},
  Journal                  = {電子情報通信学会論文誌 A 基礎・境界},
  Year                     = {1989-03},
  Number                   = {3},
  Pages                    = {p597-607},
  Volume                   = {72},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\yamanishi_fermat_1989.pdf:PDF},
  ISSN                     = {09135707},
  Publisher                = {電子情報通信学会基礎・境界ソサイエティ},
  Timestamp                = {2011.07.15},
  Url                      = {http://ci.nii.ac.jp/naid/40004637388/}
}

@Article{yamanishi_modular_1988,
  Title                    = {2元Modular符号の新しい漸近的性能評価について},
  Author                   = {山西 健司},
  Journal                  = {電子情報通信学会論文誌 A 基礎・境界},
  Year                     = {1988-12},
  Number                   = {12},
  Pages                    = {2172-2182},
  Volume                   = {71},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\yamanishi_modular_1988.pdf:PDF},
  ISSN                     = {09135707},
  Publisher                = {電子情報通信学会基礎・境界ソサイエティ},
  Timestamp                = {2011.07.15},
  Url                      = {http://ci.nii.ac.jp/naid/40004637257/}
}

@Article{yamanishi_elliptic_1988,
  Title                    = {だ円符号および超だ円符号による高性能符号の導出について},
  Author                   = {山西 健司},
  Journal                  = {電子情報通信学会論文誌 A 基礎・境界},
  Year                     = {1988-10},
  Number                   = {10},
  Pages                    = {1936-1946},
  Volume                   = {71},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\yamanishi_elliptic_1988.pdf:PDF},
  ISSN                     = {09135707},
  Publisher                = {電子情報通信学会基礎・境界ソサイエティ},
  Timestamp                = {2011.07.15},
  Url                      = {http://ci.nii.ac.jp/naid/40004637293/}
}

@Booklet{hirasawa,
  Title                    = {符号理論},
  Author                   = {平澤 茂一},
  Year                     = {2008}
}

@Book{mikoshiba1991,
  Title                    = {半導体の物理[改訂版]},
  Author                   = {御子柴　宣夫},
  Publisher                = {培風館},
  Year                     = {1991},

  Timestamp                = {2011.09.22}
}

@Booklet{uematsu2007,
  Title                    = {代数系と符号理論-講義ノート-東京工業大学OCW},
  Author                   = {植松 友彦},
  Year                     = {2007},

  File                     = {:PDF\\代数系と符号理論-東京工業大学OCW2007.pdf:PDF}
}

@Booklet{uematsu2004,
  Title                    = {代数系と符号理論-講義ノート-東京工業大学OCW},
  Author                   = {植松 友彦},
  Year                     = {2004},

  File                     = {:PDF\\代数系と符号理論-東京工業大学OCW2004.pdf:PDF}
}

@Book{ECC_ohm,
  Title                    = {誤り訂正符号とその応用},
  Author                   = {横山 克哉ほか},
  Publisher                = {オーム社},
  Year                     = {1996}
}

@InBook{statisticalcomputing,
  Title                    = {計算統計 I-統計科学のフロンティア11},
  Author                   = {樺島 祥介 and 上田 修功},
  Chapter                  = {平均場近似・EM法・変分ベイズ法},
  Pages                    = {121-189},
  Publisher                = {岩波書店},
  Year                     = {2003}
}

@Book{asymptotic_analysis,
  Title                    = {漸近解析},
  Author                   = {江沢 洋},
  Publisher                = {岩波書店},
  Year                     = {1995},

  Timestamp                = {2011.12.16}
}

@Article{j83-a_11_1309,
  Title                    = {Reed-Solomon符号のリスト復号のための高速補間法},
  Author                   = {沼上　幸夫 and 藤澤　匡哉 and 阪田　省二郎},
  Journal                  = {電子情報通信学会論文誌 A},
  Year                     = {2000},
  Number                   = {11},
  Pages                    = {1309-1317},
  Volume                   = {83-A},

  File                     = {:\\\\homePD\\pd6\\ユニット管理\\refereces\\PDF\\j83-a_11_1309.pdf:PDF},
  Timestamp                = {2014.05.16}
}

@Article{watanabe2009-08-01,
  Title                    = {半導体メモリの過去40年の歴史と将来展望(シリコン材料・デバイス,<特集>エレクトロニクスソサイエティ和文論文誌500号記念論文)},
  Author                   = {渡辺 重佳},
  Journal                  = {電子情報通信学会論文誌. C, エレクトロニクス},
  Year                     = {2009-08-01},
  Number                   = {8},
  Pages                    = {467-476},
  Volume                   = {92},

  File                     = {:\\\\yrlnas.yrl.intra.hitachi.co.jp\\homePublic\\NMP\\ユニット管理\\refereces\\PDF\\watanabe2009-08-01.pdf:PDF},
  ISSN                     = {13452827},
  Publisher                = {社団法人電子情報通信学会},
  Timestamp                = {2011.09.14},
  Url                      = {http://ci.nii.ac.jp/naid/110007359668/}
}

@Book{igi-kawai-qm-I,
  Title                    = {量子力学I},
  Author                   = {猪木　慶治 and 川合　光},
  Publisher                = {講談社},
  Year                     = {1994},

  Timestamp                = {2011.09.18}
}

@Book{igi-kawai-qm-II,
  Title                    = {量子力学II},
  Author                   = {猪木 慶治 and 川合　光},
  Publisher                = {講談社},
  Year                     = {1994},

  Timestamp                = {2011.09.18}
}

@Book{amari_inf_theory,
  Title                    = {情報理論},
  Author                   = {甘利 俊一},
  Publisher                = {筑摩書房},
  Year                     = {2011},

  Timestamp                = {2012.04.04}
}

@Book{bayesiannetwork,
  Title                    = {ベイジアンネットワークの統計推論の数理},
  Author                   = {田中　和之},
  Publisher                = {コロナ社},
  Year                     = {2009}
}

@Book{hazamaAG2005,
  Title                    = {代数幾何学POD版},
  Author                   = {硲　文夫},
  Publisher                = {森北出版},
  Year                     = {2005},
  Month                    = {December},

  Timestamp                = {2011.09.23}
}

@Article{j87-c-2-260,
  Title                    = {代数幾何符号の磁気ディスク装置への適用についての基礎検討},
  Author                   = {近藤　昌晴 and 高師　輝実 and 泉田　守司 and 河野　隆二},
  Journal                  = {電子情報通信学会論文誌 C},
  Year                     = {2004},
  Number                   = {2},
  Pages                    = {260-269},
  Volume                   = {J87-C},

  File                     = {:PDF\\j87-c_2_260.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.03}
}

@Book{learn_euler,
  Title                    = {オイラーに学ぶ},
  Author                   = {野海　正俊},
  Publisher                = {日本評論社},
  Year                     = {2007},

  ISBN                     = {978-4535784888},
  Timestamp                = {2011.12.16},
  Url                      = {http://www.nippyo.co.jp/book/3187.html}
}

@Article{2level-ECC,
  Title                    = {フラッシュメモリを用いた大容量SSDのための2段階誤り制御符号},
  Author                   = {金子 晴彦 and 松坂 拓哉 and 藤原 英一},
  Journal                  = {第7回情報科学技術フォーラム},
  Year                     = {2008},

  Month                    = {aug},
  Pages                    = {59-62}
}

@Book{MC_in_Condensed_Matter,
  Title                    = {The Monte Carlo Method in Condensed Matter Physics},
  Editor                   = {Kurt Binder},
  Publisher                = {Springer Verlag},
  Year                     = {1995},

  Abstract                 = {Alongside experimental and theoretical work, computer simulation now forms one of the major tools of research in physics. The Monte Carlo method is the most important simulation method in the area of condensed matter physics. This book, written by foremost experts in the field, describes the state of the art of simulation methods in solid state physics. It also reviews selected applications in areas of particular current interest like simulations of growth processes far from equilibrium, interfacial phenomena, quantum and classical fluids, polymers, quantum problems on lattices, and random systems. A new chapter on recent developments in the Monte Carlo simulation of condensed matter has been attached.},
  ISBN                     = {978-3-540-60174-6},
  Timestamp                = {2012.03.22},
  Url                      = {http://www.springer.com/physics/theoretical%2C+mathematical+%26+computational+physics/book/978-3-540-60174-6}
}

@Book{978-94-011-1068-6,
  Title                    = {From Statistical Physics to Statistical Inference and Back},
  Editor                   = {Peter Grassberger and Jean-Pierre Nadal},
  Publisher                = {Springer},
  Year                     = {1994},

  File                     = {:PDF\\978-94-011-1068-6.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.15}
}

@Book{adv_in_comp_sim,
  Title                    = {Advances in Computer Simulation},
  Editor                   = {Janos Kertesz and Imre Kondor},
  Publisher                = {Springer Verlag},
  Year                     = {1996},
  Month                    = {July},

  Abstract                 = {Computer simulation has become a basic tool in many branches of physics such as statistical physics, particle physics, or materials science. The application of efficient algorithms is at least as important as good hardware in large-scale computation. This volume contains didactic lectures on such techniques based on physical insight. The emphasis is on Monte Carlo methods (introduction, cluster algorithms, reweighting and multihistogram techniques, umbrella sampling), efficient data analysis and optimization methods, but aspects of supercomputing, the solution of stochastic differential equations, and molecular dynamics are also discussed. The book addresses graduate students and researchers in theoretical and computational physics.},
  ISBN                     = {978-3-540-63942-8},
  Keywords                 = {Monte Carlo methods - computer simulation - molecular dynamics - optimization - stochastic differential equations},
  Timestamp                = {2012.03.22},
  Url                      = {http://www.springer.com/physics/theoretical%2C+mathematical+%26+computational+physics/book/978-3-540-63942-8}
}

@Book{AdvancedMFM,
  Title                    = {Advanced Mean Field Methods},
  Editor                   = {Manfred Opper and David Saad},
  Publisher                = {The MIT Press},
  Year                     = {2001},

  Owner                    = {jason},
  Timestamp                = {2015.02.05}
}

@Book{9783540441830,
  Title                    = {Exploratory Data Analysis in Empirical Research},
  Editor                   = {Manfred Schwaiger and Otto Opitz},
  Publisher                = {Springer},
  Year                     = {2003},

  File                     = {:PDF\\3A978-3-642-55721-7.pdf:PDF},
  Owner                    = {jason},
  Timestamp                = {2015.07.15},
  Url                      = {http://www.springer.com/jp/book/9783540441830}
}

